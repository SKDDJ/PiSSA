05/01/2024 16:22:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/01/2024 16:22:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.01,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=../roberta_glue_reproduce/roberta-lora/log,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=80.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=../roberta_glue_reproduce/roberta-lora/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=roberta-lora,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=0,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.06,
warmup_steps=0,
weight_decay=0.0,
)
05/01/2024 16:22:59 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='FacebookAI/roberta-base', lora_path=None, l_num=512, mode='base', config_name=None, rank=[768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768], lora_alpha=[768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768], target_modules=['query', 'value', 'key'], lora_dropout=0.0, lora_bias='none', lora_task_type='SEQ_CLS', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False)
05/01/2024 16:23:15 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
05/01/2024 16:23:15 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
05/01/2024 16:23:15 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
05/01/2024 16:23:15 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
*** Just Lora !!! ***
                                                                                                    get_PEFT_model...                                                                                                    get_PEFT_model[32m...[DONE][0m[34m	43,815.59ms[0m
*** Parameter number after share ***
*** label and id ***
{'unacceptable': 0, 'acceptable': 1}
05/01/2024 16:24:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-99710716a8d3c0d3.arrow
05/01/2024 16:24:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b42e752626d029a5.arrow
05/01/2024 16:24:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-2752f7e612e9bb33.arrow
05/01/2024 16:24:01 - INFO - __main__ - Sample 6311 of the training set: {'sentence': 'Brandon read every book that Megan did.', 'label': 1, 'idx': 6311, 'input_ids': [0, 40468, 1166, 358, 1040, 14, 11266, 222, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/01/2024 16:24:01 - INFO - __main__ - Sample 6890 of the training set: {'sentence': 'What the baby did was chew the biscuit.', 'label': 1, 'idx': 6890, 'input_ids': [0, 2264, 5, 1928, 222, 21, 34431, 5, 39315, 6439, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/01/2024 16:24:01 - INFO - __main__ - Sample 663 of the training set: {'sentence': 'The ship sank beneath the waves.', 'label': 1, 'idx': 663, 'input_ids': [0, 133, 3627, 14501, 11352, 5, 6995, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
========================================
hello! vector_Z
base_model.model.roberta.embeddings.word_embeddings.weight : 38,603,520

base_model.model.roberta.embeddings.position_embeddings.weight : 394,752

base_model.model.roberta.embeddings.token_type_embeddings.weight : 768

base_model.model.roberta.embeddings.LayerNorm.weight : 768

base_model.model.roberta.embeddings.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.0.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.0.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.0.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.0.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.0.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.0.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.0.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.0.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.0.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.0.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.0.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.0.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.0.output.dense.bias : 768

base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.1.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.1.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.1.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.1.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.1.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.1.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.1.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.1.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.1.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.1.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.1.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.1.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.1.output.dense.bias : 768

base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.2.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.2.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.2.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.2.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.2.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.2.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.2.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.2.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.2.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.2.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.2.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.2.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.2.output.dense.bias : 768

base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.3.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.3.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.3.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.3.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.3.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.3.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.3.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.3.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.3.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.3.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.3.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.3.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.3.output.dense.bias : 768

base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.4.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.4.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.4.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.4.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.4.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.4.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.4.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.4.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.4.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.4.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.4.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.4.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.4.output.dense.bias : 768

base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.5.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.5.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.5.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.5.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.5.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.5.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.5.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.5.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.5.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.5.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.5.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.5.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.5.output.dense.bias : 768

base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.6.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.6.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.6.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.6.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.6.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.6.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.6.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.6.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.6.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.6.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.6.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.6.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.6.output.dense.bias : 768

base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.7.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.7.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.7.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.7.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.7.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.7.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.7.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.7.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.7.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.7.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.7.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.7.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.7.output.dense.bias : 768

base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.8.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.8.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.8.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.8.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.8.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.8.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.8.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.8.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.8.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.8.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.8.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.8.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.8.output.dense.bias : 768

base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.9.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.9.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.9.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.9.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.9.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.9.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.9.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.9.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.9.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.9.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.9.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.9.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.9.output.dense.bias : 768

base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.10.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.10.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.10.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.10.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.10.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.10.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.10.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.10.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.10.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.10.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.10.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.10.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.10.output.dense.bias : 768

base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.bias : 768

base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.11.attention.self.query.vector_z.default : 768

base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.11.attention.self.key.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.11.attention.self.key.base_layer.bias : 768

base_model.model.roberta.encoder.layer.11.attention.self.key.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.11.attention.self.key.vector_z.default : 768

base_model.model.roberta.encoder.layer.11.attention.self.key.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight : 589,824

base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.bias : 768

base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight : 589,824

base_model.model.roberta.encoder.layer.11.attention.self.value.vector_z.default : 768

base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight : 589,824

base_model.model.roberta.encoder.layer.11.attention.output.dense.weight : 589,824

base_model.model.roberta.encoder.layer.11.attention.output.dense.bias : 768

base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias : 768

base_model.model.roberta.encoder.layer.11.intermediate.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.11.intermediate.dense.bias : 3,072

base_model.model.roberta.encoder.layer.11.output.dense.weight : 2,359,296

base_model.model.roberta.encoder.layer.11.output.dense.bias : 768

base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight : 768

base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias : 768

base_model.model.classifier.modules_to_save.default.dense.weight : 589,824

base_model.model.classifier.modules_to_save.default.dense.bias : 768

base_model.model.classifier.modules_to_save.default.out_proj.weight : 1,536

base_model.model.classifier.modules_to_save.default.out_proj.bias : 2

vector params: 27,648 || trainable params: 619,778 || all params: 167,734,276 || trainable%: 0.3694999106801522
base_model.model.roberta.encoder.layer.0.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.0.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.0.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.1.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.1.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.1.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.2.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.2.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.2.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.3.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.3.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.3.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.4.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.4.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.4.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.5.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.5.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.5.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.6.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.6.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.6.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.7.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.7.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.7.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.8.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.8.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.8.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.9.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.9.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.9.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.10.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.10.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.10.attention.self.value.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.11.attention.self.query.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.11.attention.self.key.vector_z.default 是可训练的: True
base_model.model.roberta.encoder.layer.11.attention.self.value.vector_z.default 是可训练的: True
base_model.model.classifier.modules_to_save.default.dense.weight 是可训练的: True
base_model.model.classifier.modules_to_save.default.dense.bias 是可训练的: True
base_model.model.classifier.modules_to_save.default.out_proj.weight 是可训练的: True
base_model.model.classifier.modules_to_save.default.out_proj.bias 是可训练的: True
{'loss': 0.7045, 'grad_norm': 1.9777021408081055, 'learning_rate': 1.9485580670303975e-05, 'epoch': 0.01}
{'loss': 0.6825, 'grad_norm': 2.690122365951538, 'learning_rate': 3.897116134060795e-05, 'epoch': 0.02}
{'loss': 0.685, 'grad_norm': 1.2884037494659424, 'learning_rate': 5.8456742010911926e-05, 'epoch': 0.03}
{'loss': 0.6112, 'grad_norm': 1.8466085195541382, 'learning_rate': 7.79423226812159e-05, 'epoch': 0.04}
{'loss': 0.6482, 'grad_norm': 1.9441577196121216, 'learning_rate': 9.742790335151988e-05, 'epoch': 0.05}
{'loss': 0.687, 'grad_norm': 1.8580446243286133, 'learning_rate': 0.00011691348402182385, 'epoch': 0.06}
{'loss': 0.6051, 'grad_norm': 1.483444094657898, 'learning_rate': 0.00013639906469212781, 'epoch': 0.07}
{'loss': 0.5659, 'grad_norm': 1.0490511655807495, 'learning_rate': 0.0001558846453624318, 'epoch': 0.07}
{'loss': 0.569, 'grad_norm': 2.8536217212677, 'learning_rate': 0.00017537022603273576, 'epoch': 0.08}
{'loss': 0.7006, 'grad_norm': 3.145948648452759, 'learning_rate': 0.00019485580670303975, 'epoch': 0.09}
{'loss': 0.6162, 'grad_norm': 2.3774077892303467, 'learning_rate': 0.00021434138737334374, 'epoch': 0.1}
{'loss': 0.5918, 'grad_norm': 1.461163878440857, 'learning_rate': 0.0002338269680436477, 'epoch': 0.11}
{'loss': 0.6334, 'grad_norm': 0.7276607751846313, 'learning_rate': 0.00025331254871395167, 'epoch': 0.12}
{'loss': 0.6213, 'grad_norm': 2.9807281494140625, 'learning_rate': 0.00027279812938425563, 'epoch': 0.13}
{'loss': 0.6624, 'grad_norm': 3.978198766708374, 'learning_rate': 0.00029228371005455964, 'epoch': 0.14}
{'loss': 0.6661, 'grad_norm': 5.068288326263428, 'learning_rate': 0.0003117692907248636, 'epoch': 0.15}
{'loss': 0.6221, 'grad_norm': 1.9164429903030396, 'learning_rate': 0.00033125487139516757, 'epoch': 0.16}
{'loss': 0.5615, 'grad_norm': 1.6291053295135498, 'learning_rate': 0.00035074045206547153, 'epoch': 0.17}
{'loss': 0.5501, 'grad_norm': 2.4723658561706543, 'learning_rate': 0.0003702260327357755, 'epoch': 0.18}
{'loss': 0.6615, 'grad_norm': 4.417022228240967, 'learning_rate': 0.0003897116134060795, 'epoch': 0.19}
{'loss': 0.6547, 'grad_norm': 1.7920265197753906, 'learning_rate': 0.00040919719407638347, 'epoch': 0.2}
{'loss': 0.6586, 'grad_norm': 2.694394826889038, 'learning_rate': 0.0004286827747466875, 'epoch': 0.21}
{'loss': 0.6609, 'grad_norm': 3.8443877696990967, 'learning_rate': 0.00044816835541699145, 'epoch': 0.22}
{'loss': 0.6759, 'grad_norm': 1.7489253282546997, 'learning_rate': 0.0004676539360872954, 'epoch': 0.22}
{'loss': 0.6512, 'grad_norm': 0.9684823751449585, 'learning_rate': 0.0004871395167575994, 'epoch': 0.23}
{'loss': 0.7073, 'grad_norm': 3.0408923625946045, 'learning_rate': 0.0005066250974279033, 'epoch': 0.24}
{'loss': 0.6035, 'grad_norm': 0.8087384104728699, 'learning_rate': 0.0005261106780982073, 'epoch': 0.25}
{'loss': 0.63, 'grad_norm': 3.7642533779144287, 'learning_rate': 0.0005455962587685113, 'epoch': 0.26}
{'loss': 0.6579, 'grad_norm': 0.836199164390564, 'learning_rate': 0.0005650818394388153, 'epoch': 0.27}
{'loss': 0.6189, 'grad_norm': 1.869246006011963, 'learning_rate': 0.0005845674201091193, 'epoch': 0.28}
{'loss': 0.6427, 'grad_norm': 1.7810111045837402, 'learning_rate': 0.0006040530007794232, 'epoch': 0.29}
{'loss': 0.506, 'grad_norm': 1.2185289859771729, 'learning_rate': 0.0006235385814497272, 'epoch': 0.3}
{'loss': 0.6788, 'grad_norm': 4.229643821716309, 'learning_rate': 0.0006430241621200312, 'epoch': 0.31}
{'loss': 0.6092, 'grad_norm': 1.0101860761642456, 'learning_rate': 0.0006625097427903351, 'epoch': 0.32}
{'loss': 0.6214, 'grad_norm': 2.0617029666900635, 'learning_rate': 0.000681995323460639, 'epoch': 0.33}
{'loss': 0.6679, 'grad_norm': 3.288360595703125, 'learning_rate': 0.0007014809041309431, 'epoch': 0.34}
{'loss': 0.5432, 'grad_norm': 2.887549877166748, 'learning_rate': 0.0007209664848012471, 'epoch': 0.35}
{'loss': 0.7525, 'grad_norm': 5.1887054443359375, 'learning_rate': 0.000740452065471551, 'epoch': 0.36}
{'loss': 0.5641, 'grad_norm': 2.1135146617889404, 'learning_rate': 0.000759937646141855, 'epoch': 0.36}
{'loss': 0.5149, 'grad_norm': 0.595108687877655, 'learning_rate': 0.000779423226812159, 'epoch': 0.37}
{'loss': 0.7503, 'grad_norm': 3.5927178859710693, 'learning_rate': 0.0007989088074824629, 'epoch': 0.38}
{'loss': 0.5904, 'grad_norm': 2.0369153022766113, 'learning_rate': 0.0008183943881527669, 'epoch': 0.39}
{'loss': 0.6636, 'grad_norm': 1.4748256206512451, 'learning_rate': 0.000837879968823071, 'epoch': 0.4}
{'loss': 0.6575, 'grad_norm': 6.232381343841553, 'learning_rate': 0.000857365549493375, 'epoch': 0.41}
{'loss': 0.5319, 'grad_norm': 1.6869977712631226, 'learning_rate': 0.0008768511301636789, 'epoch': 0.42}
{'loss': 0.5767, 'grad_norm': 2.7092041969299316, 'learning_rate': 0.0008963367108339829, 'epoch': 0.43}
{'loss': 0.6192, 'grad_norm': 1.9942561388015747, 'learning_rate': 0.0009158222915042869, 'epoch': 0.44}
{'loss': 0.5364, 'grad_norm': 2.6329691410064697, 'learning_rate': 0.0009353078721745908, 'epoch': 0.45}
{'loss': 0.6572, 'grad_norm': 1.5498926639556885, 'learning_rate': 0.0009547934528448948, 'epoch': 0.46}
{'loss': 0.6648, 'grad_norm': 3.340474843978882, 'learning_rate': 0.0009742790335151988, 'epoch': 0.47}
{'loss': 0.6277, 'grad_norm': 0.9428625702857971, 'learning_rate': 0.0009937646141855029, 'epoch': 0.48}
{'loss': 0.5574, 'grad_norm': 1.9987637996673584, 'learning_rate': 0.0010132501948558067, 'epoch': 0.49}
{'loss': 0.592, 'grad_norm': 1.4674487113952637, 'learning_rate': 0.0010327357755261107, 'epoch': 0.5}
{'loss': 0.5191, 'grad_norm': 1.5531835556030273, 'learning_rate': 0.0010522213561964147, 'epoch': 0.51}
{'loss': 0.5857, 'grad_norm': 2.1780426502227783, 'learning_rate': 0.0010717069368667187, 'epoch': 0.51}
{'loss': 0.5757, 'grad_norm': 0.7683107256889343, 'learning_rate': 0.0010911925175370225, 'epoch': 0.52}
{'loss': 0.5311, 'grad_norm': 0.8058608174324036, 'learning_rate': 0.0011106780982073265, 'epoch': 0.53}
{'loss': 0.6975, 'grad_norm': 1.1381504535675049, 'learning_rate': 0.0011301636788776305, 'epoch': 0.54}
{'loss': 0.5268, 'grad_norm': 0.7759996056556702, 'learning_rate': 0.0011496492595479346, 'epoch': 0.55}
{'loss': 0.8219, 'grad_norm': 1.3623207807540894, 'learning_rate': 0.0011691348402182386, 'epoch': 0.56}
{'loss': 0.5904, 'grad_norm': 1.1940797567367554, 'learning_rate': 0.0011886204208885424, 'epoch': 0.57}
{'loss': 0.5374, 'grad_norm': 1.4544780254364014, 'learning_rate': 0.0012081060015588464, 'epoch': 0.58}
{'loss': 0.6231, 'grad_norm': 1.621291995048523, 'learning_rate': 0.0012275915822291504, 'epoch': 0.59}
{'loss': 0.5497, 'grad_norm': 3.0684640407562256, 'learning_rate': 0.0012470771628994544, 'epoch': 0.6}
{'loss': 0.5638, 'grad_norm': 3.385908365249634, 'learning_rate': 0.0012665627435697584, 'epoch': 0.61}
{'loss': 0.5723, 'grad_norm': 1.8487910032272339, 'learning_rate': 0.0012860483242400625, 'epoch': 0.62}
{'loss': 0.6332, 'grad_norm': 4.2962799072265625, 'learning_rate': 0.0013055339049103663, 'epoch': 0.63}
{'loss': 0.7955, 'grad_norm': 3.792250633239746, 'learning_rate': 0.0013250194855806703, 'epoch': 0.64}
{'loss': 0.5131, 'grad_norm': 0.6104488968849182, 'learning_rate': 0.0013445050662509743, 'epoch': 0.65}
{'loss': 0.6694, 'grad_norm': 3.5184171199798584, 'learning_rate': 0.001363990646921278, 'epoch': 0.65}
{'loss': 0.5624, 'grad_norm': 1.1896593570709229, 'learning_rate': 0.001383476227591582, 'epoch': 0.66}
{'loss': 0.5653, 'grad_norm': 4.291105270385742, 'learning_rate': 0.0014029618082618861, 'epoch': 0.67}
{'loss': 0.5311, 'grad_norm': 2.2908942699432373, 'learning_rate': 0.0014224473889321901, 'epoch': 0.68}
{'loss': 0.5957, 'grad_norm': 2.981476068496704, 'learning_rate': 0.0014419329696024942, 'epoch': 0.69}
{'loss': 0.588, 'grad_norm': 1.5686253309249878, 'learning_rate': 0.0014614185502727982, 'epoch': 0.7}
{'loss': 0.5574, 'grad_norm': 4.446465969085693, 'learning_rate': 0.001480904130943102, 'epoch': 0.71}
{'loss': 0.5274, 'grad_norm': 0.9388167262077332, 'learning_rate': 0.001500389711613406, 'epoch': 0.72}
{'loss': 0.6038, 'grad_norm': 1.7123680114746094, 'learning_rate': 0.00151987529228371, 'epoch': 0.73}
{'loss': 0.5358, 'grad_norm': 1.6983541250228882, 'learning_rate': 0.001539360872954014, 'epoch': 0.74}
{'loss': 0.6947, 'grad_norm': 1.954113483428955, 'learning_rate': 0.001558846453624318, 'epoch': 0.75}
{'loss': 0.5493, 'grad_norm': 0.9773955345153809, 'learning_rate': 0.001578332034294622, 'epoch': 0.76}
{'loss': 0.5424, 'grad_norm': 3.5041818618774414, 'learning_rate': 0.0015978176149649258, 'epoch': 0.77}
{'loss': 0.6749, 'grad_norm': 1.1932634115219116, 'learning_rate': 0.0016173031956352299, 'epoch': 0.78}
{'loss': 0.7407, 'grad_norm': 2.027686357498169, 'learning_rate': 0.0016367887763055339, 'epoch': 0.79}
{'loss': 0.6126, 'grad_norm': 3.0928447246551514, 'learning_rate': 0.001656274356975838, 'epoch': 0.8}
{'loss': 0.6288, 'grad_norm': 4.4388108253479, 'learning_rate': 0.001675759937646142, 'epoch': 0.8}
{'loss': 0.7372, 'grad_norm': 1.081549882888794, 'learning_rate': 0.001695245518316446, 'epoch': 0.81}
{'loss': 0.5823, 'grad_norm': 2.1383843421936035, 'learning_rate': 0.00171473109898675, 'epoch': 0.82}
{'loss': 0.5627, 'grad_norm': 1.7894871234893799, 'learning_rate': 0.0017342166796570537, 'epoch': 0.83}
{'loss': 0.586, 'grad_norm': 1.667797327041626, 'learning_rate': 0.0017537022603273578, 'epoch': 0.84}
{'loss': 0.4858, 'grad_norm': 2.2314019203186035, 'learning_rate': 0.0017731878409976618, 'epoch': 0.85}
{'loss': 0.5101, 'grad_norm': 2.1841816902160645, 'learning_rate': 0.0017926734216679658, 'epoch': 0.86}
{'loss': 0.5515, 'grad_norm': 1.6373199224472046, 'learning_rate': 0.0018121590023382698, 'epoch': 0.87}
{'loss': 0.4895, 'grad_norm': 1.7508671283721924, 'learning_rate': 0.0018316445830085738, 'epoch': 0.88}
{'loss': 0.5161, 'grad_norm': 1.4853484630584717, 'learning_rate': 0.0018511301636788776, 'epoch': 0.89}
{'loss': 0.5358, 'grad_norm': 3.639268636703491, 'learning_rate': 0.0018706157443491816, 'epoch': 0.9}
{'loss': 0.3524, 'grad_norm': 3.500807046890259, 'learning_rate': 0.0018901013250194857, 'epoch': 0.91}
{'loss': 0.6944, 'grad_norm': 1.7373789548873901, 'learning_rate': 0.0019095869056897897, 'epoch': 0.92}
{'loss': 0.5066, 'grad_norm': 2.260791301727295, 'learning_rate': 0.0019290724863600937, 'epoch': 0.93}
{'loss': 0.4467, 'grad_norm': 2.07505464553833, 'learning_rate': 0.0019485580670303977, 'epoch': 0.94}
{'loss': 0.6097, 'grad_norm': 0.9311022162437439, 'learning_rate': 0.0019680436477007017, 'epoch': 0.94}
{'loss': 0.4994, 'grad_norm': 3.9452364444732666, 'learning_rate': 0.0019875292283710057, 'epoch': 0.95}
{'loss': 0.612, 'grad_norm': 2.4592854976654053, 'learning_rate': 0.0020070148090413097, 'epoch': 0.96}
{'loss': 0.6544, 'grad_norm': 4.267354965209961, 'learning_rate': 0.0020265003897116133, 'epoch': 0.97}
{'loss': 0.5526, 'grad_norm': 4.287135601043701, 'learning_rate': 0.0020459859703819173, 'epoch': 0.98}
{'loss': 0.5402, 'grad_norm': 5.022130966186523, 'learning_rate': 0.0020654715510522214, 'epoch': 0.99}
{'eval_loss': 0.4799339175224304, 'eval_matthews_correlation': 0.458136114916564, 'eval_runtime': 30.5582, 'eval_samples_per_second': 34.132, 'eval_steps_per_second': 4.287, 'epoch': 1.0}
{'loss': 0.6518, 'grad_norm': 2.7967963218688965, 'learning_rate': 0.0020849571317225254, 'epoch': 1.0}
{'loss': 0.745, 'grad_norm': 3.473477840423584, 'learning_rate': 0.0021044427123928294, 'epoch': 1.01}
{'loss': 0.5607, 'grad_norm': 4.117523670196533, 'learning_rate': 0.0021239282930631334, 'epoch': 1.02}
{'loss': 0.4834, 'grad_norm': 2.4418392181396484, 'learning_rate': 0.0021434138737334374, 'epoch': 1.03}
{'loss': 0.6082, 'grad_norm': 3.0144541263580322, 'learning_rate': 0.002162899454403741, 'epoch': 1.04}
{'loss': 0.554, 'grad_norm': 1.8201603889465332, 'learning_rate': 0.002182385035074045, 'epoch': 1.05}
{'loss': 0.658, 'grad_norm': 2.3797218799591064, 'learning_rate': 0.002201870615744349, 'epoch': 1.06}
{'loss': 0.5719, 'grad_norm': 1.0586439371109009, 'learning_rate': 0.002221356196414653, 'epoch': 1.07}
{'loss': 0.6355, 'grad_norm': 2.7360472679138184, 'learning_rate': 0.002240841777084957, 'epoch': 1.08}
{'loss': 0.7744, 'grad_norm': 2.0275487899780273, 'learning_rate': 0.002260327357755261, 'epoch': 1.09}
{'loss': 0.6012, 'grad_norm': 3.7647249698638916, 'learning_rate': 0.002279812938425565, 'epoch': 1.09}
{'loss': 0.5161, 'grad_norm': 1.6876893043518066, 'learning_rate': 0.002299298519095869, 'epoch': 1.1}
{'loss': 0.5106, 'grad_norm': 1.9960519075393677, 'learning_rate': 0.002318784099766173, 'epoch': 1.11}
{'loss': 0.4918, 'grad_norm': 1.7040765285491943, 'learning_rate': 0.002338269680436477, 'epoch': 1.12}
{'loss': 0.4787, 'grad_norm': 1.2177879810333252, 'learning_rate': 0.002357755261106781, 'epoch': 1.13}
{'loss': 0.5162, 'grad_norm': 1.221943736076355, 'learning_rate': 0.0023772408417770848, 'epoch': 1.14}
{'loss': 0.4665, 'grad_norm': 2.6035239696502686, 'learning_rate': 0.0023967264224473888, 'epoch': 1.15}
{'loss': 0.4842, 'grad_norm': 7.482902526855469, 'learning_rate': 0.0024162120031176928, 'epoch': 1.16}
{'loss': 0.4588, 'grad_norm': 1.3014813661575317, 'learning_rate': 0.002435697583787997, 'epoch': 1.17}
{'loss': 0.9777, 'grad_norm': 2.9426608085632324, 'learning_rate': 0.002455183164458301, 'epoch': 1.18}
{'loss': 0.4337, 'grad_norm': 1.8957607746124268, 'learning_rate': 0.002474668745128605, 'epoch': 1.19}
{'loss': 0.5542, 'grad_norm': 1.182965636253357, 'learning_rate': 0.002494154325798909, 'epoch': 1.2}
{'loss': 0.3442, 'grad_norm': 2.82171368598938, 'learning_rate': 0.002513639906469213, 'epoch': 1.21}
{'loss': 0.5211, 'grad_norm': 1.0353323221206665, 'learning_rate': 0.002533125487139517, 'epoch': 1.22}
{'loss': 0.4848, 'grad_norm': 4.629068851470947, 'learning_rate': 0.002552611067809821, 'epoch': 1.23}
{'loss': 0.6527, 'grad_norm': 1.3470685482025146, 'learning_rate': 0.002572096648480125, 'epoch': 1.23}
{'loss': 0.5234, 'grad_norm': 1.0636656284332275, 'learning_rate': 0.002591582229150429, 'epoch': 1.24}
{'loss': 0.504, 'grad_norm': 1.1408495903015137, 'learning_rate': 0.0026110678098207325, 'epoch': 1.25}
{'loss': 0.5824, 'grad_norm': 2.2319772243499756, 'learning_rate': 0.0026305533904910365, 'epoch': 1.26}
{'loss': 0.6519, 'grad_norm': 7.653367519378662, 'learning_rate': 0.0026500389711613405, 'epoch': 1.27}
{'loss': 0.6185, 'grad_norm': 3.4608845710754395, 'learning_rate': 0.0026695245518316446, 'epoch': 1.28}
{'loss': 0.4829, 'grad_norm': 3.5298805236816406, 'learning_rate': 0.0026890101325019486, 'epoch': 1.29}
{'loss': 0.6046, 'grad_norm': 3.472057342529297, 'learning_rate': 0.0027084957131722526, 'epoch': 1.3}
{'loss': 0.535, 'grad_norm': 4.466287612915039, 'learning_rate': 0.002727981293842556, 'epoch': 1.31}
{'loss': 0.4556, 'grad_norm': 1.6074975728988647, 'learning_rate': 0.0027474668745128606, 'epoch': 1.32}
{'loss': 0.6715, 'grad_norm': 2.7814481258392334, 'learning_rate': 0.002766952455183164, 'epoch': 1.33}
{'loss': 0.583, 'grad_norm': 2.310497283935547, 'learning_rate': 0.0027864380358534687, 'epoch': 1.34}
{'loss': 0.5755, 'grad_norm': 1.40659761428833, 'learning_rate': 0.0028059236165237722, 'epoch': 1.35}
{'loss': 0.6037, 'grad_norm': 1.9535118341445923, 'learning_rate': 0.0028254091971940767, 'epoch': 1.36}
{'loss': 0.5, 'grad_norm': 3.981015920639038, 'learning_rate': 0.0028448947778643803, 'epoch': 1.37}
{'loss': 0.5252, 'grad_norm': 1.5763418674468994, 'learning_rate': 0.0028643803585346843, 'epoch': 1.38}
{'loss': 0.4904, 'grad_norm': 2.0954296588897705, 'learning_rate': 0.0028838659392049883, 'epoch': 1.38}
{'loss': 0.5638, 'grad_norm': 3.438917398452759, 'learning_rate': 0.0029033515198752923, 'epoch': 1.39}
{'loss': 0.561, 'grad_norm': 3.5832955837249756, 'learning_rate': 0.0029228371005455963, 'epoch': 1.4}
{'loss': 0.4558, 'grad_norm': 2.988997459411621, 'learning_rate': 0.0029423226812159003, 'epoch': 1.41}
{'loss': 0.4721, 'grad_norm': 1.0017499923706055, 'learning_rate': 0.002961808261886204, 'epoch': 1.42}
{'loss': 0.4253, 'grad_norm': 3.2572436332702637, 'learning_rate': 0.0029812938425565084, 'epoch': 1.43}
{'loss': 0.6189, 'grad_norm': 3.354438543319702, 'learning_rate': 0.003000779423226812, 'epoch': 1.44}
{'loss': 0.5079, 'grad_norm': 1.104145884513855, 'learning_rate': 0.0030202650038971164, 'epoch': 1.45}
{'loss': 0.6524, 'grad_norm': 4.033065319061279, 'learning_rate': 0.00303975058456742, 'epoch': 1.46}
{'loss': 0.5646, 'grad_norm': 2.444737195968628, 'learning_rate': 0.0030592361652377244, 'epoch': 1.47}
{'loss': 0.4479, 'grad_norm': 1.713622808456421, 'learning_rate': 0.003078721745908028, 'epoch': 1.48}
{'loss': 0.3564, 'grad_norm': 1.912776231765747, 'learning_rate': 0.0030982073265783325, 'epoch': 1.49}
{'loss': 0.8015, 'grad_norm': 1.526057243347168, 'learning_rate': 0.003117692907248636, 'epoch': 1.5}
{'loss': 0.6203, 'grad_norm': 2.20043683052063, 'learning_rate': 0.00313717848791894, 'epoch': 1.51}
{'loss': 0.513, 'grad_norm': 0.8532243371009827, 'learning_rate': 0.003156664068589244, 'epoch': 1.52}
{'loss': 0.606, 'grad_norm': 1.6909642219543457, 'learning_rate': 0.003176149649259548, 'epoch': 1.52}
{'loss': 0.4932, 'grad_norm': 1.7391417026519775, 'learning_rate': 0.0031956352299298517, 'epoch': 1.53}
{'loss': 0.6029, 'grad_norm': 2.492436408996582, 'learning_rate': 0.003215120810600156, 'epoch': 1.54}
{'loss': 0.4284, 'grad_norm': 1.49055814743042, 'learning_rate': 0.0032346063912704597, 'epoch': 1.55}
{'loss': 0.9562, 'grad_norm': 1.1503485441207886, 'learning_rate': 0.003254091971940764, 'epoch': 1.56}
{'loss': 0.5941, 'grad_norm': 4.26430606842041, 'learning_rate': 0.0032735775526110678, 'epoch': 1.57}
{'loss': 0.4737, 'grad_norm': 3.1930878162384033, 'learning_rate': 0.003293063133281372, 'epoch': 1.58}
{'loss': 0.5314, 'grad_norm': 3.5269341468811035, 'learning_rate': 0.003312548713951676, 'epoch': 1.59}
{'loss': 0.5579, 'grad_norm': 1.085801124572754, 'learning_rate': 0.0033320342946219802, 'epoch': 1.6}
{'loss': 0.6442, 'grad_norm': 4.922977924346924, 'learning_rate': 0.003351519875292284, 'epoch': 1.61}
{'loss': 0.3239, 'grad_norm': 3.415168285369873, 'learning_rate': 0.003371005455962588, 'epoch': 1.62}
{'loss': 0.719, 'grad_norm': 2.799870014190674, 'learning_rate': 0.003390491036632892, 'epoch': 1.63}
{'loss': 0.5248, 'grad_norm': 4.164980888366699, 'learning_rate': 0.003409976617303196, 'epoch': 1.64}
{'loss': 0.5126, 'grad_norm': 2.8290672302246094, 'learning_rate': 0.0034294621979735, 'epoch': 1.65}
{'loss': 0.4774, 'grad_norm': 3.2354373931884766, 'learning_rate': 0.0034489477786438035, 'epoch': 1.66}
{'loss': 0.5729, 'grad_norm': 2.2951037883758545, 'learning_rate': 0.0034684333593141075, 'epoch': 1.67}
{'loss': 0.6477, 'grad_norm': 5.134143352508545, 'learning_rate': 0.0034879189399844115, 'epoch': 1.67}
{'loss': 0.6463, 'grad_norm': 1.8958590030670166, 'learning_rate': 0.0035074045206547155, 'epoch': 1.68}
{'loss': 0.507, 'grad_norm': 2.043821096420288, 'learning_rate': 0.003526890101325019, 'epoch': 1.69}
{'loss': 0.4635, 'grad_norm': 2.2153615951538086, 'learning_rate': 0.0035463756819953235, 'epoch': 1.7}
{'loss': 0.4556, 'grad_norm': 1.6098610162734985, 'learning_rate': 0.003565861262665627, 'epoch': 1.71}
{'loss': 0.6938, 'grad_norm': 2.913703680038452, 'learning_rate': 0.0035853468433359316, 'epoch': 1.72}
{'loss': 0.5437, 'grad_norm': 1.9786043167114258, 'learning_rate': 0.003604832424006235, 'epoch': 1.73}
{'loss': 0.6719, 'grad_norm': 1.3394752740859985, 'learning_rate': 0.0036243180046765396, 'epoch': 1.74}
{'loss': 0.4134, 'grad_norm': 1.549634575843811, 'learning_rate': 0.003643803585346843, 'epoch': 1.75}
{'loss': 0.3079, 'grad_norm': 2.8653781414031982, 'learning_rate': 0.0036632891660171476, 'epoch': 1.76}
{'loss': 0.5718, 'grad_norm': 6.566344738006592, 'learning_rate': 0.0036827747466874512, 'epoch': 1.77}
{'loss': 0.6677, 'grad_norm': 2.9744887351989746, 'learning_rate': 0.0037022603273577552, 'epoch': 1.78}
{'loss': 0.5692, 'grad_norm': 1.8744494915008545, 'learning_rate': 0.0037217459080280593, 'epoch': 1.79}
{'loss': 0.6386, 'grad_norm': 1.0272376537322998, 'learning_rate': 0.0037412314886983633, 'epoch': 1.8}
{'loss': 0.5, 'grad_norm': 6.0894575119018555, 'learning_rate': 0.0037607170693686673, 'epoch': 1.81}
{'loss': 0.3879, 'grad_norm': 2.3472142219543457, 'learning_rate': 0.0037802026500389713, 'epoch': 1.81}
{'loss': 0.4914, 'grad_norm': 2.0178768634796143, 'learning_rate': 0.003799688230709275, 'epoch': 1.82}
{'loss': 0.5117, 'grad_norm': 3.6580796241760254, 'learning_rate': 0.0038191738113795793, 'epoch': 1.83}
{'loss': 0.5061, 'grad_norm': 2.8771250247955322, 'learning_rate': 0.003838659392049883, 'epoch': 1.84}
{'loss': 0.618, 'grad_norm': 3.510267734527588, 'learning_rate': 0.0038581449727201874, 'epoch': 1.85}

05/04/2024 16:06:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/04/2024 16:06:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0004,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/log,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=80.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=64,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.06,
warmup_steps=0,
weight_decay=0.0,
)
05/04/2024 16:06:30 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='FacebookAI/roberta-base', lora_path=None, l_num=None, mode='base', config_name=None, rank=[128], lora_alpha=[768], target_modules=['query', 'value', 'key'], lora_dropout=0.0, lora_bias='none', lora_task_type='SEQ_CLS', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False)
05/04/2024 16:06:30 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
05/04/2024 16:06:30 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
05/04/2024 16:06:30 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
05/04/2024 16:06:47 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
05/04/2024 16:06:47 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
05/04/2024 16:06:47 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
05/04/2024 16:06:47 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
*** PiSSA !!! ***
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	786.23ms[0m
*** Parameter number after share ***
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	792.30ms[0m
*** Parameter number after share ***
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	795.19ms[0m
*** Parameter number after share ***
*** label and id ***
{'unacceptable': 0, 'acceptable': 1}
*** label and id ***
{'unacceptable': 0, 'acceptable': 1}
*** label and id ***
{'unacceptable': 0, 'acceptable': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	801.00ms[0m
*** Parameter number after share ***
pissa params: 7,077,888 || trainable params: 7,670,018 || all params: 132,317,188 || trainable%: 5.796690600770627
pissa params: 7,077,888 || trainable params(wo classfier): 7,670,018 || all params: 132,317,188 || trainable%: 5.796690600770627
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.0.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.0.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.1.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.1.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.2.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.2.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.3.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.3.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.4.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.4.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.5.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.5.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.6.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.6.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.7.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.7.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.8.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.8.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.9.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.9.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.10.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.10.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.11.attention.self.key.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.11.attention.self.key.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.classifier.modules_to_save.default.dense.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.classifier.modules_to_save.default.dense.bias æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.classifier.modules_to_save.default.out_proj.weight æ˜¯å¯è®­ç»ƒçš„: True
base_model.model.module.classifier.modules_to_save.default.out_proj.bias æ˜¯å¯è®­ç»ƒçš„: True
*** label and id ***
{'unacceptable': 0, 'acceptable': 1}
05/04/2024 16:06:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f5d627a0779d7abd.arrow
05/04/2024 16:06:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-16d4fac30a92b326.arrow
05/04/2024 16:06:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5d45ed2a505ee051.arrow
05/04/2024 16:06:53 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [0, 100, 5055, 14, 127, 1150, 6, 37, 21, 3229, 25, 41, 37323, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/04/2024 16:06:53 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [0, 2709, 123, 7, 109, 14, 74, 28, 10, 5021, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/04/2024 16:06:53 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [0, 24877, 11944, 10, 2214, 6, 53, 2094, 393, 222, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
{'loss': 0.7394, 'grad_norm': 4.039407730102539, 'learning_rate': 2.4390243902439026e-05, 'epoch': 0.29}
{'loss': 0.6692, 'grad_norm': 7.32451868057251, 'learning_rate': 4.878048780487805e-05, 'epoch': 0.59}
{'loss': 0.6021, 'grad_norm': 2.2743210792541504, 'learning_rate': 7.317073170731707e-05, 'epoch': 0.88}
{'eval_loss': 0.5353006720542908, 'eval_matthews_correlation': 0.0, 'eval_runtime': 2.9282, 'eval_samples_per_second': 356.193, 'eval_steps_per_second': 1.708, 'epoch': 1.0}
{'loss': 0.5245, 'grad_norm': 6.352140426635742, 'learning_rate': 9.75609756097561e-05, 'epoch': 1.18}
{'loss': 0.5014, 'grad_norm': 8.290996551513672, 'learning_rate': 0.00012195121951219512, 'epoch': 1.47}
{'loss': 0.4631, 'grad_norm': 8.11368465423584, 'learning_rate': 0.00014634146341463414, 'epoch': 1.76}
{'eval_loss': 0.5219438672065735, 'eval_matthews_correlation': 0.46419999997102035, 'eval_runtime': 2.9323, 'eval_samples_per_second': 355.693, 'eval_steps_per_second': 1.705, 'epoch': 2.0}
{'loss': 0.4341, 'grad_norm': 5.575012683868408, 'learning_rate': 0.0001707317073170732, 'epoch': 2.06}
{'loss': 0.3562, 'grad_norm': 9.384161949157715, 'learning_rate': 0.0001951219512195122, 'epoch': 2.35}
{'loss': 0.3852, 'grad_norm': 13.759902954101562, 'learning_rate': 0.00021951219512195125, 'epoch': 2.65}
{'loss': 0.368, 'grad_norm': 6.9860944747924805, 'learning_rate': 0.00024390243902439024, 'epoch': 2.94}
{'eval_loss': 0.44195565581321716, 'eval_matthews_correlation': 0.5578770438298012, 'eval_runtime': 2.9382, 'eval_samples_per_second': 354.978, 'eval_steps_per_second': 1.702, 'epoch': 3.0}
{'loss': 0.3247, 'grad_norm': 8.995729446411133, 'learning_rate': 0.0002682926829268293, 'epoch': 3.24}
{'loss': 0.2928, 'grad_norm': 15.433903694152832, 'learning_rate': 0.0002926829268292683, 'epoch': 3.53}
{'loss': 0.3074, 'grad_norm': 8.569324493408203, 'learning_rate': 0.00031707317073170733, 'epoch': 3.82}
{'eval_loss': 0.56989985704422, 'eval_matthews_correlation': 0.49784061977662064, 'eval_runtime': 2.9531, 'eval_samples_per_second': 353.187, 'eval_steps_per_second': 1.693, 'epoch': 4.0}
{'loss': 0.3152, 'grad_norm': 7.414783954620361, 'learning_rate': 0.0003414634146341464, 'epoch': 4.12}
{'loss': 0.2756, 'grad_norm': 9.349753379821777, 'learning_rate': 0.0003658536585365854, 'epoch': 4.41}
{'loss': 0.2795, 'grad_norm': 7.597947120666504, 'learning_rate': 0.0003902439024390244, 'epoch': 4.71}
{'loss': 0.2892, 'grad_norm': 8.230294227600098, 'learning_rate': 0.00039906103286384975, 'epoch': 5.0}
{'eval_loss': 0.4703252911567688, 'eval_matthews_correlation': 0.5562974350301167, 'eval_runtime': 2.9374, 'eval_samples_per_second': 355.071, 'eval_steps_per_second': 1.702, 'epoch': 5.0}
{'loss': 0.2428, 'grad_norm': 7.370977401733398, 'learning_rate': 0.0003974960876369327, 'epoch': 5.29}
{'loss': 0.2601, 'grad_norm': 9.586000442504883, 'learning_rate': 0.00039593114241001567, 'epoch': 5.59}
{'loss': 0.2767, 'grad_norm': 8.328523635864258, 'learning_rate': 0.0003943661971830986, 'epoch': 5.88}
{'eval_loss': 0.5312618017196655, 'eval_matthews_correlation': 0.5234928415614652, 'eval_runtime': 2.9599, 'eval_samples_per_second': 352.38, 'eval_steps_per_second': 1.689, 'epoch': 6.0}
{'loss': 0.2288, 'grad_norm': 15.455547332763672, 'learning_rate': 0.0003928012519561815, 'epoch': 6.18}
{'loss': 0.252, 'grad_norm': 8.78454875946045, 'learning_rate': 0.0003912363067292645, 'epoch': 6.47}
{'loss': 0.2264, 'grad_norm': 12.3283052444458, 'learning_rate': 0.00038967136150234744, 'epoch': 6.76}
{'eval_loss': 0.4968225359916687, 'eval_matthews_correlation': 0.5323002583628187, 'eval_runtime': 2.9631, 'eval_samples_per_second': 352.0, 'eval_steps_per_second': 1.687, 'epoch': 7.0}
{'loss': 0.255, 'grad_norm': 7.351251125335693, 'learning_rate': 0.0003881064162754304, 'epoch': 7.06}
{'loss': 0.1774, 'grad_norm': 7.588705062866211, 'learning_rate': 0.0003865414710485133, 'epoch': 7.35}
{'loss': 0.2213, 'grad_norm': 10.058406829833984, 'learning_rate': 0.00038497652582159626, 'epoch': 7.65}
{'loss': 0.2195, 'grad_norm': 8.911999702453613, 'learning_rate': 0.00038341158059467916, 'epoch': 7.94}
{'eval_loss': 0.5324662923812866, 'eval_matthews_correlation': 0.5604772031378175, 'eval_runtime': 2.9415, 'eval_samples_per_second': 354.583, 'eval_steps_per_second': 1.7, 'epoch': 8.0}
{'loss': 0.171, 'grad_norm': 12.139443397521973, 'learning_rate': 0.0003818466353677622, 'epoch': 8.24}
{'loss': 0.2117, 'grad_norm': 21.330297470092773, 'learning_rate': 0.0003802816901408451, 'epoch': 8.53}
{'loss': 0.204, 'grad_norm': 10.264169692993164, 'learning_rate': 0.00037871674491392804, 'epoch': 8.82}
{'eval_loss': 0.845338761806488, 'eval_matthews_correlation': 0.5022774185444647, 'eval_runtime': 2.9749, 'eval_samples_per_second': 350.604, 'eval_steps_per_second': 1.681, 'epoch': 9.0}
{'loss': 0.1663, 'grad_norm': 9.796834945678711, 'learning_rate': 0.00037715179968701094, 'epoch': 9.12}
{'loss': 0.146, 'grad_norm': 13.047562599182129, 'learning_rate': 0.0003755868544600939, 'epoch': 9.41}
{'loss': 0.1961, 'grad_norm': 13.074616432189941, 'learning_rate': 0.00037402190923317686, 'epoch': 9.71}
{'loss': 0.1723, 'grad_norm': 8.05978012084961, 'learning_rate': 0.0003724569640062598, 'epoch': 10.0}
{'eval_loss': 0.6260468363761902, 'eval_matthews_correlation': 0.5593209709812511, 'eval_runtime': 2.9433, 'eval_samples_per_second': 354.369, 'eval_steps_per_second': 1.699, 'epoch': 10.0}
{'loss': 0.1447, 'grad_norm': 11.580221176147461, 'learning_rate': 0.0003708920187793427, 'epoch': 10.29}
{'loss': 0.1673, 'grad_norm': 6.825438976287842, 'learning_rate': 0.0003693270735524257, 'epoch': 10.59}
{'loss': 0.1571, 'grad_norm': 9.195096015930176, 'learning_rate': 0.0003677621283255086, 'epoch': 10.88}
{'eval_loss': 0.5420828461647034, 'eval_matthews_correlation': 0.5757702121003263, 'eval_runtime': 2.9437, 'eval_samples_per_second': 354.321, 'eval_steps_per_second': 1.699, 'epoch': 11.0}
{'loss': 0.1537, 'grad_norm': 9.184673309326172, 'learning_rate': 0.0003661971830985916, 'epoch': 11.18}
{'loss': 0.1442, 'grad_norm': 7.30514669418335, 'learning_rate': 0.0003646322378716745, 'epoch': 11.47}

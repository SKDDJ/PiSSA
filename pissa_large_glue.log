05/05/2024 16:03:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:03:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/log,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.06,
warmup_steps=0,
weight_decay=0.0,
)
05/05/2024 16:03:10 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='FacebookAI/roberta-large', lora_path=None, l_num=None, mode='base', config_name=None, rank=[8], lora_alpha=[768], target_modules=['query', 'value'], lora_dropout=0.0, lora_bias='none', lora_task_type='SEQ_CLS', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False)
05/05/2024 16:03:10 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:03:10 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:03:11 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:03:11 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:03:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
05/05/2024 16:03:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
05/05/2024 16:03:51 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
05/05/2024 16:03:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	548.72ms[0m
*** Parameter number after share ***
*** label and id ***
{'unacceptable': 0, 'acceptable': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	597.88ms[0m
*** Parameter number after share ***
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	590.36ms[0m
*** Parameter number after share ***
*** label and id ***
{'unacceptable': 0, 'acceptable': 1}
*** label and id ***
{'unacceptable': 0, 'acceptable': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	621.33ms[0m
*** Parameter number after share ***
pissa params: 786,432 || trainable params: 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
pissa params: 786,432 || trainable params(wo classfier): 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	631.38ms[0m
*** Parameter number after share ***
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight 是可训练的: True*** label and id ***

{'unacceptable': 0, 'acceptable': 1}
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.bias 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.bias 是可训练的: True
*** label and id ***
{'unacceptable': 0, 'acceptable': 1}
05/05/2024 16:04:08 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-91090868ffd71e49.arrow
05/05/2024 16:04:08 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-a096c5a92d476af2.arrow
05/05/2024 16:04:08 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-692f32fb07b86a99.arrow
05/05/2024 16:04:08 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [0, 100, 5055, 14, 127, 1150, 6, 37, 21, 3229, 25, 41, 37323, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 16:04:08 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [0, 2709, 123, 7, 109, 14, 74, 28, 10, 5021, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 16:04:08 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [0, 24877, 11944, 10, 2214, 6, 53, 2094, 393, 222, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
{'loss': 0.8068, 'grad_norm': 9.17017650604248, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.19}
{'loss': 0.6563, 'grad_norm': 18.01020050048828, 'learning_rate': 6.153846153846155e-05, 'epoch': 0.37}
{'loss': 0.5974, 'grad_norm': 4.640515327453613, 'learning_rate': 9.230769230769232e-05, 'epoch': 0.56}
{'loss': 0.498, 'grad_norm': 19.407560348510742, 'learning_rate': 0.0001230769230769231, 'epoch': 0.74}
{'loss': 0.4611, 'grad_norm': 20.22886848449707, 'learning_rate': 0.00015384615384615385, 'epoch': 0.93}
{'eval_loss': 0.529469907283783, 'eval_matthews_correlation': 0.4984147011250188, 'eval_runtime': 0.8394, 'eval_samples_per_second': 1242.605, 'eval_steps_per_second': 8.34, 'epoch': 1.0}
{'loss': 0.4658, 'grad_norm': 20.614431381225586, 'learning_rate': 0.00018461538461538463, 'epoch': 1.11}
{'loss': 0.4501, 'grad_norm': 38.54009246826172, 'learning_rate': 0.00019901477832512317, 'epoch': 1.3}
{'loss': 0.4415, 'grad_norm': 49.586952209472656, 'learning_rate': 0.00019704433497536947, 'epoch': 1.48}
{'loss': 0.4473, 'grad_norm': 19.201265335083008, 'learning_rate': 0.00019507389162561577, 'epoch': 1.67}
{'loss': 0.4275, 'grad_norm': 28.214712142944336, 'learning_rate': 0.0001931034482758621, 'epoch': 1.85}
{'eval_loss': 0.5284034609794617, 'eval_matthews_correlation': 0.5359216286120269, 'eval_runtime': 0.8024, 'eval_samples_per_second': 1299.788, 'eval_steps_per_second': 8.723, 'epoch': 2.0}
{'loss': 0.3841, 'grad_norm': 23.433269500732422, 'learning_rate': 0.0001911330049261084, 'epoch': 2.04}
{'loss': 0.3675, 'grad_norm': 16.903413772583008, 'learning_rate': 0.0001891625615763547, 'epoch': 2.22}
{'loss': 0.3667, 'grad_norm': 16.886348724365234, 'learning_rate': 0.000187192118226601, 'epoch': 2.41}
{'loss': 0.3856, 'grad_norm': 14.543277740478516, 'learning_rate': 0.00018522167487684729, 'epoch': 2.59}
{'loss': 0.3525, 'grad_norm': 25.95240592956543, 'learning_rate': 0.0001832512315270936, 'epoch': 2.78}
{'loss': 0.3573, 'grad_norm': 24.891481399536133, 'learning_rate': 0.0001812807881773399, 'epoch': 2.96}
{'eval_loss': 0.5194578766822815, 'eval_matthews_correlation': 0.5820194424772402, 'eval_runtime': 0.8015, 'eval_samples_per_second': 1301.345, 'eval_steps_per_second': 8.734, 'epoch': 3.0}
{'loss': 0.3445, 'grad_norm': 19.145998001098633, 'learning_rate': 0.0001793103448275862, 'epoch': 3.15}
{'loss': 0.3156, 'grad_norm': 19.49688148498535, 'learning_rate': 0.00017733990147783253, 'epoch': 3.33}
{'loss': 0.3162, 'grad_norm': 16.603635787963867, 'learning_rate': 0.00017536945812807883, 'epoch': 3.52}
{'loss': 0.3186, 'grad_norm': 21.885494232177734, 'learning_rate': 0.00017339901477832515, 'epoch': 3.7}
{'loss': 0.329, 'grad_norm': 24.791324615478516, 'learning_rate': 0.00017142857142857143, 'epoch': 3.89}
{'eval_loss': 0.4589177668094635, 'eval_matthews_correlation': 0.6060933436539927, 'eval_runtime': 0.8044, 'eval_samples_per_second': 1296.637, 'eval_steps_per_second': 8.702, 'epoch': 4.0}
{'loss': 0.2851, 'grad_norm': 19.347949981689453, 'learning_rate': 0.00016945812807881772, 'epoch': 4.07}
{'loss': 0.292, 'grad_norm': 24.712766647338867, 'learning_rate': 0.00016748768472906405, 'epoch': 4.26}
{'loss': 0.2709, 'grad_norm': 12.57046890258789, 'learning_rate': 0.00016551724137931035, 'epoch': 4.44}
{'loss': 0.2626, 'grad_norm': 57.79170608520508, 'learning_rate': 0.00016354679802955667, 'epoch': 4.63}
{'loss': 0.2917, 'grad_norm': 25.620203018188477, 'learning_rate': 0.00016157635467980297, 'epoch': 4.81}
{'loss': 0.28, 'grad_norm': 19.905969619750977, 'learning_rate': 0.00015960591133004927, 'epoch': 5.0}
{'eval_loss': 0.4442303478717804, 'eval_matthews_correlation': 0.6431079373028232, 'eval_runtime': 0.7509, 'eval_samples_per_second': 1389.017, 'eval_steps_per_second': 9.322, 'epoch': 5.0}
{'loss': 0.2383, 'grad_norm': 25.981782913208008, 'learning_rate': 0.0001576354679802956, 'epoch': 5.19}
{'loss': 0.2186, 'grad_norm': 23.459257125854492, 'learning_rate': 0.0001556650246305419, 'epoch': 5.37}
{'loss': 0.2188, 'grad_norm': 18.663639068603516, 'learning_rate': 0.00015369458128078816, 'epoch': 5.56}
{'loss': 0.2262, 'grad_norm': 20.41287612915039, 'learning_rate': 0.00015172413793103449, 'epoch': 5.74}
{'loss': 0.2305, 'grad_norm': 29.471214294433594, 'learning_rate': 0.00014975369458128078, 'epoch': 5.93}
{'eval_loss': 0.4185889661312103, 'eval_matthews_correlation': 0.6293307552514323, 'eval_runtime': 0.7672, 'eval_samples_per_second': 1359.572, 'eval_steps_per_second': 9.125, 'epoch': 6.0}
{'loss': 0.1981, 'grad_norm': 22.068279266357422, 'learning_rate': 0.0001477832512315271, 'epoch': 6.11}
{'loss': 0.2028, 'grad_norm': 23.992652893066406, 'learning_rate': 0.0001458128078817734, 'epoch': 6.3}
{'loss': 0.1986, 'grad_norm': 35.896728515625, 'learning_rate': 0.0001438423645320197, 'epoch': 6.48}
{'loss': 0.1827, 'grad_norm': 51.69439697265625, 'learning_rate': 0.00014187192118226603, 'epoch': 6.67}
{'loss': 0.2519, 'grad_norm': 32.622100830078125, 'learning_rate': 0.00013990147783251233, 'epoch': 6.85}
{'eval_loss': 0.5283386707305908, 'eval_matthews_correlation': 0.6181999490554073, 'eval_runtime': 0.7526, 'eval_samples_per_second': 1385.917, 'eval_steps_per_second': 9.301, 'epoch': 7.0}
{'loss': 0.1848, 'grad_norm': 11.842052459716797, 'learning_rate': 0.00013793103448275863, 'epoch': 7.04}
{'loss': 0.158, 'grad_norm': 37.988304138183594, 'learning_rate': 0.00013596059113300492, 'epoch': 7.22}
{'loss': 0.1688, 'grad_norm': 18.423643112182617, 'learning_rate': 0.00013399014778325122, 'epoch': 7.41}
{'loss': 0.1724, 'grad_norm': 22.987653732299805, 'learning_rate': 0.00013201970443349755, 'epoch': 7.59}
{'loss': 0.1862, 'grad_norm': 23.534589767456055, 'learning_rate': 0.00013004926108374385, 'epoch': 7.78}
{'loss': 0.1801, 'grad_norm': 11.412612915039062, 'learning_rate': 0.00012807881773399014, 'epoch': 7.96}
{'eval_loss': 0.4333168864250183, 'eval_matthews_correlation': 0.6848492477503537, 'eval_runtime': 0.7535, 'eval_samples_per_second': 1384.249, 'eval_steps_per_second': 9.29, 'epoch': 8.0}
{'loss': 0.1373, 'grad_norm': 18.971830368041992, 'learning_rate': 0.00012610837438423647, 'epoch': 8.15}
{'loss': 0.1394, 'grad_norm': 45.09051513671875, 'learning_rate': 0.00012413793103448277, 'epoch': 8.33}
{'loss': 0.1752, 'grad_norm': 29.071537017822266, 'learning_rate': 0.00012216748768472906, 'epoch': 8.52}
{'loss': 0.1585, 'grad_norm': 33.192626953125, 'learning_rate': 0.00012019704433497539, 'epoch': 8.7}
{'loss': 0.1537, 'grad_norm': 15.25434684753418, 'learning_rate': 0.00011822660098522169, 'epoch': 8.89}
{'eval_loss': 0.541203498840332, 'eval_matthews_correlation': 0.6505087408484611, 'eval_runtime': 0.807, 'eval_samples_per_second': 1292.368, 'eval_steps_per_second': 8.674, 'epoch': 9.0}
{'loss': 0.1451, 'grad_norm': 32.78783416748047, 'learning_rate': 0.00011625615763546797, 'epoch': 9.07}
{'loss': 0.1144, 'grad_norm': 32.863033294677734, 'learning_rate': 0.00011428571428571428, 'epoch': 9.26}
{'loss': 0.1465, 'grad_norm': 17.57520294189453, 'learning_rate': 0.0001123152709359606, 'epoch': 9.44}
{'loss': 0.1231, 'grad_norm': 24.69205665588379, 'learning_rate': 0.0001103448275862069, 'epoch': 9.63}
{'loss': 0.1361, 'grad_norm': 18.22759246826172, 'learning_rate': 0.0001083743842364532, 'epoch': 9.81}
{'loss': 0.1357, 'grad_norm': 29.077678680419922, 'learning_rate': 0.00010640394088669952, 'epoch': 10.0}
{'eval_loss': 0.6301818490028381, 'eval_matthews_correlation': 0.6261333902108016, 'eval_runtime': 0.8062, 'eval_samples_per_second': 1293.791, 'eval_steps_per_second': 8.683, 'epoch': 10.0}
{'loss': 0.1233, 'grad_norm': 22.84745979309082, 'learning_rate': 0.00010443349753694583, 'epoch': 10.19}
{'loss': 0.1006, 'grad_norm': 15.248126983642578, 'learning_rate': 0.00010246305418719213, 'epoch': 10.37}
{'loss': 0.1224, 'grad_norm': 17.446346282958984, 'learning_rate': 0.00010049261083743844, 'epoch': 10.56}
{'loss': 0.1133, 'grad_norm': 14.433562278747559, 'learning_rate': 9.852216748768474e-05, 'epoch': 10.74}
{'loss': 0.1382, 'grad_norm': 24.747804641723633, 'learning_rate': 9.655172413793105e-05, 'epoch': 10.93}
{'eval_loss': 0.5910554528236389, 'eval_matthews_correlation': 0.6306425398187112, 'eval_runtime': 0.7555, 'eval_samples_per_second': 1380.453, 'eval_steps_per_second': 9.265, 'epoch': 11.0}
{'loss': 0.1149, 'grad_norm': 11.08751106262207, 'learning_rate': 9.458128078817734e-05, 'epoch': 11.11}
{'loss': 0.0857, 'grad_norm': 31.49169921875, 'learning_rate': 9.261083743842364e-05, 'epoch': 11.3}
{'loss': 0.0946, 'grad_norm': 34.213375091552734, 'learning_rate': 9.064039408866995e-05, 'epoch': 11.48}
{'loss': 0.094, 'grad_norm': 31.76152229309082, 'learning_rate': 8.866995073891627e-05, 'epoch': 11.67}
{'loss': 0.1213, 'grad_norm': 19.31126594543457, 'learning_rate': 8.669950738916258e-05, 'epoch': 11.85}
{'eval_loss': 0.6382976174354553, 'eval_matthews_correlation': 0.66271185211871, 'eval_runtime': 0.8037, 'eval_samples_per_second': 1297.8, 'eval_steps_per_second': 8.71, 'epoch': 12.0}
{'loss': 0.1128, 'grad_norm': 19.533855438232422, 'learning_rate': 8.472906403940886e-05, 'epoch': 12.04}
{'loss': 0.079, 'grad_norm': 17.311616897583008, 'learning_rate': 8.275862068965517e-05, 'epoch': 12.22}
{'loss': 0.0979, 'grad_norm': 22.003416061401367, 'learning_rate': 8.078817733990148e-05, 'epoch': 12.41}
{'loss': 0.1002, 'grad_norm': 23.320905685424805, 'learning_rate': 7.88177339901478e-05, 'epoch': 12.59}
{'loss': 0.0972, 'grad_norm': 20.80135154724121, 'learning_rate': 7.684729064039408e-05, 'epoch': 12.78}
{'loss': 0.0921, 'grad_norm': 19.212400436401367, 'learning_rate': 7.487684729064039e-05, 'epoch': 12.96}
{'eval_loss': 0.6073818206787109, 'eval_matthews_correlation': 0.6530464576994947, 'eval_runtime': 0.8133, 'eval_samples_per_second': 1282.467, 'eval_steps_per_second': 8.607, 'epoch': 13.0}
{'loss': 0.078, 'grad_norm': 32.085914611816406, 'learning_rate': 7.29064039408867e-05, 'epoch': 13.15}
{'loss': 0.0871, 'grad_norm': 28.24880599975586, 'learning_rate': 7.093596059113302e-05, 'epoch': 13.33}
{'loss': 0.0779, 'grad_norm': 35.69451141357422, 'learning_rate': 6.896551724137931e-05, 'epoch': 13.52}
{'loss': 0.0831, 'grad_norm': 25.194936752319336, 'learning_rate': 6.699507389162561e-05, 'epoch': 13.7}
{'loss': 0.0628, 'grad_norm': 17.26207160949707, 'learning_rate': 6.502463054187192e-05, 'epoch': 13.89}
{'eval_loss': 0.6580296158790588, 'eval_matthews_correlation': 0.6481914570909114, 'eval_runtime': 0.8066, 'eval_samples_per_second': 1293.157, 'eval_steps_per_second': 8.679, 'epoch': 14.0}
{'loss': 0.0725, 'grad_norm': 32.69908142089844, 'learning_rate': 6.305418719211823e-05, 'epoch': 14.07}
{'loss': 0.0736, 'grad_norm': 19.66472625732422, 'learning_rate': 6.108374384236453e-05, 'epoch': 14.26}
{'loss': 0.0648, 'grad_norm': 37.758026123046875, 'learning_rate': 5.9113300492610844e-05, 'epoch': 14.44}
{'loss': 0.0703, 'grad_norm': 19.59044647216797, 'learning_rate': 5.714285714285714e-05, 'epoch': 14.63}
{'loss': 0.0683, 'grad_norm': 15.403812408447266, 'learning_rate': 5.517241379310345e-05, 'epoch': 14.81}
{'loss': 0.0748, 'grad_norm': 16.781417846679688, 'learning_rate': 5.320197044334976e-05, 'epoch': 15.0}
{'eval_loss': 0.6376569271087646, 'eval_matthews_correlation': 0.6529329633658166, 'eval_runtime': 0.8082, 'eval_samples_per_second': 1290.523, 'eval_steps_per_second': 8.661, 'epoch': 15.0}
{'loss': 0.0589, 'grad_norm': 18.1428165435791, 'learning_rate': 5.123152709359606e-05, 'epoch': 15.19}
{'loss': 0.0533, 'grad_norm': 25.154787063598633, 'learning_rate': 4.926108374384237e-05, 'epoch': 15.37}
{'loss': 0.0573, 'grad_norm': 9.000269889831543, 'learning_rate': 4.729064039408867e-05, 'epoch': 15.56}
{'loss': 0.056, 'grad_norm': 13.86419677734375, 'learning_rate': 4.532019704433498e-05, 'epoch': 15.74}
{'loss': 0.0595, 'grad_norm': 10.979578018188477, 'learning_rate': 4.334975369458129e-05, 'epoch': 15.93}
{'eval_loss': 0.7776030898094177, 'eval_matthews_correlation': 0.620934625159678, 'eval_runtime': 0.8131, 'eval_samples_per_second': 1282.717, 'eval_steps_per_second': 8.609, 'epoch': 16.0}
{'loss': 0.065, 'grad_norm': 17.08795928955078, 'learning_rate': 4.1379310344827587e-05, 'epoch': 16.11}
{'loss': 0.0386, 'grad_norm': 13.48011589050293, 'learning_rate': 3.94088669950739e-05, 'epoch': 16.3}
{'loss': 0.0473, 'grad_norm': 18.294828414916992, 'learning_rate': 3.7438423645320196e-05, 'epoch': 16.48}
{'loss': 0.0544, 'grad_norm': 17.25157928466797, 'learning_rate': 3.546798029556651e-05, 'epoch': 16.67}
{'loss': 0.0596, 'grad_norm': 9.881828308105469, 'learning_rate': 3.3497536945812806e-05, 'epoch': 16.85}
{'eval_loss': 0.7095027565956116, 'eval_matthews_correlation': 0.6725027991090756, 'eval_runtime': 0.803, 'eval_samples_per_second': 1298.873, 'eval_steps_per_second': 8.717, 'epoch': 17.0}
{'loss': 0.0536, 'grad_norm': 16.133014678955078, 'learning_rate': 3.152709359605912e-05, 'epoch': 17.04}
{'loss': 0.0473, 'grad_norm': 16.67279052734375, 'learning_rate': 2.9556650246305422e-05, 'epoch': 17.22}
{'loss': 0.047, 'grad_norm': 8.064016342163086, 'learning_rate': 2.7586206896551727e-05, 'epoch': 17.41}
{'loss': 0.0459, 'grad_norm': 29.21160316467285, 'learning_rate': 2.561576354679803e-05, 'epoch': 17.59}
{'loss': 0.0493, 'grad_norm': 19.221281051635742, 'learning_rate': 2.3645320197044336e-05, 'epoch': 17.78}
{'loss': 0.0478, 'grad_norm': 12.51978874206543, 'learning_rate': 2.1674876847290644e-05, 'epoch': 17.96}
{'eval_loss': 0.7613936066627502, 'eval_matthews_correlation': 0.6480906778081594, 'eval_runtime': 0.845, 'eval_samples_per_second': 1234.3, 'eval_steps_per_second': 8.284, 'epoch': 18.0}
{'loss': 0.0419, 'grad_norm': 11.861228942871094, 'learning_rate': 1.970443349753695e-05, 'epoch': 18.15}
{'loss': 0.0353, 'grad_norm': 6.2683305740356445, 'learning_rate': 1.7733990147783254e-05, 'epoch': 18.33}
{'loss': 0.0313, 'grad_norm': 19.238012313842773, 'learning_rate': 1.576354679802956e-05, 'epoch': 18.52}
{'loss': 0.0402, 'grad_norm': 5.572690963745117, 'learning_rate': 1.3793103448275863e-05, 'epoch': 18.7}
{'loss': 0.0489, 'grad_norm': 22.126724243164062, 'learning_rate': 1.1822660098522168e-05, 'epoch': 18.89}
{'eval_loss': 0.7700229287147522, 'eval_matthews_correlation': 0.6479802060710294, 'eval_runtime': 0.8059, 'eval_samples_per_second': 1294.135, 'eval_steps_per_second': 8.685, 'epoch': 19.0}
{'loss': 0.035, 'grad_norm': 4.698119163513184, 'learning_rate': 9.852216748768475e-06, 'epoch': 19.07}
{'loss': 0.0503, 'grad_norm': 26.155298233032227, 'learning_rate': 7.88177339901478e-06, 'epoch': 19.26}
{'loss': 0.0217, 'grad_norm': 18.99274444580078, 'learning_rate': 5.911330049261084e-06, 'epoch': 19.44}
{'loss': 0.0345, 'grad_norm': 12.590012550354004, 'learning_rate': 3.94088669950739e-06, 'epoch': 19.63}
{'loss': 0.022, 'grad_norm': 11.617853164672852, 'learning_rate': 1.970443349753695e-06, 'epoch': 19.81}
{'loss': 0.0323, 'grad_norm': 17.867027282714844, 'learning_rate': 0.0, 'epoch': 20.0}
{'eval_loss': 0.7969162464141846, 'eval_matthews_correlation': 0.6479544437772453, 'eval_runtime': 0.8065, 'eval_samples_per_second': 1293.197, 'eval_steps_per_second': 8.679, 'epoch': 20.0}
{'train_runtime': 549.4968, 'train_samples_per_second': 311.23, 'train_steps_per_second': 1.965, 'train_loss': 0.17859509009178037, 'epoch': 20.0}
***** train metrics *****
  epoch                    =       20.0
  total_flos               = 37721700GF
  train_loss               =     0.1786
  train_runtime            = 0:09:09.49
  train_samples            =       8551
  train_samples_per_second =     311.23
  train_steps_per_second   =      1.965
05/05/2024 16:13:24 - INFO - __main__ - *** Evaluate ***
***** eval metrics *****
  epoch                     =       20.0
  eval_loss                 =     0.4333
  eval_matthews_correlation =     0.6848
  eval_runtime              = 0:00:00.81
  eval_samples              =       1043
  eval_samples_per_second   =   1283.749
  eval_steps_per_second     =      8.616
05/05/2024 16:13:40 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:13:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0003,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/log,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.06,
warmup_steps=0,
weight_decay=0.0,
)
05/05/2024 16:13:40 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='FacebookAI/roberta-large', lora_path=None, l_num=None, mode='base', config_name=None, rank=[8], lora_alpha=[768], target_modules=['query', 'value'], lora_dropout=0.0, lora_bias='none', lora_task_type='SEQ_CLS', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False)
05/05/2024 16:13:40 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:13:40 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:13:40 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:13:40 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:14:18 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
05/05/2024 16:14:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
05/05/2024 16:14:18 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
05/05/2024 16:14:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	531.09ms[0m
*** Parameter number after share ***
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	537.35ms[0m
*** Parameter number after share ***
pissa params: 786,432 || trainable params: 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
pissa params: 786,432 || trainable params(wo classfier): 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight 是可训练的: True
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	554.36ms[0m
*** Parameter number after share ***
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight 是可训练的: True
*** label and id ***
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight 是可训练的: True{'not_equivalent': 0, 'equivalent': 1}

base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.bias 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.bias 是可训练的: True
*** label and id ***
{'not_equivalent': 0, 'equivalent': 1}
*** label and id ***
{'not_equivalent': 0, 'equivalent': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	624.61ms[0m
*** Parameter number after share ***
*** label and id ***                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	662.82ms[0m

{'not_equivalent': 0, 'equivalent': 1}
05/05/2024 16:14:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b36185e5b254381f.arrow
*** Parameter number after share ***
*** label and id ***
{'not_equivalent': 0, 'equivalent': 1}
05/05/2024 16:14:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0bac0b812d641075.arrow
05/05/2024 16:14:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6c74d23ed3bdead1.arrow
05/05/2024 16:14:37 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [0, 133, 7069, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 2357, 6052, 9, 2339, 24387, 1857, 136, 123, 479, 2, 2, 10653, 32326, 1033, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 10, 2357, 12, 8596, 17984, 1601, 7, 5, 461, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 16:14:37 - INFO - __main__ - Sample 456 of the training set: {'sentence1': "Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .", 'sentence2': "Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .", 'label': 1, 'idx': 509, 'input_ids': [0, 16764, 8224, 503, 447, 13, 5, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 8, 8556, 16, 878, 239, 789, 9, 220, 395, 128, 29, 1939, 729, 11, 997, 12, 25566, 3576, 13212, 2636, 479, 2, 2, 35779, 11, 3576, 13212, 2636, 128, 29, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 2156, 8, 8556, 16, 878, 239, 789, 9, 395, 128, 29, 1939, 729, 11, 5, 997, 12, 6472, 4628, 976, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 16:14:37 - INFO - __main__ - Sample 102 of the training set: {'sentence1': "Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .", 'sentence2': "The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .", 'label': 0, 'idx': 116, 'input_ids': [0, 35167, 359, 11253, 128, 29, 1764, 388, 1965, 5060, 1981, 204, 4, 1749, 332, 7, 361, 6361, 4, 1096, 2156, 150, 4417, 5946, 5060, 1064, 231, 4, 245, 332, 7, 112, 6, 27639, 4, 1096, 479, 2, 2, 133, 5787, 359, 11253, 128, 29, 1764, 4648, 21, 62, 112, 4, 2545, 332, 2156, 50, 321, 4, 1366, 135, 2156, 7, 361, 4718, 4, 4671, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
05/05/2024 16:14:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:14:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0003,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/log,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.06,
warmup_steps=0,
weight_decay=0.0,
)
05/05/2024 16:14:59 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='FacebookAI/roberta-large', lora_path=None, l_num=None, mode='base', config_name=None, rank=[8], lora_alpha=[768], target_modules=['query', 'value'], lora_dropout=0.0, lora_bias='none', lora_task_type='SEQ_CLS', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False)
05/05/2024 16:14:59 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:14:59 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:14:59 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:14:59 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 16:15:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
05/05/2024 16:15:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
05/05/2024 16:15:43 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
05/05/2024 16:15:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	554.77ms[0m
*** Parameter number after share ***
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	544.81ms[0m
*** Parameter number after share ***
*** label and id ***
{'entailment': 0, 'neutral': 1, 'contradiction': 2}
*** label and id ***
{'entailment': 0, 'neutral': 1, 'contradiction': 2}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	559.33ms[0m
*** Parameter number after share ***
pissa params: 786,432 || trainable params: 1,839,107 || all params: 357,201,926 || trainable%: 0.5148648050682683
pissa params: 786,432 || trainable params(wo classfier): 1,839,107 || all params: 357,201,926 || trainable%: 0.5148648050682683
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight 是可训练的: True                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	576.14ms[0m

*** Parameter number after share ***
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.bias 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.bias 是可训练的: True
*** label and id ***
{'entailment': 0, 'neutral': 1, 'contradiction': 2}
*** label and id ***
{'entailment': 0, 'neutral': 1, 'contradiction': 2}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	616.11ms[0m
*** Parameter number after share ***
*** label and id ***
{'entailment': 0, 'neutral': 1, 'contradiction': 2}
05/05/2024 16:16:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-a1631ffdd7886d95.arrow
05/05/2024 16:16:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-36c8f9e7e601393e.arrow
05/05/2024 16:16:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c8f46d8716998638.arrow
05/05/2024 16:16:02 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-3a2da235382378df.arrow
05/05/2024 16:16:02 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5140f4ea0424c84e.arrow
05/05/2024 16:16:02 - INFO - __main__ - Sample 335243 of the training set: {'premise': "you know when their parents come and it's hard to get them out and a lot of parents have places to go and and things like that and it's late at night so", 'hypothesis': "Parents are busy and it's sometimes hard to get them out.", 'label': 0, 'idx': 335243, 'input_ids': [0, 6968, 216, 77, 49, 1041, 283, 8, 24, 18, 543, 7, 120, 106, 66, 8, 10, 319, 9, 1041, 33, 2127, 7, 213, 8, 8, 383, 101, 14, 8, 24, 18, 628, 23, 363, 98, 2, 2, 35835, 32, 3610, 8, 24, 18, 2128, 543, 7, 120, 106, 66, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 16:16:02 - INFO - __main__ - Sample 58369 of the training set: {'premise': 'Where is art?', 'hypothesis': 'Where and what is art? ', 'label': 1, 'idx': 58369, 'input_ids': [0, 13841, 16, 1808, 116, 2, 2, 13841, 8, 99, 16, 1808, 116, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 16:16:02 - INFO - __main__ - Sample 13112 of the training set: {'premise': 'Alcohol and injury, as well as brief interventions, are on the list.', 'hypothesis': 'The list says alcohol and injury are negatives facing staff.', 'label': 1, 'idx': 13112, 'input_ids': [0, 7083, 45270, 8, 1356, 6, 25, 157, 25, 4315, 15985, 6, 32, 15, 5, 889, 4, 2, 2, 133, 889, 161, 3766, 8, 1356, 32, 34784, 2114, 813, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
{'loss': 1.1507, 'grad_norm': 6.848097801208496, 'learning_rate': 2.036659877800407e-06, 'epoch': 0.0}
{'loss': 1.1465, 'grad_norm': 3.932828426361084, 'learning_rate': 4.073319755600814e-06, 'epoch': 0.01}
{'loss': 1.1236, 'grad_norm': 4.27008581161499, 'learning_rate': 6.109979633401222e-06, 'epoch': 0.01}
{'loss': 1.1092, 'grad_norm': 4.021182060241699, 'learning_rate': 8.146639511201628e-06, 'epoch': 0.02}
{'loss': 1.1123, 'grad_norm': 3.1245017051696777, 'learning_rate': 1.0183299389002036e-05, 'epoch': 0.02}
{'loss': 1.1066, 'grad_norm': 5.907740592956543, 'learning_rate': 1.2219959266802444e-05, 'epoch': 0.02}
{'loss': 1.1027, 'grad_norm': 5.00449275970459, 'learning_rate': 1.425661914460285e-05, 'epoch': 0.03}
{'loss': 1.1015, 'grad_norm': 3.672084331512451, 'learning_rate': 1.6293279022403257e-05, 'epoch': 0.03}
{'loss': 1.1052, 'grad_norm': 5.23975133895874, 'learning_rate': 1.8329938900203665e-05, 'epoch': 0.04}
{'loss': 1.0978, 'grad_norm': 2.6940054893493652, 'learning_rate': 2.0366598778004072e-05, 'epoch': 0.04}
{'loss': 1.0978, 'grad_norm': 4.9407124519348145, 'learning_rate': 2.2403258655804476e-05, 'epoch': 0.04}
{'loss': 1.0856, 'grad_norm': 10.47781753540039, 'learning_rate': 2.4439918533604887e-05, 'epoch': 0.05}
{'loss': 1.0746, 'grad_norm': 13.444506645202637, 'learning_rate': 2.647657841140529e-05, 'epoch': 0.05}
{'loss': 1.0358, 'grad_norm': 19.407197952270508, 'learning_rate': 2.85132382892057e-05, 'epoch': 0.06}
{'loss': 0.9751, 'grad_norm': 17.198562622070312, 'learning_rate': 3.054989816700611e-05, 'epoch': 0.06}
{'loss': 0.9275, 'grad_norm': 18.814416885375977, 'learning_rate': 3.2586558044806514e-05, 'epoch': 0.07}
{'loss': 0.8254, 'grad_norm': 18.798603057861328, 'learning_rate': 3.4623217922606925e-05, 'epoch': 0.07}
{'loss': 0.7374, 'grad_norm': 42.494384765625, 'learning_rate': 3.665987780040733e-05, 'epoch': 0.07}
{'loss': 0.6416, 'grad_norm': 30.309782028198242, 'learning_rate': 3.869653767820774e-05, 'epoch': 0.08}
{'loss': 0.5935, 'grad_norm': 26.380088806152344, 'learning_rate': 4.0733197556008144e-05, 'epoch': 0.08}
{'loss': 0.5848, 'grad_norm': 31.395368576049805, 'learning_rate': 4.276985743380855e-05, 'epoch': 0.09}
{'loss': 0.4898, 'grad_norm': 39.21271896362305, 'learning_rate': 4.480651731160895e-05, 'epoch': 0.09}
{'loss': 0.5422, 'grad_norm': 25.712739944458008, 'learning_rate': 4.6843177189409363e-05, 'epoch': 0.09}
{'loss': 0.5151, 'grad_norm': 29.794801712036133, 'learning_rate': 4.8879837067209774e-05, 'epoch': 0.1}
{'loss': 0.5454, 'grad_norm': 19.085378646850586, 'learning_rate': 5.0916496945010185e-05, 'epoch': 0.1}
{'loss': 0.5293, 'grad_norm': 31.41111183166504, 'learning_rate': 5.295315682281058e-05, 'epoch': 0.11}
{'loss': 0.4837, 'grad_norm': 34.79578399658203, 'learning_rate': 5.4989816700610994e-05, 'epoch': 0.11}
{'loss': 0.4958, 'grad_norm': 35.3859748840332, 'learning_rate': 5.70264765784114e-05, 'epoch': 0.11}
{'loss': 0.4983, 'grad_norm': 27.315799713134766, 'learning_rate': 5.906313645621181e-05, 'epoch': 0.12}
{'loss': 0.4934, 'grad_norm': 26.863130569458008, 'learning_rate': 6.109979633401222e-05, 'epoch': 0.12}
{'loss': 0.4364, 'grad_norm': 42.00786209106445, 'learning_rate': 6.313645621181262e-05, 'epoch': 0.13}
{'loss': 0.4489, 'grad_norm': 23.16621971130371, 'learning_rate': 6.517311608961303e-05, 'epoch': 0.13}
{'loss': 0.4739, 'grad_norm': 20.86455535888672, 'learning_rate': 6.720977596741344e-05, 'epoch': 0.13}
{'loss': 0.4518, 'grad_norm': 21.97979164123535, 'learning_rate': 6.924643584521385e-05, 'epoch': 0.14}
{'loss': 0.4608, 'grad_norm': 26.266645431518555, 'learning_rate': 7.128309572301425e-05, 'epoch': 0.14}
{'loss': 0.4452, 'grad_norm': 26.827802658081055, 'learning_rate': 7.331975560081466e-05, 'epoch': 0.15}
{'loss': 0.4291, 'grad_norm': 19.952777862548828, 'learning_rate': 7.535641547861506e-05, 'epoch': 0.15}
{'loss': 0.4419, 'grad_norm': 24.207332611083984, 'learning_rate': 7.739307535641548e-05, 'epoch': 0.15}
{'loss': 0.4638, 'grad_norm': 22.13768196105957, 'learning_rate': 7.942973523421588e-05, 'epoch': 0.16}
{'loss': 0.4767, 'grad_norm': 34.12198257446289, 'learning_rate': 8.146639511201629e-05, 'epoch': 0.16}
{'loss': 0.4422, 'grad_norm': 20.084003448486328, 'learning_rate': 8.350305498981669e-05, 'epoch': 0.17}
{'loss': 0.4424, 'grad_norm': 29.11241340637207, 'learning_rate': 8.55397148676171e-05, 'epoch': 0.17}
{'loss': 0.422, 'grad_norm': 33.03870391845703, 'learning_rate': 8.757637474541751e-05, 'epoch': 0.18}
{'loss': 0.4133, 'grad_norm': 25.605897903442383, 'learning_rate': 8.96130346232179e-05, 'epoch': 0.18}
{'loss': 0.4371, 'grad_norm': 20.188379287719727, 'learning_rate': 9.164969450101833e-05, 'epoch': 0.18}
{'loss': 0.4205, 'grad_norm': 199.372802734375, 'learning_rate': 9.368635437881873e-05, 'epoch': 0.19}
{'loss': 0.4669, 'grad_norm': 33.76466369628906, 'learning_rate': 9.572301425661912e-05, 'epoch': 0.19}
{'loss': 0.4698, 'grad_norm': 203.8799591064453, 'learning_rate': 9.775967413441955e-05, 'epoch': 0.2}
{'loss': 0.4324, 'grad_norm': 93.60796356201172, 'learning_rate': 9.979633401221995e-05, 'epoch': 0.2}
{'loss': 0.3896, 'grad_norm': 19.36438751220703, 'learning_rate': 0.00010183299389002037, 'epoch': 0.2}
{'loss': 0.3971, 'grad_norm': 30.352523803710938, 'learning_rate': 0.00010386965376782077, 'epoch': 0.21}
{'loss': 0.4161, 'grad_norm': 20.5517520904541, 'learning_rate': 0.00010590631364562117, 'epoch': 0.21}
{'loss': 0.4264, 'grad_norm': 30.89913558959961, 'learning_rate': 0.00010794297352342159, 'epoch': 0.22}
{'loss': 0.441, 'grad_norm': 21.570981979370117, 'learning_rate': 0.00010997963340122199, 'epoch': 0.22}
{'loss': 0.4593, 'grad_norm': 27.603029251098633, 'learning_rate': 0.0001120162932790224, 'epoch': 0.22}
{'loss': 0.4418, 'grad_norm': 26.413278579711914, 'learning_rate': 0.0001140529531568228, 'epoch': 0.23}
{'loss': 0.4089, 'grad_norm': 19.860883712768555, 'learning_rate': 0.0001160896130346232, 'epoch': 0.23}
{'loss': 0.4343, 'grad_norm': 33.601566314697266, 'learning_rate': 0.00011812627291242362, 'epoch': 0.24}
{'loss': 0.4215, 'grad_norm': 34.82693862915039, 'learning_rate': 0.00012016293279022401, 'epoch': 0.24}
{'loss': 0.4301, 'grad_norm': 18.459890365600586, 'learning_rate': 0.00012219959266802444, 'epoch': 0.24}
{'loss': 0.4219, 'grad_norm': 25.859155654907227, 'learning_rate': 0.00012423625254582484, 'epoch': 0.25}
{'loss': 0.4764, 'grad_norm': 23.60275650024414, 'learning_rate': 0.00012627291242362523, 'epoch': 0.25}
{'loss': 0.4616, 'grad_norm': 23.22381591796875, 'learning_rate': 0.00012830957230142566, 'epoch': 0.26}
{'loss': 0.4457, 'grad_norm': 30.69509506225586, 'learning_rate': 0.00013034623217922606, 'epoch': 0.26}
{'loss': 0.438, 'grad_norm': 33.14722442626953, 'learning_rate': 0.00013238289205702645, 'epoch': 0.26}
{'loss': 0.4485, 'grad_norm': 20.022125244140625, 'learning_rate': 0.00013441955193482688, 'epoch': 0.27}
{'loss': 0.4157, 'grad_norm': 19.157554626464844, 'learning_rate': 0.00013645621181262728, 'epoch': 0.27}
{'loss': 0.4517, 'grad_norm': 28.52934455871582, 'learning_rate': 0.0001384928716904277, 'epoch': 0.28}
{'loss': 0.4266, 'grad_norm': 22.96829605102539, 'learning_rate': 0.0001405295315682281, 'epoch': 0.28}
{'loss': 0.4076, 'grad_norm': 18.987546920776367, 'learning_rate': 0.0001425661914460285, 'epoch': 0.29}
{'loss': 0.39, 'grad_norm': 40.034156799316406, 'learning_rate': 0.00014460285132382892, 'epoch': 0.29}
{'loss': 0.4289, 'grad_norm': 13.644678115844727, 'learning_rate': 0.00014663951120162932, 'epoch': 0.29}
{'loss': 0.3857, 'grad_norm': 41.09221649169922, 'learning_rate': 0.00014867617107942974, 'epoch': 0.3}
{'loss': 0.4747, 'grad_norm': 24.587297439575195, 'learning_rate': 0.0001507128309572301, 'epoch': 0.3}
{'loss': 0.3927, 'grad_norm': 34.81560516357422, 'learning_rate': 0.00015274949083503054, 'epoch': 0.31}
{'loss': 0.4047, 'grad_norm': 110.2584228515625, 'learning_rate': 0.00015478615071283096, 'epoch': 0.31}
{'loss': 0.4154, 'grad_norm': 36.783714294433594, 'learning_rate': 0.00015682281059063136, 'epoch': 0.31}
{'loss': 0.4763, 'grad_norm': 26.42949104309082, 'learning_rate': 0.00015885947046843175, 'epoch': 0.32}
{'loss': 0.4, 'grad_norm': 35.955589294433594, 'learning_rate': 0.00016089613034623218, 'epoch': 0.32}
{'loss': 0.4442, 'grad_norm': 54.02009582519531, 'learning_rate': 0.00016293279022403258, 'epoch': 0.33}
{'loss': 0.3969, 'grad_norm': 23.772476196289062, 'learning_rate': 0.00016496945010183297, 'epoch': 0.33}
{'loss': 0.3965, 'grad_norm': 30.324695587158203, 'learning_rate': 0.00016700610997963337, 'epoch': 0.33}
{'loss': 0.4411, 'grad_norm': 21.385955810546875, 'learning_rate': 0.0001690427698574338, 'epoch': 0.34}
{'loss': 0.3981, 'grad_norm': 15.349550247192383, 'learning_rate': 0.0001710794297352342, 'epoch': 0.34}
{'loss': 0.436, 'grad_norm': 42.292686462402344, 'learning_rate': 0.0001731160896130346, 'epoch': 0.35}
{'loss': 0.4309, 'grad_norm': 22.76724624633789, 'learning_rate': 0.00017515274949083502, 'epoch': 0.35}
{'loss': 0.4626, 'grad_norm': 28.638065338134766, 'learning_rate': 0.00017718940936863544, 'epoch': 0.35}
{'loss': 0.4151, 'grad_norm': 25.23023796081543, 'learning_rate': 0.0001792260692464358, 'epoch': 0.36}
{'loss': 0.418, 'grad_norm': 28.381322860717773, 'learning_rate': 0.00018126272912423623, 'epoch': 0.36}
{'loss': 0.4445, 'grad_norm': 19.96287727355957, 'learning_rate': 0.00018329938900203666, 'epoch': 0.37}
{'loss': 0.4408, 'grad_norm': 18.845666885375977, 'learning_rate': 0.00018533604887983703, 'epoch': 0.37}
{'loss': 0.424, 'grad_norm': 38.97088623046875, 'learning_rate': 0.00018737270875763745, 'epoch': 0.37}
{'loss': 0.4401, 'grad_norm': 21.811851501464844, 'learning_rate': 0.00018940936863543788, 'epoch': 0.38}
{'loss': 0.4374, 'grad_norm': 159.91285705566406, 'learning_rate': 0.00019144602851323825, 'epoch': 0.38}
{'loss': 0.4045, 'grad_norm': 21.656770706176758, 'learning_rate': 0.00019348268839103867, 'epoch': 0.39}
{'loss': 0.4481, 'grad_norm': 46.77548599243164, 'learning_rate': 0.0001955193482688391, 'epoch': 0.39}
{'loss': 0.4177, 'grad_norm': 34.33296203613281, 'learning_rate': 0.00019755600814663947, 'epoch': 0.4}
{'loss': 0.4519, 'grad_norm': 87.5862045288086, 'learning_rate': 0.0001995926680244399, 'epoch': 0.4}
{'loss': 0.4521, 'grad_norm': 19.576669692993164, 'learning_rate': 0.00020162932790224032, 'epoch': 0.4}
{'loss': 0.4085, 'grad_norm': 21.232845306396484, 'learning_rate': 0.00020366598778004074, 'epoch': 0.41}
{'loss': 0.4016, 'grad_norm': 16.995391845703125, 'learning_rate': 0.0002057026476578411, 'epoch': 0.41}
{'loss': 0.4218, 'grad_norm': 23.518096923828125, 'learning_rate': 0.00020773930753564154, 'epoch': 0.42}
{'loss': 0.4432, 'grad_norm': 19.27800178527832, 'learning_rate': 0.00020977596741344196, 'epoch': 0.42}
{'loss': 0.4281, 'grad_norm': 23.64598846435547, 'learning_rate': 0.00021181262729124233, 'epoch': 0.42}
{'loss': 0.4339, 'grad_norm': 184.40106201171875, 'learning_rate': 0.00021384928716904276, 'epoch': 0.43}
{'loss': 0.466, 'grad_norm': 23.126245498657227, 'learning_rate': 0.00021588594704684318, 'epoch': 0.43}
{'loss': 0.4728, 'grad_norm': 31.29538345336914, 'learning_rate': 0.00021792260692464355, 'epoch': 0.44}
{'loss': 0.4488, 'grad_norm': 30.397464752197266, 'learning_rate': 0.00021995926680244397, 'epoch': 0.44}
{'loss': 0.4597, 'grad_norm': 24.271347045898438, 'learning_rate': 0.00022199592668024437, 'epoch': 0.44}
{'loss': 0.4267, 'grad_norm': 24.792734146118164, 'learning_rate': 0.0002240325865580448, 'epoch': 0.45}
{'loss': 0.4544, 'grad_norm': 23.642576217651367, 'learning_rate': 0.0002260692464358452, 'epoch': 0.45}
{'loss': 0.4282, 'grad_norm': 42.13547134399414, 'learning_rate': 0.0002281059063136456, 'epoch': 0.46}
{'loss': 0.4658, 'grad_norm': 28.281017303466797, 'learning_rate': 0.00023014256619144602, 'epoch': 0.46}
{'loss': 0.4447, 'grad_norm': 35.01040267944336, 'learning_rate': 0.0002321792260692464, 'epoch': 0.46}
{'loss': 0.4718, 'grad_norm': 40.758506774902344, 'learning_rate': 0.0002342158859470468, 'epoch': 0.47}
{'loss': 0.4063, 'grad_norm': 22.075546264648438, 'learning_rate': 0.00023625254582484723, 'epoch': 0.47}
{'loss': 0.4537, 'grad_norm': 41.41385269165039, 'learning_rate': 0.00023828920570264763, 'epoch': 0.48}
{'loss': 0.4996, 'grad_norm': 42.50367736816406, 'learning_rate': 0.00024032586558044803, 'epoch': 0.48}
{'loss': 0.4609, 'grad_norm': 21.460132598876953, 'learning_rate': 0.00024236252545824845, 'epoch': 0.48}
{'loss': 0.4663, 'grad_norm': 29.42190933227539, 'learning_rate': 0.0002443991853360489, 'epoch': 0.49}
{'loss': 0.4412, 'grad_norm': 19.895421981811523, 'learning_rate': 0.00024643584521384925, 'epoch': 0.49}
{'loss': 0.4326, 'grad_norm': 24.005226135253906, 'learning_rate': 0.0002484725050916497, 'epoch': 0.5}
{'loss': 0.421, 'grad_norm': 29.25088882446289, 'learning_rate': 0.0002505091649694501, 'epoch': 0.5}
{'loss': 0.4504, 'grad_norm': 36.559505462646484, 'learning_rate': 0.00025254582484725047, 'epoch': 0.51}
{'loss': 0.4318, 'grad_norm': 26.316898345947266, 'learning_rate': 0.0002545824847250509, 'epoch': 0.51}
{'loss': 0.4668, 'grad_norm': 29.7685489654541, 'learning_rate': 0.0002566191446028513, 'epoch': 0.51}
{'loss': 0.4665, 'grad_norm': 85.824462890625, 'learning_rate': 0.0002586558044806517, 'epoch': 0.52}
{'loss': 0.4841, 'grad_norm': 28.03271484375, 'learning_rate': 0.0002606924643584521, 'epoch': 0.52}
{'loss': 0.4375, 'grad_norm': 28.79757308959961, 'learning_rate': 0.00026272912423625254, 'epoch': 0.53}
{'loss': 0.4635, 'grad_norm': 93.36930847167969, 'learning_rate': 0.0002647657841140529, 'epoch': 0.53}
{'loss': 0.4406, 'grad_norm': 38.35292053222656, 'learning_rate': 0.00026680244399185333, 'epoch': 0.53}
{'loss': 0.4681, 'grad_norm': 32.4693489074707, 'learning_rate': 0.00026883910386965376, 'epoch': 0.54}
{'loss': 0.4193, 'grad_norm': 30.74474334716797, 'learning_rate': 0.0002708757637474542, 'epoch': 0.54}
{'loss': 0.4498, 'grad_norm': 27.12921142578125, 'learning_rate': 0.00027291242362525455, 'epoch': 0.55}
{'loss': 0.4294, 'grad_norm': 27.397008895874023, 'learning_rate': 0.000274949083503055, 'epoch': 0.55}
{'loss': 0.4942, 'grad_norm': 37.557884216308594, 'learning_rate': 0.0002769857433808554, 'epoch': 0.55}
{'loss': 0.4253, 'grad_norm': 25.483745574951172, 'learning_rate': 0.00027902240325865577, 'epoch': 0.56}
{'loss': 0.4743, 'grad_norm': 28.80553436279297, 'learning_rate': 0.0002810590631364562, 'epoch': 0.56}
{'loss': 0.4651, 'grad_norm': 22.02339744567871, 'learning_rate': 0.0002830957230142566, 'epoch': 0.57}
{'loss': 0.4623, 'grad_norm': 27.51675796508789, 'learning_rate': 0.000285132382892057, 'epoch': 0.57}
{'loss': 0.429, 'grad_norm': 92.93502044677734, 'learning_rate': 0.0002871690427698574, 'epoch': 0.57}
{'loss': 0.4992, 'grad_norm': 25.484148025512695, 'learning_rate': 0.00028920570264765784, 'epoch': 0.58}
{'loss': 0.4735, 'grad_norm': 57.27817153930664, 'learning_rate': 0.00029124236252545826, 'epoch': 0.58}
{'loss': 0.4644, 'grad_norm': 25.360393524169922, 'learning_rate': 0.00029327902240325863, 'epoch': 0.59}
{'loss': 0.4463, 'grad_norm': 36.54547119140625, 'learning_rate': 0.00029531568228105906, 'epoch': 0.59}
{'loss': 0.463, 'grad_norm': 47.834651947021484, 'learning_rate': 0.0002973523421588595, 'epoch': 0.59}
{'loss': 0.4634, 'grad_norm': 33.51468276977539, 'learning_rate': 0.00029938900203665985, 'epoch': 0.6}
{'loss': 0.4821, 'grad_norm': 29.725921630859375, 'learning_rate': 0.0002999090003033323, 'epoch': 0.6}
{'loss': 0.485, 'grad_norm': 36.88929748535156, 'learning_rate': 0.00029977900073666417, 'epoch': 0.61}
{'loss': 0.5484, 'grad_norm': 33.06683349609375, 'learning_rate': 0.0002996490011699961, 'epoch': 0.61}
{'loss': 0.6881, 'grad_norm': 44.903053283691406, 'learning_rate': 0.000299519001603328, 'epoch': 0.62}
{'loss': 0.5904, 'grad_norm': 89.3260269165039, 'learning_rate': 0.00029938900203665985, 'epoch': 0.62}
{'loss': 0.4775, 'grad_norm': 33.968570709228516, 'learning_rate': 0.00029925900246999173, 'epoch': 0.62}
{'loss': 0.4755, 'grad_norm': 28.507654190063477, 'learning_rate': 0.00029912900290332366, 'epoch': 0.63}
{'loss': 0.4755, 'grad_norm': 125.8869857788086, 'learning_rate': 0.00029899900333665554, 'epoch': 0.63}
{'loss': 0.4657, 'grad_norm': 42.67948532104492, 'learning_rate': 0.0002988690037699874, 'epoch': 0.64}
{'loss': 0.4933, 'grad_norm': 49.75992202758789, 'learning_rate': 0.0002987390042033193, 'epoch': 0.64}
{'loss': 0.4945, 'grad_norm': 23.329566955566406, 'learning_rate': 0.00029860900463665117, 'epoch': 0.64}
{'loss': 0.5022, 'grad_norm': 25.68758773803711, 'learning_rate': 0.00029847900506998305, 'epoch': 0.65}
{'loss': 0.4772, 'grad_norm': 56.185699462890625, 'learning_rate': 0.000298349005503315, 'epoch': 0.65}
{'loss': 0.4511, 'grad_norm': 29.999391555786133, 'learning_rate': 0.00029821900593664686, 'epoch': 0.66}
{'loss': 0.4869, 'grad_norm': 29.598997116088867, 'learning_rate': 0.00029808900636997873, 'epoch': 0.66}
{'loss': 0.4806, 'grad_norm': 32.81031036376953, 'learning_rate': 0.00029795900680331067, 'epoch': 0.66}
{'loss': 0.504, 'grad_norm': 53.233341217041016, 'learning_rate': 0.00029782900723664254, 'epoch': 0.67}
{'loss': 0.4934, 'grad_norm': 40.71456527709961, 'learning_rate': 0.0002976990076699744, 'epoch': 0.67}
{'loss': 0.4561, 'grad_norm': 23.762351989746094, 'learning_rate': 0.0002975690081033063, 'epoch': 0.68}
{'loss': 0.457, 'grad_norm': 22.745588302612305, 'learning_rate': 0.0002974390085366382, 'epoch': 0.68}
{'loss': 0.5235, 'grad_norm': 46.154701232910156, 'learning_rate': 0.00029730900896997005, 'epoch': 0.68}
{'loss': 0.4901, 'grad_norm': 36.35997772216797, 'learning_rate': 0.00029717900940330193, 'epoch': 0.69}
{'loss': 0.4795, 'grad_norm': 22.23843002319336, 'learning_rate': 0.00029704900983663386, 'epoch': 0.69}
{'loss': 0.5178, 'grad_norm': 28.718881607055664, 'learning_rate': 0.00029691901026996574, 'epoch': 0.7}
{'loss': 0.4647, 'grad_norm': 26.38880729675293, 'learning_rate': 0.0002967890107032976, 'epoch': 0.7}
{'loss': 0.494, 'grad_norm': 36.76798629760742, 'learning_rate': 0.0002966590111366295, 'epoch': 0.7}
{'loss': 0.4568, 'grad_norm': 23.78179359436035, 'learning_rate': 0.0002965290115699614, 'epoch': 0.71}
{'loss': 0.4843, 'grad_norm': 37.27975845336914, 'learning_rate': 0.0002963990120032933, 'epoch': 0.71}
{'loss': 0.5081, 'grad_norm': 30.224491119384766, 'learning_rate': 0.0002962690124366252, 'epoch': 0.72}
{'loss': 0.5068, 'grad_norm': 22.95779037475586, 'learning_rate': 0.00029613901286995706, 'epoch': 0.72}
{'loss': 0.5253, 'grad_norm': 28.61412811279297, 'learning_rate': 0.000296009013303289, 'epoch': 0.73}
{'loss': 0.5226, 'grad_norm': 50.42391586303711, 'learning_rate': 0.00029587901373662087, 'epoch': 0.73}
{'loss': 0.4978, 'grad_norm': 39.02936935424805, 'learning_rate': 0.00029574901416995274, 'epoch': 0.73}
{'loss': 0.4576, 'grad_norm': 85.87913513183594, 'learning_rate': 0.0002956190146032846, 'epoch': 0.74}
{'loss': 0.5048, 'grad_norm': 35.59404373168945, 'learning_rate': 0.0002954890150366165, 'epoch': 0.74}
{'loss': 0.6418, 'grad_norm': 57.12630844116211, 'learning_rate': 0.0002953590154699484, 'epoch': 0.75}
{'loss': 0.5052, 'grad_norm': 46.00872039794922, 'learning_rate': 0.0002952290159032803, 'epoch': 0.75}
{'loss': 0.8337, 'grad_norm': 70.30950164794922, 'learning_rate': 0.0002950990163366122, 'epoch': 0.75}
{'loss': 0.7605, 'grad_norm': 46.839630126953125, 'learning_rate': 0.00029496901676994406, 'epoch': 0.76}
{'loss': 0.5499, 'grad_norm': 128.0758056640625, 'learning_rate': 0.000294839017203276, 'epoch': 0.76}
{'loss': 0.5827, 'grad_norm': 52.31040573120117, 'learning_rate': 0.00029470901763660787, 'epoch': 0.77}
{'loss': 0.5196, 'grad_norm': 29.555007934570312, 'learning_rate': 0.00029457901806993975, 'epoch': 0.77}
{'loss': 0.5068, 'grad_norm': 35.9971923828125, 'learning_rate': 0.00029444901850327163, 'epoch': 0.77}
{'loss': 0.4668, 'grad_norm': 49.247581481933594, 'learning_rate': 0.0002943190189366035, 'epoch': 0.78}
{'loss': 0.4908, 'grad_norm': 32.28853225708008, 'learning_rate': 0.0002941890193699354, 'epoch': 0.78}
{'loss': 0.495, 'grad_norm': 45.20730209350586, 'learning_rate': 0.00029405901980326726, 'epoch': 0.79}
{'loss': 0.504, 'grad_norm': 68.93269348144531, 'learning_rate': 0.0002939290202365992, 'epoch': 0.79}
{'loss': 0.5171, 'grad_norm': 45.13235855102539, 'learning_rate': 0.00029379902066993107, 'epoch': 0.79}
{'loss': 0.5361, 'grad_norm': 39.79774475097656, 'learning_rate': 0.00029366902110326295, 'epoch': 0.8}
{'loss': 0.4859, 'grad_norm': 37.40116882324219, 'learning_rate': 0.0002935390215365949, 'epoch': 0.8}
{'loss': 0.4552, 'grad_norm': 31.105268478393555, 'learning_rate': 0.00029340902196992676, 'epoch': 0.81}
{'loss': 0.4797, 'grad_norm': 43.290645599365234, 'learning_rate': 0.00029327902240325863, 'epoch': 0.81}
{'loss': 0.4649, 'grad_norm': 27.730182647705078, 'learning_rate': 0.0002931490228365905, 'epoch': 0.81}
{'loss': 0.4644, 'grad_norm': 45.150516510009766, 'learning_rate': 0.0002930190232699224, 'epoch': 0.82}
{'loss': 0.503, 'grad_norm': 45.53510665893555, 'learning_rate': 0.0002928890237032543, 'epoch': 0.82}
{'loss': 0.4936, 'grad_norm': 33.423370361328125, 'learning_rate': 0.0002927590241365862, 'epoch': 0.83}
{'loss': 0.4802, 'grad_norm': 34.00967025756836, 'learning_rate': 0.0002926290245699181, 'epoch': 0.83}
{'loss': 0.5389, 'grad_norm': 42.50904846191406, 'learning_rate': 0.00029249902500324995, 'epoch': 0.84}
{'loss': 0.5953, 'grad_norm': 74.58256530761719, 'learning_rate': 0.00029236902543658183, 'epoch': 0.84}
{'loss': 0.5186, 'grad_norm': 59.217185974121094, 'learning_rate': 0.00029223902586991376, 'epoch': 0.84}
{'loss': 0.5173, 'grad_norm': 34.639793395996094, 'learning_rate': 0.00029210902630324564, 'epoch': 0.85}
{'loss': 0.4871, 'grad_norm': 37.33313751220703, 'learning_rate': 0.0002919790267365775, 'epoch': 0.85}
{'loss': 0.5732, 'grad_norm': 38.249210357666016, 'learning_rate': 0.00029184902716990945, 'epoch': 0.86}
{'loss': 0.5173, 'grad_norm': 44.60013198852539, 'learning_rate': 0.0002917190276032413, 'epoch': 0.86}
{'loss': 0.5553, 'grad_norm': 40.68849182128906, 'learning_rate': 0.0002915890280365732, 'epoch': 0.86}
{'loss': 0.519, 'grad_norm': 36.84480667114258, 'learning_rate': 0.0002914590284699051, 'epoch': 0.87}
{'loss': 0.553, 'grad_norm': 129.26800537109375, 'learning_rate': 0.00029132902890323696, 'epoch': 0.87}
{'loss': 0.5408, 'grad_norm': 46.85935592651367, 'learning_rate': 0.00029119902933656883, 'epoch': 0.88}
{'loss': 0.55, 'grad_norm': 36.062255859375, 'learning_rate': 0.0002910690297699007, 'epoch': 0.88}
{'loss': 0.4959, 'grad_norm': 60.45211410522461, 'learning_rate': 0.0002909390302032326, 'epoch': 0.88}
{'loss': 0.5302, 'grad_norm': 53.7371940612793, 'learning_rate': 0.0002908090306365645, 'epoch': 0.89}
{'loss': 0.5248, 'grad_norm': 99.94447326660156, 'learning_rate': 0.0002906790310698964, 'epoch': 0.89}
{'loss': 0.5798, 'grad_norm': 40.514442443847656, 'learning_rate': 0.00029054903150322833, 'epoch': 0.9}
{'loss': 0.5599, 'grad_norm': 32.38037109375, 'learning_rate': 0.0002904190319365602, 'epoch': 0.9}
{'loss': 0.5299, 'grad_norm': 118.16962432861328, 'learning_rate': 0.0002902890323698921, 'epoch': 0.9}
{'loss': 0.5479, 'grad_norm': 351.6002502441406, 'learning_rate': 0.00029015903280322396, 'epoch': 0.91}
{'loss': 0.5167, 'grad_norm': 48.75714111328125, 'learning_rate': 0.00029002903323655584, 'epoch': 0.91}
{'loss': 0.5313, 'grad_norm': 53.26343536376953, 'learning_rate': 0.0002898990336698877, 'epoch': 0.92}
{'loss': 0.6071, 'grad_norm': 86.06481170654297, 'learning_rate': 0.00028976903410321965, 'epoch': 0.92}
{'loss': 0.552, 'grad_norm': 32.88262939453125, 'learning_rate': 0.0002896390345365515, 'epoch': 0.92}
{'loss': 0.4898, 'grad_norm': 39.22136306762695, 'learning_rate': 0.0002895090349698834, 'epoch': 0.93}
{'loss': 0.4789, 'grad_norm': 52.60490036010742, 'learning_rate': 0.0002893790354032153, 'epoch': 0.93}
{'loss': 0.5444, 'grad_norm': 28.512454986572266, 'learning_rate': 0.00028924903583654716, 'epoch': 0.94}
{'loss': 0.5806, 'grad_norm': 56.21112060546875, 'learning_rate': 0.0002891190362698791, 'epoch': 0.94}
{'loss': 0.5663, 'grad_norm': 32.19839859008789, 'learning_rate': 0.00028898903670321097, 'epoch': 0.95}
{'loss': 0.4909, 'grad_norm': 39.4906005859375, 'learning_rate': 0.00028885903713654284, 'epoch': 0.95}
{'loss': 0.5117, 'grad_norm': 94.58047485351562, 'learning_rate': 0.0002887290375698748, 'epoch': 0.95}
{'loss': 0.5165, 'grad_norm': 36.027313232421875, 'learning_rate': 0.00028859903800320665, 'epoch': 0.96}
{'loss': 0.5574, 'grad_norm': 40.55650329589844, 'learning_rate': 0.00028846903843653853, 'epoch': 0.96}
{'loss': 0.5132, 'grad_norm': 57.76079177856445, 'learning_rate': 0.0002883390388698704, 'epoch': 0.97}
{'loss': 0.5461, 'grad_norm': 44.81925582885742, 'learning_rate': 0.0002882090393032023, 'epoch': 0.97}
{'loss': 0.6166, 'grad_norm': 72.17523193359375, 'learning_rate': 0.00028807903973653416, 'epoch': 0.97}
{'loss': 0.5755, 'grad_norm': 59.116947174072266, 'learning_rate': 0.00028794904016986604, 'epoch': 0.98}
{'loss': 0.5576, 'grad_norm': 49.852848052978516, 'learning_rate': 0.00028781904060319797, 'epoch': 0.98}
{'loss': 0.6178, 'grad_norm': 52.716426849365234, 'learning_rate': 0.00028768904103652985, 'epoch': 0.99}
{'loss': 0.5124, 'grad_norm': 44.24049377441406, 'learning_rate': 0.0002875590414698617, 'epoch': 0.99}
{'loss': 0.5637, 'grad_norm': 32.79844284057617, 'learning_rate': 0.00028742904190319366, 'epoch': 0.99}
{'loss': 0.5245, 'grad_norm': 85.81472778320312, 'learning_rate': 0.00028729904233652554, 'epoch': 1.0}
{'eval_loss': 0.4054616391658783, 'eval_accuracy': 0.8475802343352012, 'eval_runtime': 5.4097, 'eval_samples_per_second': 1814.319, 'eval_steps_per_second': 11.461, 'epoch': 1.0}
{'loss': 0.5211, 'grad_norm': 50.648292541503906, 'learning_rate': 0.0002871690427698574, 'epoch': 1.0}
{'loss': 0.5035, 'grad_norm': 32.74430465698242, 'learning_rate': 0.0002870390432031893, 'epoch': 1.01}
{'loss': 0.4864, 'grad_norm': 29.180482864379883, 'learning_rate': 0.00028690904363652117, 'epoch': 1.01}
{'loss': 0.5095, 'grad_norm': 41.97466278076172, 'learning_rate': 0.00028677904406985305, 'epoch': 1.01}
{'loss': 0.4928, 'grad_norm': 31.48866081237793, 'learning_rate': 0.000286649044503185, 'epoch': 1.02}
{'loss': 0.5101, 'grad_norm': 47.262977600097656, 'learning_rate': 0.00028651904493651685, 'epoch': 1.02}
{'loss': 0.5039, 'grad_norm': 51.020790100097656, 'learning_rate': 0.00028638904536984873, 'epoch': 1.03}
{'loss': 0.4683, 'grad_norm': 90.0477294921875, 'learning_rate': 0.0002862590458031806, 'epoch': 1.03}
{'loss': 0.5129, 'grad_norm': 44.62057113647461, 'learning_rate': 0.00028612904623651254, 'epoch': 1.03}
{'loss': 0.5259, 'grad_norm': 55.525115966796875, 'learning_rate': 0.0002859990466698444, 'epoch': 1.04}
{'loss': 0.4869, 'grad_norm': 39.4398193359375, 'learning_rate': 0.0002858690471031763, 'epoch': 1.04}
{'loss': 0.5236, 'grad_norm': 62.507301330566406, 'learning_rate': 0.0002857390475365082, 'epoch': 1.05}
{'loss': 0.5352, 'grad_norm': 56.34059143066406, 'learning_rate': 0.0002856090479698401, 'epoch': 1.05}
{'loss': 0.5252, 'grad_norm': 64.27214813232422, 'learning_rate': 0.000285479048403172, 'epoch': 1.05}
{'loss': 0.5105, 'grad_norm': 48.405582427978516, 'learning_rate': 0.00028534904883650386, 'epoch': 1.06}
{'loss': 0.5391, 'grad_norm': 51.57163619995117, 'learning_rate': 0.00028521904926983574, 'epoch': 1.06}
{'loss': 0.5641, 'grad_norm': 42.53848648071289, 'learning_rate': 0.0002850890497031676, 'epoch': 1.07}
{'loss': 0.5136, 'grad_norm': 43.5255012512207, 'learning_rate': 0.0002849590501364995, 'epoch': 1.07}
{'loss': 0.5826, 'grad_norm': 43.73149871826172, 'learning_rate': 0.00028482905056983137, 'epoch': 1.08}
{'loss': 0.5618, 'grad_norm': 55.04639434814453, 'learning_rate': 0.0002846990510031633, 'epoch': 1.08}
{'loss': 0.7186, 'grad_norm': 62.744449615478516, 'learning_rate': 0.0002845690514364952, 'epoch': 1.08}
{'loss': 0.6036, 'grad_norm': 36.78160858154297, 'learning_rate': 0.0002844390518698271, 'epoch': 1.09}
{'loss': 0.6151, 'grad_norm': 63.87038803100586, 'learning_rate': 0.000284309052303159, 'epoch': 1.09}
{'loss': 0.5183, 'grad_norm': 36.964454650878906, 'learning_rate': 0.00028417905273649086, 'epoch': 1.1}
{'loss': 0.5292, 'grad_norm': 104.5488052368164, 'learning_rate': 0.00028404905316982274, 'epoch': 1.1}
{'loss': 0.6238, 'grad_norm': 46.24842071533203, 'learning_rate': 0.0002839190536031546, 'epoch': 1.1}
{'loss': 0.5489, 'grad_norm': 38.934043884277344, 'learning_rate': 0.0002837890540364865, 'epoch': 1.11}
{'loss': 0.54, 'grad_norm': 48.30662155151367, 'learning_rate': 0.00028365905446981843, 'epoch': 1.11}
{'loss': 0.5433, 'grad_norm': 39.96428680419922, 'learning_rate': 0.0002835290549031503, 'epoch': 1.12}
{'loss': 0.4911, 'grad_norm': 59.219154357910156, 'learning_rate': 0.0002833990553364822, 'epoch': 1.12}
{'loss': 0.4857, 'grad_norm': 67.00346374511719, 'learning_rate': 0.00028326905576981406, 'epoch': 1.12}
{'loss': 0.5637, 'grad_norm': 79.031982421875, 'learning_rate': 0.00028313905620314594, 'epoch': 1.13}
{'loss': 0.6254, 'grad_norm': 48.67348861694336, 'learning_rate': 0.00028300905663647787, 'epoch': 1.13}
{'loss': 0.5879, 'grad_norm': 45.77816390991211, 'learning_rate': 0.00028287905706980975, 'epoch': 1.14}
{'loss': 0.5635, 'grad_norm': 59.299442291259766, 'learning_rate': 0.0002827490575031416, 'epoch': 1.14}
{'loss': 0.5611, 'grad_norm': 69.3881607055664, 'learning_rate': 0.00028261905793647356, 'epoch': 1.14}
{'loss': 0.5335, 'grad_norm': 36.46224594116211, 'learning_rate': 0.00028248905836980543, 'epoch': 1.15}
{'loss': 0.5267, 'grad_norm': 170.62635803222656, 'learning_rate': 0.0002823590588031373, 'epoch': 1.15}
{'loss': 0.5628, 'grad_norm': 140.0185089111328, 'learning_rate': 0.0002822290592364692, 'epoch': 1.16}
{'loss': 0.5844, 'grad_norm': 38.217159271240234, 'learning_rate': 0.00028209905966980107, 'epoch': 1.16}
{'loss': 0.5074, 'grad_norm': 53.67612838745117, 'learning_rate': 0.00028196906010313294, 'epoch': 1.16}
{'loss': 0.5497, 'grad_norm': 34.189109802246094, 'learning_rate': 0.0002818390605364648, 'epoch': 1.17}
{'loss': 0.5513, 'grad_norm': 41.2651481628418, 'learning_rate': 0.00028170906096979675, 'epoch': 1.17}
{'loss': 0.5292, 'grad_norm': 33.06822967529297, 'learning_rate': 0.00028157906140312863, 'epoch': 1.18}
{'loss': 0.5412, 'grad_norm': 41.93929672241211, 'learning_rate': 0.0002814490618364605, 'epoch': 1.18}
{'loss': 0.5599, 'grad_norm': 42.001121520996094, 'learning_rate': 0.00028131906226979244, 'epoch': 1.19}
{'loss': 0.5398, 'grad_norm': 117.029541015625, 'learning_rate': 0.0002811890627031243, 'epoch': 1.19}
{'loss': 0.5495, 'grad_norm': 40.097862243652344, 'learning_rate': 0.0002810590631364562, 'epoch': 1.19}
{'loss': 0.5569, 'grad_norm': 74.5731430053711, 'learning_rate': 0.00028092906356978807, 'epoch': 1.2}
{'loss': 0.5967, 'grad_norm': 65.57632446289062, 'learning_rate': 0.00028079906400311995, 'epoch': 1.2}
{'loss': 0.5267, 'grad_norm': 67.75984191894531, 'learning_rate': 0.0002806690644364518, 'epoch': 1.21}
{'loss': 0.5886, 'grad_norm': 59.26362991333008, 'learning_rate': 0.00028053906486978376, 'epoch': 1.21}
{'loss': 0.5179, 'grad_norm': 47.971275329589844, 'learning_rate': 0.00028040906530311564, 'epoch': 1.21}
{'loss': 0.5453, 'grad_norm': 34.70003128051758, 'learning_rate': 0.0002802790657364475, 'epoch': 1.22}
{'loss': 0.5357, 'grad_norm': 34.19835662841797, 'learning_rate': 0.0002801490661697794, 'epoch': 1.22}
{'loss': 0.5795, 'grad_norm': 76.71209716796875, 'learning_rate': 0.0002800190666031113, 'epoch': 1.23}
{'loss': 0.6392, 'grad_norm': 43.566864013671875, 'learning_rate': 0.0002798890670364432, 'epoch': 1.23}
{'loss': 0.5893, 'grad_norm': 110.10235595703125, 'learning_rate': 0.0002797590674697751, 'epoch': 1.23}
{'loss': 0.6224, 'grad_norm': 53.4791259765625, 'learning_rate': 0.00027962906790310695, 'epoch': 1.24}
{'loss': 0.5836, 'grad_norm': 57.52212905883789, 'learning_rate': 0.0002794990683364389, 'epoch': 1.24}
{'loss': 0.5792, 'grad_norm': 82.37520599365234, 'learning_rate': 0.00027936906876977076, 'epoch': 1.25}
{'loss': 0.594, 'grad_norm': 63.08879852294922, 'learning_rate': 0.00027923906920310264, 'epoch': 1.25}
{'loss': 0.5717, 'grad_norm': 42.06730270385742, 'learning_rate': 0.0002791090696364345, 'epoch': 1.25}
{'loss': 0.5562, 'grad_norm': 84.20804595947266, 'learning_rate': 0.0002789790700697664, 'epoch': 1.26}
{'loss': 0.5336, 'grad_norm': 52.880897521972656, 'learning_rate': 0.00027884907050309827, 'epoch': 1.26}
{'loss': 0.595, 'grad_norm': 50.14780044555664, 'learning_rate': 0.0002787190709364302, 'epoch': 1.27}
{'loss': 0.5431, 'grad_norm': 57.94069290161133, 'learning_rate': 0.0002785890713697621, 'epoch': 1.27}
{'loss': 0.5408, 'grad_norm': 36.84412384033203, 'learning_rate': 0.00027845907180309396, 'epoch': 1.27}
{'loss': 0.5027, 'grad_norm': 62.743202209472656, 'learning_rate': 0.0002783290722364259, 'epoch': 1.28}
{'loss': 0.5584, 'grad_norm': 55.69007873535156, 'learning_rate': 0.00027819907266975777, 'epoch': 1.28}
{'loss': 0.5806, 'grad_norm': 58.4109992980957, 'learning_rate': 0.00027806907310308965, 'epoch': 1.29}
{'loss': 0.5933, 'grad_norm': 68.57217407226562, 'learning_rate': 0.0002779390735364215, 'epoch': 1.29}
{'loss': 0.5632, 'grad_norm': 47.2661247253418, 'learning_rate': 0.0002778090739697534, 'epoch': 1.3}
{'loss': 0.5833, 'grad_norm': 45.827537536621094, 'learning_rate': 0.0002776790744030853, 'epoch': 1.3}
{'loss': 0.5354, 'grad_norm': 108.23594665527344, 'learning_rate': 0.00027754907483641716, 'epoch': 1.3}
{'loss': 0.606, 'grad_norm': 85.67884063720703, 'learning_rate': 0.0002774190752697491, 'epoch': 1.31}
{'loss': 0.5445, 'grad_norm': 38.32168197631836, 'learning_rate': 0.00027728907570308096, 'epoch': 1.31}
{'loss': 0.548, 'grad_norm': 45.684017181396484, 'learning_rate': 0.00027715907613641284, 'epoch': 1.32}
{'loss': 0.5232, 'grad_norm': 46.9032096862793, 'learning_rate': 0.0002770290765697447, 'epoch': 1.32}
{'loss': 0.6945, 'grad_norm': 95.16024017333984, 'learning_rate': 0.00027689907700307665, 'epoch': 1.32}
{'loss': 0.7841, 'grad_norm': 48.092288970947266, 'learning_rate': 0.00027676907743640853, 'epoch': 1.33}
{'loss': 0.8147, 'grad_norm': 90.55669403076172, 'learning_rate': 0.0002766390778697404, 'epoch': 1.33}
{'loss': 0.9111, 'grad_norm': 43.40077590942383, 'learning_rate': 0.0002765090783030723, 'epoch': 1.34}
{'loss': 0.8805, 'grad_norm': 59.41244125366211, 'learning_rate': 0.0002763790787364042, 'epoch': 1.34}
{'loss': 0.89, 'grad_norm': 40.16267395019531, 'learning_rate': 0.0002762490791697361, 'epoch': 1.34}
{'loss': 0.9542, 'grad_norm': 49.129234313964844, 'learning_rate': 0.00027611907960306797, 'epoch': 1.35}
{'loss': 0.9919, 'grad_norm': 80.69981384277344, 'learning_rate': 0.00027598908003639985, 'epoch': 1.35}
{'loss': 0.9818, 'grad_norm': 41.51716232299805, 'learning_rate': 0.0002758590804697317, 'epoch': 1.36}
{'loss': 0.932, 'grad_norm': 29.969480514526367, 'learning_rate': 0.0002757290809030636, 'epoch': 1.36}
{'loss': 0.9104, 'grad_norm': 33.82656478881836, 'learning_rate': 0.00027559908133639553, 'epoch': 1.36}
{'loss': 1.0311, 'grad_norm': 29.40763282775879, 'learning_rate': 0.0002754690817697274, 'epoch': 1.37}
{'loss': 1.0302, 'grad_norm': 30.680940628051758, 'learning_rate': 0.0002753390822030593, 'epoch': 1.37}
{'loss': 0.9937, 'grad_norm': 239.66375732421875, 'learning_rate': 0.0002752090826363912, 'epoch': 1.38}
{'loss': 0.9621, 'grad_norm': 40.90361022949219, 'learning_rate': 0.0002750790830697231, 'epoch': 1.38}
{'loss': 0.9368, 'grad_norm': 45.751220703125, 'learning_rate': 0.000274949083503055, 'epoch': 1.38}
{'loss': 0.9359, 'grad_norm': 58.723209381103516, 'learning_rate': 0.00027481908393638685, 'epoch': 1.39}
{'loss': 0.9788, 'grad_norm': 30.765003204345703, 'learning_rate': 0.00027468908436971873, 'epoch': 1.39}
{'loss': 1.0161, 'grad_norm': 44.79045867919922, 'learning_rate': 0.0002745590848030506, 'epoch': 1.4}
{'loss': 1.0128, 'grad_norm': 81.28422546386719, 'learning_rate': 0.0002744290852363825, 'epoch': 1.4}
{'loss': 1.1026, 'grad_norm': 3.995145082473755, 'learning_rate': 0.0002742990856697144, 'epoch': 1.41}
{'loss': 1.1375, 'grad_norm': 5.703722953796387, 'learning_rate': 0.0002741690861030463, 'epoch': 1.41}
{'loss': 1.1337, 'grad_norm': 3.4932873249053955, 'learning_rate': 0.00027403908653637817, 'epoch': 1.41}
{'loss': 1.1074, 'grad_norm': 1.5290398597717285, 'learning_rate': 0.0002739090869697101, 'epoch': 1.42}
{'loss': 1.157, 'grad_norm': 3.1747488975524902, 'learning_rate': 0.000273779087403042, 'epoch': 1.42}
{'loss': 1.1547, 'grad_norm': 3.632075071334839, 'learning_rate': 0.00027364908783637386, 'epoch': 1.43}
{'loss': 1.1285, 'grad_norm': 1.0177220106124878, 'learning_rate': 0.00027351908826970573, 'epoch': 1.43}
{'loss': 1.1129, 'grad_norm': 1.2879589796066284, 'learning_rate': 0.0002733890887030376, 'epoch': 1.43}
{'loss': 1.1156, 'grad_norm': 1.7936534881591797, 'learning_rate': 0.00027325908913636954, 'epoch': 1.44}
{'loss': 1.1131, 'grad_norm': 0.4993578791618347, 'learning_rate': 0.0002731290895697014, 'epoch': 1.44}
{'loss': 1.109, 'grad_norm': 1.3958195447921753, 'learning_rate': 0.0002729990900030333, 'epoch': 1.45}
{'loss': 1.1116, 'grad_norm': 1.5939525365829468, 'learning_rate': 0.0002728690904363652, 'epoch': 1.45}
{'loss': 1.1174, 'grad_norm': 1.5058073997497559, 'learning_rate': 0.00027273909086969705, 'epoch': 1.45}
{'loss': 1.1203, 'grad_norm': 1.0595799684524536, 'learning_rate': 0.000272609091303029, 'epoch': 1.46}
{'loss': 1.1136, 'grad_norm': 3.3682029247283936, 'learning_rate': 0.00027247909173636086, 'epoch': 1.46}
{'loss': 1.1127, 'grad_norm': 11.727683067321777, 'learning_rate': 0.00027234909216969274, 'epoch': 1.47}
{'loss': 1.1092, 'grad_norm': 5.394418239593506, 'learning_rate': 0.00027221909260302467, 'epoch': 1.47}
{'loss': 1.1037, 'grad_norm': 0.8623189330101013, 'learning_rate': 0.00027208909303635655, 'epoch': 1.47}
{'loss': 1.1081, 'grad_norm': 47.692813873291016, 'learning_rate': 0.0002719590934696884, 'epoch': 1.48}
{'loss': 1.106, 'grad_norm': 2.640857458114624, 'learning_rate': 0.0002718290939030203, 'epoch': 1.48}
{'loss': 1.1029, 'grad_norm': 2.33146071434021, 'learning_rate': 0.0002716990943363522, 'epoch': 1.49}
{'loss': 1.1116, 'grad_norm': 0.7625966668128967, 'learning_rate': 0.00027156909476968406, 'epoch': 1.49}
{'loss': 1.1077, 'grad_norm': 1.7722785472869873, 'learning_rate': 0.00027143909520301594, 'epoch': 1.49}
{'loss': 1.1044, 'grad_norm': 0.8032447099685669, 'learning_rate': 0.0002713090956363478, 'epoch': 1.5}
{'loss': 1.1041, 'grad_norm': 0.6262878179550171, 'learning_rate': 0.00027117909606967975, 'epoch': 1.5}
{'loss': 1.1065, 'grad_norm': 1.3136624097824097, 'learning_rate': 0.0002710490965030116, 'epoch': 1.51}
{'loss': 1.1029, 'grad_norm': 4.443220138549805, 'learning_rate': 0.0002709190969363435, 'epoch': 1.51}
{'loss': 1.1115, 'grad_norm': 1.54642915725708, 'learning_rate': 0.00027078909736967543, 'epoch': 1.52}
{'loss': 1.1071, 'grad_norm': 1.4214571714401245, 'learning_rate': 0.0002706590978030073, 'epoch': 1.52}
{'loss': 1.1134, 'grad_norm': 1.437788486480713, 'learning_rate': 0.0002705290982363392, 'epoch': 1.52}
{'loss': 1.1043, 'grad_norm': 4.160828113555908, 'learning_rate': 0.00027039909866967106, 'epoch': 1.53}
{'loss': 1.1027, 'grad_norm': 3.408870220184326, 'learning_rate': 0.00027026909910300294, 'epoch': 1.53}
{'loss': 1.1055, 'grad_norm': 1.9724864959716797, 'learning_rate': 0.0002701390995363349, 'epoch': 1.54}
{'loss': 1.1049, 'grad_norm': 23.288715362548828, 'learning_rate': 0.00027000909996966675, 'epoch': 1.54}
{'loss': 1.1044, 'grad_norm': 10.818875312805176, 'learning_rate': 0.00026987910040299863, 'epoch': 1.54}
{'loss': 1.1045, 'grad_norm': 1.9731113910675049, 'learning_rate': 0.0002697491008363305, 'epoch': 1.55}
{'loss': 1.1092, 'grad_norm': 0.8141428232192993, 'learning_rate': 0.0002696191012696624, 'epoch': 1.55}
{'loss': 1.1046, 'grad_norm': 1.5543662309646606, 'learning_rate': 0.0002694891017029943, 'epoch': 1.56}
{'loss': 1.1041, 'grad_norm': 10.952117919921875, 'learning_rate': 0.0002693591021363262, 'epoch': 1.56}
{'loss': 1.1035, 'grad_norm': 2.361955165863037, 'learning_rate': 0.00026922910256965807, 'epoch': 1.56}
{'loss': 1.1066, 'grad_norm': 3.915707588195801, 'learning_rate': 0.00026909910300299, 'epoch': 1.57}
{'loss': 1.1048, 'grad_norm': 3.398142099380493, 'learning_rate': 0.0002689691034363219, 'epoch': 1.57}
{'loss': 1.1028, 'grad_norm': 1.3936110734939575, 'learning_rate': 0.00026883910386965376, 'epoch': 1.58}
{'loss': 1.1042, 'grad_norm': 4.118357181549072, 'learning_rate': 0.00026870910430298563, 'epoch': 1.58}
{'loss': 1.102, 'grad_norm': 0.675154447555542, 'learning_rate': 0.0002685791047363175, 'epoch': 1.58}
{'loss': 1.1069, 'grad_norm': 1.0022578239440918, 'learning_rate': 0.0002684491051696494, 'epoch': 1.59}
{'loss': 1.1055, 'grad_norm': 0.9588037133216858, 'learning_rate': 0.00026831910560298127, 'epoch': 1.59}
{'loss': 1.1021, 'grad_norm': 10.040003776550293, 'learning_rate': 0.0002681891060363132, 'epoch': 1.6}
{'loss': 1.1045, 'grad_norm': 1.5333738327026367, 'learning_rate': 0.0002680591064696451, 'epoch': 1.6}
{'loss': 1.1015, 'grad_norm': 3.4808237552642822, 'learning_rate': 0.00026792910690297695, 'epoch': 1.6}
{'loss': 1.1079, 'grad_norm': 1.6359994411468506, 'learning_rate': 0.0002677991073363089, 'epoch': 1.61}
{'loss': 1.1039, 'grad_norm': 1.3549647331237793, 'learning_rate': 0.00026766910776964076, 'epoch': 1.61}
{'loss': 1.1016, 'grad_norm': 1.504790186882019, 'learning_rate': 0.00026753910820297264, 'epoch': 1.62}
{'loss': 1.1027, 'grad_norm': 2.1901493072509766, 'learning_rate': 0.0002674091086363045, 'epoch': 1.62}
{'loss': 1.1019, 'grad_norm': 2.77828311920166, 'learning_rate': 0.0002672791090696364, 'epoch': 1.63}
{'loss': 1.0993, 'grad_norm': 0.7375189661979675, 'learning_rate': 0.00026714910950296827, 'epoch': 1.63}
{'loss': 1.1049, 'grad_norm': 2.5718555450439453, 'learning_rate': 0.0002670191099363002, 'epoch': 1.63}
{'loss': 1.1022, 'grad_norm': 0.9374787211418152, 'learning_rate': 0.0002668891103696321, 'epoch': 1.64}
{'loss': 1.1032, 'grad_norm': 3.852910041809082, 'learning_rate': 0.00026675911080296396, 'epoch': 1.64}
{'loss': 1.1072, 'grad_norm': 0.9250544309616089, 'learning_rate': 0.00026662911123629583, 'epoch': 1.65}
{'loss': 1.1037, 'grad_norm': 4.298912048339844, 'learning_rate': 0.00026649911166962777, 'epoch': 1.65}
{'loss': 1.1105, 'grad_norm': 1.3807215690612793, 'learning_rate': 0.00026636911210295964, 'epoch': 1.65}
{'loss': 1.1014, 'grad_norm': 1.401562213897705, 'learning_rate': 0.0002662391125362915, 'epoch': 1.66}
{'loss': 1.0991, 'grad_norm': 1.138898491859436, 'learning_rate': 0.0002661091129696234, 'epoch': 1.66}
{'loss': 1.0982, 'grad_norm': 0.9908941984176636, 'learning_rate': 0.00026597911340295533, 'epoch': 1.67}
{'loss': 1.1096, 'grad_norm': 4.605405330657959, 'learning_rate': 0.0002658491138362872, 'epoch': 1.67}
{'loss': 1.1063, 'grad_norm': 0.5963995456695557, 'learning_rate': 0.0002657191142696191, 'epoch': 1.67}
{'loss': 1.1014, 'grad_norm': 9.261491775512695, 'learning_rate': 0.00026558911470295096, 'epoch': 1.68}
{'loss': 1.1053, 'grad_norm': 1.6611157655715942, 'learning_rate': 0.00026545911513628284, 'epoch': 1.68}
{'loss': 1.1014, 'grad_norm': 5.357351303100586, 'learning_rate': 0.0002653291155696147, 'epoch': 1.69}
{'loss': 1.1026, 'grad_norm': 0.9166000485420227, 'learning_rate': 0.0002651991160029466, 'epoch': 1.69}
{'loss': 1.1054, 'grad_norm': 0.6855928301811218, 'learning_rate': 0.0002650691164362785, 'epoch': 1.69}
{'loss': 1.1035, 'grad_norm': 1.8022211790084839, 'learning_rate': 0.0002649391168696104, 'epoch': 1.7}
{'loss': 1.1071, 'grad_norm': 37.86958312988281, 'learning_rate': 0.00026480911730294233, 'epoch': 1.7}
{'loss': 1.1041, 'grad_norm': 1.3864243030548096, 'learning_rate': 0.0002646791177362742, 'epoch': 1.71}
{'loss': 1.1019, 'grad_norm': 14.295038223266602, 'learning_rate': 0.0002645491181696061, 'epoch': 1.71}
{'loss': 1.1032, 'grad_norm': 0.9151926040649414, 'learning_rate': 0.00026441911860293797, 'epoch': 1.71}
{'loss': 1.0981, 'grad_norm': 25.363685607910156, 'learning_rate': 0.00026428911903626984, 'epoch': 1.72}
{'loss': 1.1022, 'grad_norm': 2.8141024112701416, 'learning_rate': 0.0002641591194696017, 'epoch': 1.72}
{'loss': 1.0982, 'grad_norm': 0.873586893081665, 'learning_rate': 0.00026402911990293365, 'epoch': 1.73}
{'loss': 1.102, 'grad_norm': 4.691591739654541, 'learning_rate': 0.00026389912033626553, 'epoch': 1.73}
{'loss': 1.1027, 'grad_norm': 20.84629249572754, 'learning_rate': 0.0002637691207695974, 'epoch': 1.74}
{'loss': 1.1013, 'grad_norm': 15.19812297821045, 'learning_rate': 0.0002636391212029293, 'epoch': 1.74}
{'loss': 1.1044, 'grad_norm': 1.0596786737442017, 'learning_rate': 0.00026350912163626116, 'epoch': 1.74}
{'loss': 1.102, 'grad_norm': 7.8696513175964355, 'learning_rate': 0.0002633791220695931, 'epoch': 1.75}
{'loss': 1.1016, 'grad_norm': 1.7471026182174683, 'learning_rate': 0.00026324912250292497, 'epoch': 1.75}
{'loss': 1.1001, 'grad_norm': 1.1669565439224243, 'learning_rate': 0.00026311912293625685, 'epoch': 1.76}
{'loss': 1.1046, 'grad_norm': 2.537980794906616, 'learning_rate': 0.0002629891233695888, 'epoch': 1.76}
{'loss': 1.0994, 'grad_norm': 2.1765098571777344, 'learning_rate': 0.00026285912380292066, 'epoch': 1.76}
{'loss': 1.1006, 'grad_norm': 2.0606844425201416, 'learning_rate': 0.00026272912423625254, 'epoch': 1.77}
{'loss': 1.1031, 'grad_norm': 1.4722387790679932, 'learning_rate': 0.0002625991246695844, 'epoch': 1.77}
{'loss': 1.1016, 'grad_norm': 5.923036575317383, 'learning_rate': 0.0002624691251029163, 'epoch': 1.78}
{'loss': 1.0974, 'grad_norm': 11.353403091430664, 'learning_rate': 0.00026233912553624817, 'epoch': 1.78}
{'loss': 1.1002, 'grad_norm': 1.2069047689437866, 'learning_rate': 0.00026220912596958005, 'epoch': 1.78}
{'loss': 1.1003, 'grad_norm': 29.627193450927734, 'learning_rate': 0.000262079126402912, 'epoch': 1.79}
{'loss': 1.0994, 'grad_norm': 19.43914794921875, 'learning_rate': 0.00026194912683624385, 'epoch': 1.79}
{'loss': 1.102, 'grad_norm': 6.1363935470581055, 'learning_rate': 0.00026181912726957573, 'epoch': 1.8}
{'loss': 1.1014, 'grad_norm': 6.021264553070068, 'learning_rate': 0.00026168912770290766, 'epoch': 1.8}
{'loss': 1.0994, 'grad_norm': 1.887960433959961, 'learning_rate': 0.00026155912813623954, 'epoch': 1.8}
{'loss': 1.1016, 'grad_norm': 35.17216491699219, 'learning_rate': 0.0002614291285695714, 'epoch': 1.81}
{'loss': 1.0999, 'grad_norm': 41.013893127441406, 'learning_rate': 0.0002612991290029033, 'epoch': 1.81}
{'loss': 1.1012, 'grad_norm': 2.6192426681518555, 'learning_rate': 0.0002611691294362352, 'epoch': 1.82}
{'loss': 1.097, 'grad_norm': 740.4945068359375, 'learning_rate': 0.00026103912986956705, 'epoch': 1.82}
{'loss': 1.1059, 'grad_norm': 979.9971923828125, 'learning_rate': 0.000260909130302899, 'epoch': 1.82}
{'loss': 1.1049, 'grad_norm': 32511.25, 'learning_rate': 0.00026077913073623086, 'epoch': 1.83}
{'loss': 1.106, 'grad_norm': 97.95066833496094, 'learning_rate': 0.00026064913116956274, 'epoch': 1.83}
{'loss': 1.0987, 'grad_norm': 16.215511322021484, 'learning_rate': 0.0002605191316028946, 'epoch': 1.84}
{'loss': 1.0999, 'grad_norm': 56.91395568847656, 'learning_rate': 0.00026038913203622655, 'epoch': 1.84}
{'loss': 1.1015, 'grad_norm': 10.26072883605957, 'learning_rate': 0.0002602591324695584, 'epoch': 1.85}
{'loss': 1.0997, 'grad_norm': 28.27490997314453, 'learning_rate': 0.0002601291329028903, 'epoch': 1.85}
{'loss': 1.1064, 'grad_norm': 13.837207794189453, 'learning_rate': 0.0002599991333362222, 'epoch': 1.85}
{'loss': 1.1017, 'grad_norm': 9.191224098205566, 'learning_rate': 0.0002598691337695541, 'epoch': 1.86}
{'loss': 1.1002, 'grad_norm': 19.198535919189453, 'learning_rate': 0.000259739134202886, 'epoch': 1.86}
{'loss': 1.0973, 'grad_norm': 13.758766174316406, 'learning_rate': 0.00025960913463621787, 'epoch': 1.87}
{'loss': 1.0986, 'grad_norm': 5.0136237144470215, 'learning_rate': 0.00025947913506954974, 'epoch': 1.87}
{'loss': 1.1028, 'grad_norm': 222.96107482910156, 'learning_rate': 0.0002593491355028816, 'epoch': 1.87}
{'loss': 1.0996, 'grad_norm': 35.24295425415039, 'learning_rate': 0.0002592191359362135, 'epoch': 1.88}
{'loss': 1.0992, 'grad_norm': 4.308363914489746, 'learning_rate': 0.00025908913636954543, 'epoch': 1.88}
{'loss': 1.1025, 'grad_norm': 83.82771301269531, 'learning_rate': 0.0002589591368028773, 'epoch': 1.89}
{'loss': 1.0993, 'grad_norm': 7.82211971282959, 'learning_rate': 0.0002588291372362092, 'epoch': 1.89}
{'loss': 1.1014, 'grad_norm': 16.116943359375, 'learning_rate': 0.0002586991376695411, 'epoch': 1.89}
{'loss': 1.0987, 'grad_norm': 372.56427001953125, 'learning_rate': 0.000258569138102873, 'epoch': 1.9}
{'loss': 1.1029, 'grad_norm': 31.05954360961914, 'learning_rate': 0.00025843913853620487, 'epoch': 1.9}
{'loss': 1.1024, 'grad_norm': 1021.2860107421875, 'learning_rate': 0.00025830913896953675, 'epoch': 1.91}
{'loss': 1.1008, 'grad_norm': 3.819783926010132, 'learning_rate': 0.0002581791394028686, 'epoch': 1.91}
{'loss': 1.1036, 'grad_norm': 9.023625373840332, 'learning_rate': 0.0002580491398362005, 'epoch': 1.91}
{'loss': 1.1011, 'grad_norm': 7.012494087219238, 'learning_rate': 0.0002579191402695324, 'epoch': 1.92}
{'loss': 1.0985, 'grad_norm': 3.8376004695892334, 'learning_rate': 0.0002577891407028643, 'epoch': 1.92}
{'loss': 1.1003, 'grad_norm': 20.469764709472656, 'learning_rate': 0.0002576591411361962, 'epoch': 1.93}
{'loss': 1.0991, 'grad_norm': 9.133573532104492, 'learning_rate': 0.00025752914156952807, 'epoch': 1.93}
{'loss': 1.0993, 'grad_norm': 131.6238250732422, 'learning_rate': 0.00025739914200285994, 'epoch': 1.93}
{'loss': 1.1033, 'grad_norm': 66.13394165039062, 'learning_rate': 0.0002572691424361919, 'epoch': 1.94}
{'loss': 1.0979, 'grad_norm': 81.08216857910156, 'learning_rate': 0.00025713914286952375, 'epoch': 1.94}
{'loss': 1.1095, 'grad_norm': 3.759060859680176, 'learning_rate': 0.00025700914330285563, 'epoch': 1.95}
{'loss': 1.1002, 'grad_norm': 185.6813507080078, 'learning_rate': 0.0002568791437361875, 'epoch': 1.95}
{'loss': 1.1033, 'grad_norm': 1808.8160400390625, 'learning_rate': 0.00025674914416951944, 'epoch': 1.96}
{'loss': 1.1019, 'grad_norm': 45.581787109375, 'learning_rate': 0.0002566191446028513, 'epoch': 1.96}
{'loss': 1.1006, 'grad_norm': 35.427799224853516, 'learning_rate': 0.0002564891450361832, 'epoch': 1.96}
{'loss': 1.1006, 'grad_norm': 127.5407943725586, 'learning_rate': 0.00025635914546951507, 'epoch': 1.97}
{'loss': 1.0995, 'grad_norm': 18.087987899780273, 'learning_rate': 0.00025622914590284695, 'epoch': 1.97}
{'loss': 1.0993, 'grad_norm': 15.758035659790039, 'learning_rate': 0.0002560991463361788, 'epoch': 1.98}
{'loss': 1.1, 'grad_norm': 193.07220458984375, 'learning_rate': 0.00025596914676951076, 'epoch': 1.98}
{'loss': 1.0997, 'grad_norm': 2.687934637069702, 'learning_rate': 0.00025583914720284264, 'epoch': 1.98}
{'loss': 1.0996, 'grad_norm': 11.235685348510742, 'learning_rate': 0.0002557091476361745, 'epoch': 1.99}
{'loss': 1.1028, 'grad_norm': 5.190185070037842, 'learning_rate': 0.00025557914806950644, 'epoch': 1.99}
{'loss': 1.0996, 'grad_norm': 36.35103225708008, 'learning_rate': 0.0002554491485028383, 'epoch': 2.0}
{'loss': 1.1, 'grad_norm': 131.396728515625, 'learning_rate': 0.0002553191489361702, 'epoch': 2.0}
{'eval_loss': 1.0994211435317993, 'eval_accuracy': 0.3273560876209883, 'eval_runtime': 5.4587, 'eval_samples_per_second': 1798.061, 'eval_steps_per_second': 11.358, 'epoch': 2.0}
{'loss': 1.0966, 'grad_norm': 15.577163696289062, 'learning_rate': 0.0002551891493695021, 'epoch': 2.0}
{'loss': 1.098, 'grad_norm': 12.270951271057129, 'learning_rate': 0.00025505914980283395, 'epoch': 2.01}
{'loss': 1.0995, 'grad_norm': 237.83148193359375, 'learning_rate': 0.00025492915023616583, 'epoch': 2.01}
{'loss': 1.0972, 'grad_norm': 7905.89599609375, 'learning_rate': 0.0002547991506694977, 'epoch': 2.02}
{'loss': 1.1002, 'grad_norm': 42.54232406616211, 'learning_rate': 0.00025466915110282964, 'epoch': 2.02}
{'loss': 1.1007, 'grad_norm': 59.746971130371094, 'learning_rate': 0.0002545391515361615, 'epoch': 2.02}
{'loss': 1.0991, 'grad_norm': 44.74367904663086, 'learning_rate': 0.0002544091519694934, 'epoch': 2.03}
{'loss': 1.1031, 'grad_norm': 92.23728942871094, 'learning_rate': 0.00025427915240282533, 'epoch': 2.03}
{'loss': 1.0999, 'grad_norm': 1103.7276611328125, 'learning_rate': 0.0002541491528361572, 'epoch': 2.04}
{'loss': 1.0973, 'grad_norm': 262.5593566894531, 'learning_rate': 0.0002540191532694891, 'epoch': 2.04}
{'loss': 1.0996, 'grad_norm': 1053.283203125, 'learning_rate': 0.00025388915370282096, 'epoch': 2.04}
{'loss': 1.0988, 'grad_norm': 180.50294494628906, 'learning_rate': 0.00025375915413615284, 'epoch': 2.05}
{'loss': 1.0967, 'grad_norm': 16.602806091308594, 'learning_rate': 0.00025362915456948477, 'epoch': 2.05}
{'loss': 1.0992, 'grad_norm': 1394.9879150390625, 'learning_rate': 0.00025349915500281665, 'epoch': 2.06}
{'loss': 1.0982, 'grad_norm': 22.31301498413086, 'learning_rate': 0.0002533691554361485, 'epoch': 2.06}
{'loss': 1.099, 'grad_norm': 15.301161766052246, 'learning_rate': 0.0002532391558694804, 'epoch': 2.07}
{'loss': 1.0963, 'grad_norm': 26.487194061279297, 'learning_rate': 0.0002531091563028123, 'epoch': 2.07}
{'loss': 1.1013, 'grad_norm': 100.19486236572266, 'learning_rate': 0.0002529791567361442, 'epoch': 2.07}
{'loss': 1.0996, 'grad_norm': 974.9102783203125, 'learning_rate': 0.0002528491571694761, 'epoch': 2.08}
{'loss': 1.1006, 'grad_norm': 6944.62353515625, 'learning_rate': 0.00025271915760280796, 'epoch': 2.08}
{'loss': 1.1013, 'grad_norm': 128.08152770996094, 'learning_rate': 0.0002525891580361399, 'epoch': 2.09}
{'loss': 1.1012, 'grad_norm': 12.874397277832031, 'learning_rate': 0.0002524591584694718, 'epoch': 2.09}
{'loss': 1.1009, 'grad_norm': 15.035009384155273, 'learning_rate': 0.00025232915890280365, 'epoch': 2.09}
{'loss': 1.0987, 'grad_norm': 3216.5537109375, 'learning_rate': 0.00025219915933613553, 'epoch': 2.1}
{'loss': 1.1006, 'grad_norm': 604.9495849609375, 'learning_rate': 0.0002520691597694674, 'epoch': 2.1}
{'loss': 1.0998, 'grad_norm': 88.59261322021484, 'learning_rate': 0.0002519391602027993, 'epoch': 2.11}
{'loss': 1.0998, 'grad_norm': 408.8574523925781, 'learning_rate': 0.00025180916063613116, 'epoch': 2.11}
{'loss': 1.1012, 'grad_norm': 41.71364212036133, 'learning_rate': 0.00025167916106946304, 'epoch': 2.11}
{'loss': 1.1006, 'grad_norm': 25.722318649291992, 'learning_rate': 0.00025154916150279497, 'epoch': 2.12}
{'loss': 1.0976, 'grad_norm': 214.86520385742188, 'learning_rate': 0.00025141916193612685, 'epoch': 2.12}
{'loss': 1.0998, 'grad_norm': 49.27553176879883, 'learning_rate': 0.0002512891623694587, 'epoch': 2.13}
{'loss': 1.099, 'grad_norm': 34.36345291137695, 'learning_rate': 0.00025115916280279066, 'epoch': 2.13}
{'loss': 1.0993, 'grad_norm': 155.61924743652344, 'learning_rate': 0.00025102916323612253, 'epoch': 2.13}
{'loss': 1.1, 'grad_norm': 659.182861328125, 'learning_rate': 0.0002508991636694544, 'epoch': 2.14}
{'loss': 1.1017, 'grad_norm': 132.57843017578125, 'learning_rate': 0.0002507691641027863, 'epoch': 2.14}
{'loss': 1.0982, 'grad_norm': 6235.3134765625, 'learning_rate': 0.00025063916453611817, 'epoch': 2.15}
{'loss': 1.1008, 'grad_norm': 21.414155960083008, 'learning_rate': 0.0002505091649694501, 'epoch': 2.15}
{'loss': 1.1011, 'grad_norm': 36.758827209472656, 'learning_rate': 0.000250379165402782, 'epoch': 2.15}
{'loss': 1.1003, 'grad_norm': 1764.615478515625, 'learning_rate': 0.00025024916583611385, 'epoch': 2.16}
{'loss': 1.1002, 'grad_norm': 18210.9609375, 'learning_rate': 0.00025011916626944573, 'epoch': 2.16}
{'loss': 1.0994, 'grad_norm': 35.833709716796875, 'learning_rate': 0.0002499891667027776, 'epoch': 2.17}
{'loss': 1.0997, 'grad_norm': 1187.1019287109375, 'learning_rate': 0.00024985916713610954, 'epoch': 2.17}
{'loss': 1.0998, 'grad_norm': 23.566234588623047, 'learning_rate': 0.0002497291675694414, 'epoch': 2.18}
{'loss': 1.1007, 'grad_norm': 13210.6201171875, 'learning_rate': 0.0002495991680027733, 'epoch': 2.18}
{'loss': 1.0976, 'grad_norm': 77.4214859008789, 'learning_rate': 0.0002494691684361052, 'epoch': 2.18}
{'loss': 1.099, 'grad_norm': 36869.23828125, 'learning_rate': 0.0002493391688694371, 'epoch': 2.19}
{'loss': 1.0999, 'grad_norm': 732.7218627929688, 'learning_rate': 0.000249209169302769, 'epoch': 2.19}
{'loss': 1.1003, 'grad_norm': 103.96385192871094, 'learning_rate': 0.00024907916973610086, 'epoch': 2.2}
{'loss': 1.1001, 'grad_norm': 31.59038543701172, 'learning_rate': 0.00024894917016943274, 'epoch': 2.2}
{'loss': 1.0998, 'grad_norm': 456.69744873046875, 'learning_rate': 0.0002488191706027646, 'epoch': 2.2}
{'loss': 1.1027, 'grad_norm': 39.23798370361328, 'learning_rate': 0.0002486891710360965, 'epoch': 2.21}
{'loss': 1.0987, 'grad_norm': 94.69625854492188, 'learning_rate': 0.0002485591714694284, 'epoch': 2.21}
{'loss': 1.0993, 'grad_norm': 64.61753845214844, 'learning_rate': 0.0002484291719027603, 'epoch': 2.22}
{'loss': 1.0989, 'grad_norm': 843.6586303710938, 'learning_rate': 0.0002482991723360922, 'epoch': 2.22}
{'loss': 1.0989, 'grad_norm': 78.85074615478516, 'learning_rate': 0.0002481691727694241, 'epoch': 2.22}
{'loss': 1.1007, 'grad_norm': 7.793815612792969, 'learning_rate': 0.000248039173202756, 'epoch': 2.23}
{'loss': 1.1001, 'grad_norm': 171.84661865234375, 'learning_rate': 0.00024790917363608786, 'epoch': 2.23}
{'loss': 1.0977, 'grad_norm': 7422.2822265625, 'learning_rate': 0.00024777917406941974, 'epoch': 2.24}
{'loss': 1.1021, 'grad_norm': 354.91180419921875, 'learning_rate': 0.0002476491745027516, 'epoch': 2.24}
{'loss': 1.1021, 'grad_norm': 72.77702331542969, 'learning_rate': 0.0002475191749360835, 'epoch': 2.24}
{'loss': 1.0982, 'grad_norm': 372.67547607421875, 'learning_rate': 0.0002473891753694154, 'epoch': 2.25}
{'loss': 1.0995, 'grad_norm': 353.59991455078125, 'learning_rate': 0.0002472591758027473, 'epoch': 2.25}
{'loss': 1.1002, 'grad_norm': 65.85001373291016, 'learning_rate': 0.0002471291762360792, 'epoch': 2.26}
{'loss': 1.1018, 'grad_norm': 73589.890625, 'learning_rate': 0.00024699917666941106, 'epoch': 2.26}
{'loss': 1.1017, 'grad_norm': 997.4054565429688, 'learning_rate': 0.000246869177102743, 'epoch': 2.26}
{'loss': 1.0984, 'grad_norm': 2619.78076171875, 'learning_rate': 0.00024673917753607487, 'epoch': 2.27}
{'loss': 1.0994, 'grad_norm': 250.8140411376953, 'learning_rate': 0.00024660917796940675, 'epoch': 2.27}
{'loss': 1.0996, 'grad_norm': 631.33447265625, 'learning_rate': 0.0002464791784027386, 'epoch': 2.28}
{'loss': 1.0993, 'grad_norm': 1377.202880859375, 'learning_rate': 0.00024634917883607055, 'epoch': 2.28}
{'loss': 1.0988, 'grad_norm': 140.18475341796875, 'learning_rate': 0.00024621917926940243, 'epoch': 2.29}
{'loss': 1.0975, 'grad_norm': 333.324951171875, 'learning_rate': 0.0002460891797027343, 'epoch': 2.29}
{'loss': 1.1, 'grad_norm': 45373.6328125, 'learning_rate': 0.0002459591801360662, 'epoch': 2.29}
{'loss': 1.0998, 'grad_norm': 107.18795776367188, 'learning_rate': 0.00024582918056939806, 'epoch': 2.3}
{'loss': 1.1007, 'grad_norm': 3966642.75, 'learning_rate': 0.00024569918100272994, 'epoch': 2.3}
{'loss': 1.099, 'grad_norm': 361.11663818359375, 'learning_rate': 0.0002455691814360618, 'epoch': 2.31}
{'loss': 1.0987, 'grad_norm': 519.04052734375, 'learning_rate': 0.00024543918186939375, 'epoch': 2.31}
{'loss': 1.0991, 'grad_norm': 407.5336608886719, 'learning_rate': 0.00024530918230272563, 'epoch': 2.31}
{'loss': 1.0996, 'grad_norm': 2492.77197265625, 'learning_rate': 0.00024517918273605756, 'epoch': 2.32}
{'loss': 1.1003, 'grad_norm': 250.05032348632812, 'learning_rate': 0.00024504918316938944, 'epoch': 2.32}
{'loss': 1.0995, 'grad_norm': 4916.017578125, 'learning_rate': 0.0002449191836027213, 'epoch': 2.33}
{'loss': 1.0992, 'grad_norm': 2418.107421875, 'learning_rate': 0.0002447891840360532, 'epoch': 2.33}
{'loss': 1.1002, 'grad_norm': 3327.99072265625, 'learning_rate': 0.00024465918446938507, 'epoch': 2.33}
{'loss': 1.0992, 'grad_norm': 2632.098876953125, 'learning_rate': 0.00024452918490271695, 'epoch': 2.34}
{'loss': 1.0996, 'grad_norm': 582.4847412109375, 'learning_rate': 0.0002443991853360489, 'epoch': 2.34}
{'loss': 1.0965, 'grad_norm': 43283.95703125, 'learning_rate': 0.00024426918576938076, 'epoch': 2.35}
{'loss': 1.0998, 'grad_norm': 10817.2294921875, 'learning_rate': 0.00024413918620271263, 'epoch': 2.35}
{'loss': 1.0977, 'grad_norm': 40754.9296875, 'learning_rate': 0.00024400918663604454, 'epoch': 2.35}
{'loss': 1.0995, 'grad_norm': 2108.023193359375, 'learning_rate': 0.00024387918706937642, 'epoch': 2.36}
{'loss': 1.0991, 'grad_norm': 1093.009765625, 'learning_rate': 0.0002437491875027083, 'epoch': 2.36}
{'loss': 1.0989, 'grad_norm': 96.64703369140625, 'learning_rate': 0.00024361918793604017, 'epoch': 2.37}
{'loss': 1.0995, 'grad_norm': 285.73736572265625, 'learning_rate': 0.00024348918836937207, 'epoch': 2.37}
{'loss': 1.0982, 'grad_norm': 202.93527221679688, 'learning_rate': 0.00024335918880270398, 'epoch': 2.37}
{'loss': 1.1001, 'grad_norm': 99.8813247680664, 'learning_rate': 0.00024322918923603588, 'epoch': 2.38}
{'loss': 1.0986, 'grad_norm': 360.5919189453125, 'learning_rate': 0.00024309918966936776, 'epoch': 2.38}
{'loss': 1.1011, 'grad_norm': 17626.107421875, 'learning_rate': 0.00024296919010269964, 'epoch': 2.39}
{'loss': 1.0998, 'grad_norm': 1637.6773681640625, 'learning_rate': 0.00024283919053603152, 'epoch': 2.39}
{'loss': 1.0969, 'grad_norm': 19240.330078125, 'learning_rate': 0.00024270919096936342, 'epoch': 2.4}
{'loss': 1.0966, 'grad_norm': 901.6961669921875, 'learning_rate': 0.0002425791914026953, 'epoch': 2.4}
{'loss': 1.0985, 'grad_norm': 446749.28125, 'learning_rate': 0.00024244919183602718, 'epoch': 2.4}
{'loss': 1.1, 'grad_norm': 482.7787780761719, 'learning_rate': 0.0002423191922693591, 'epoch': 2.41}
{'loss': 1.0981, 'grad_norm': 126.26091003417969, 'learning_rate': 0.00024218919270269098, 'epoch': 2.41}
{'loss': 1.1005, 'grad_norm': 439.0846862792969, 'learning_rate': 0.00024205919313602286, 'epoch': 2.42}
{'loss': 1.0997, 'grad_norm': 159021.875, 'learning_rate': 0.00024192919356935474, 'epoch': 2.42}
{'loss': 1.0996, 'grad_norm': 69656.203125, 'learning_rate': 0.00024179919400268664, 'epoch': 2.42}
{'loss': 1.0993, 'grad_norm': 239.49937438964844, 'learning_rate': 0.00024166919443601852, 'epoch': 2.43}
{'loss': 1.0992, 'grad_norm': 392.3692932128906, 'learning_rate': 0.0002415391948693504, 'epoch': 2.43}
{'loss': 1.1005, 'grad_norm': 9668.185546875, 'learning_rate': 0.0002414091953026823, 'epoch': 2.44}
{'loss': 1.0992, 'grad_norm': 555.2739868164062, 'learning_rate': 0.0002412791957360142, 'epoch': 2.44}
{'loss': 1.0987, 'grad_norm': 128.68258666992188, 'learning_rate': 0.00024114919616934609, 'epoch': 2.44}
{'loss': 1.1003, 'grad_norm': 392.3414611816406, 'learning_rate': 0.000241019196602678, 'epoch': 2.45}
{'loss': 1.0967, 'grad_norm': 138.85989379882812, 'learning_rate': 0.00024088919703600987, 'epoch': 2.45}
{'loss': 1.1, 'grad_norm': 286.6240234375, 'learning_rate': 0.00024075919746934174, 'epoch': 2.46}
{'loss': 1.096, 'grad_norm': 326.9325256347656, 'learning_rate': 0.00024062919790267362, 'epoch': 2.46}
{'loss': 1.0997, 'grad_norm': 2502.778076171875, 'learning_rate': 0.00024049919833600553, 'epoch': 2.46}
{'loss': 1.0999, 'grad_norm': 263.8668212890625, 'learning_rate': 0.0002403691987693374, 'epoch': 2.47}
{'loss': 1.0998, 'grad_norm': 246.01962280273438, 'learning_rate': 0.0002402391992026693, 'epoch': 2.47}
{'loss': 1.1016, 'grad_norm': 53.302066802978516, 'learning_rate': 0.0002401091996360012, 'epoch': 2.48}
{'loss': 1.101, 'grad_norm': 189.14073181152344, 'learning_rate': 0.0002399792000693331, 'epoch': 2.48}
{'loss': 1.0999, 'grad_norm': 246.83592224121094, 'learning_rate': 0.00023984920050266497, 'epoch': 2.48}
{'loss': 1.0986, 'grad_norm': 1626.2733154296875, 'learning_rate': 0.00023971920093599685, 'epoch': 2.49}
{'loss': 1.0988, 'grad_norm': 2399.1171875, 'learning_rate': 0.00023958920136932875, 'epoch': 2.49}
{'loss': 1.0998, 'grad_norm': 700.3324584960938, 'learning_rate': 0.00023945920180266063, 'epoch': 2.5}
{'loss': 1.1002, 'grad_norm': 893.4751586914062, 'learning_rate': 0.0002393292022359925, 'epoch': 2.5}
{'loss': 1.0989, 'grad_norm': 2174.442626953125, 'learning_rate': 0.00023919920266932444, 'epoch': 2.51}
{'loss': 1.0985, 'grad_norm': 1937.344482421875, 'learning_rate': 0.0002390692031026563, 'epoch': 2.51}
{'loss': 1.0981, 'grad_norm': 353.90032958984375, 'learning_rate': 0.0002389392035359882, 'epoch': 2.51}
{'loss': 1.0982, 'grad_norm': 226.49472045898438, 'learning_rate': 0.0002388092039693201, 'epoch': 2.52}
{'loss': 1.1003, 'grad_norm': 427.6218566894531, 'learning_rate': 0.00023867920440265197, 'epoch': 2.52}
{'loss': 1.099, 'grad_norm': 13.437597274780273, 'learning_rate': 0.00023854920483598385, 'epoch': 2.53}
{'loss': 1.0986, 'grad_norm': 940.4951782226562, 'learning_rate': 0.00023841920526931573, 'epoch': 2.53}
{'loss': 1.0992, 'grad_norm': 669.044677734375, 'learning_rate': 0.00023828920570264763, 'epoch': 2.53}
{'loss': 1.0992, 'grad_norm': 208.42709350585938, 'learning_rate': 0.00023815920613597954, 'epoch': 2.54}
{'loss': 1.1006, 'grad_norm': 346.1575622558594, 'learning_rate': 0.00023802920656931141, 'epoch': 2.54}
{'loss': 1.1004, 'grad_norm': 583.3428344726562, 'learning_rate': 0.00023789920700264332, 'epoch': 2.55}
{'loss': 1.1004, 'grad_norm': 487.18121337890625, 'learning_rate': 0.0002377692074359752, 'epoch': 2.55}
{'loss': 1.1003, 'grad_norm': 18.85824966430664, 'learning_rate': 0.00023763920786930707, 'epoch': 2.55}
{'loss': 1.0981, 'grad_norm': 112.45713806152344, 'learning_rate': 0.00023750920830263898, 'epoch': 2.56}
{'loss': 1.0983, 'grad_norm': 2429.24853515625, 'learning_rate': 0.00023737920873597086, 'epoch': 2.56}
{'loss': 1.1005, 'grad_norm': 5154.5341796875, 'learning_rate': 0.00023724920916930273, 'epoch': 2.57}
{'loss': 1.0991, 'grad_norm': 3035.582275390625, 'learning_rate': 0.00023711920960263466, 'epoch': 2.57}
{'loss': 1.0992, 'grad_norm': 2432.712646484375, 'learning_rate': 0.00023698921003596654, 'epoch': 2.57}
{'loss': 1.1, 'grad_norm': 4802.73095703125, 'learning_rate': 0.00023685921046929842, 'epoch': 2.58}
{'loss': 1.0981, 'grad_norm': 58.90480422973633, 'learning_rate': 0.0002367292109026303, 'epoch': 2.58}
{'loss': 1.0976, 'grad_norm': 137.0192108154297, 'learning_rate': 0.0002365992113359622, 'epoch': 2.59}
{'loss': 1.1012, 'grad_norm': 853.9334716796875, 'learning_rate': 0.00023646921176929408, 'epoch': 2.59}
{'loss': 1.0979, 'grad_norm': 10777.9677734375, 'learning_rate': 0.00023633921220262596, 'epoch': 2.59}
{'loss': 1.0977, 'grad_norm': 2765.565185546875, 'learning_rate': 0.00023620921263595783, 'epoch': 2.6}
{'loss': 1.1002, 'grad_norm': 21342.701171875, 'learning_rate': 0.00023607921306928977, 'epoch': 2.6}
{'loss': 1.0984, 'grad_norm': 9466.07421875, 'learning_rate': 0.00023594921350262164, 'epoch': 2.61}
{'loss': 1.1004, 'grad_norm': 22711.4765625, 'learning_rate': 0.00023581921393595352, 'epoch': 2.61}
{'loss': 1.0997, 'grad_norm': 78.00981903076172, 'learning_rate': 0.00023568921436928542, 'epoch': 2.62}
{'loss': 1.0983, 'grad_norm': 634.2543334960938, 'learning_rate': 0.0002355592148026173, 'epoch': 2.62}
{'loss': 1.0994, 'grad_norm': 239.00790405273438, 'learning_rate': 0.00023542921523594918, 'epoch': 2.62}
{'loss': 1.099, 'grad_norm': 738.2116088867188, 'learning_rate': 0.00023529921566928108, 'epoch': 2.63}
{'loss': 1.0988, 'grad_norm': 1168.292236328125, 'learning_rate': 0.00023516921610261296, 'epoch': 2.63}
{'loss': 1.0979, 'grad_norm': 116.65140533447266, 'learning_rate': 0.00023503921653594487, 'epoch': 2.64}
{'loss': 1.0999, 'grad_norm': 4339.3828125, 'learning_rate': 0.00023490921696927677, 'epoch': 2.64}
{'loss': 1.1006, 'grad_norm': 455.8894348144531, 'learning_rate': 0.00023477921740260865, 'epoch': 2.64}
{'loss': 1.0997, 'grad_norm': 196.35867309570312, 'learning_rate': 0.00023464921783594053, 'epoch': 2.65}
{'loss': 1.1013, 'grad_norm': 646.0076904296875, 'learning_rate': 0.0002345192182692724, 'epoch': 2.65}
{'loss': 1.0989, 'grad_norm': 935232.3125, 'learning_rate': 0.0002343892187026043, 'epoch': 2.66}
{'loss': 1.0994, 'grad_norm': 930.5618286132812, 'learning_rate': 0.00023425921913593618, 'epoch': 2.66}
{'loss': 1.0983, 'grad_norm': 4021.66796875, 'learning_rate': 0.00023412921956926806, 'epoch': 2.66}
{'loss': 1.0995, 'grad_norm': 154.16676330566406, 'learning_rate': 0.0002339992200026, 'epoch': 2.67}
{'loss': 1.0987, 'grad_norm': 289.0278015136719, 'learning_rate': 0.00023386922043593187, 'epoch': 2.67}
{'loss': 1.1014, 'grad_norm': 61907.02734375, 'learning_rate': 0.00023373922086926375, 'epoch': 2.68}
{'loss': 1.0998, 'grad_norm': 1181.103271484375, 'learning_rate': 0.00023360922130259565, 'epoch': 2.68}
{'loss': 1.1018, 'grad_norm': 54.27790832519531, 'learning_rate': 0.00023347922173592753, 'epoch': 2.68}
{'loss': 1.099, 'grad_norm': 65.17314147949219, 'learning_rate': 0.0002333492221692594, 'epoch': 2.69}
{'loss': 1.1017, 'grad_norm': 118725.84375, 'learning_rate': 0.00023321922260259129, 'epoch': 2.69}
{'loss': 1.0984, 'grad_norm': 9119.4892578125, 'learning_rate': 0.0002330892230359232, 'epoch': 2.7}
{'loss': 1.0983, 'grad_norm': 1482.5650634765625, 'learning_rate': 0.0002329592234692551, 'epoch': 2.7}
{'loss': 1.1006, 'grad_norm': 1995.68896484375, 'learning_rate': 0.00023282922390258697, 'epoch': 2.7}
{'loss': 1.1006, 'grad_norm': 765.1717529296875, 'learning_rate': 0.00023269922433591888, 'epoch': 2.71}
{'loss': 1.0993, 'grad_norm': 55.821720123291016, 'learning_rate': 0.00023256922476925075, 'epoch': 2.71}
{'loss': 1.1007, 'grad_norm': 767.7835083007812, 'learning_rate': 0.00023243922520258263, 'epoch': 2.72}
{'loss': 1.0996, 'grad_norm': 370.7106018066406, 'learning_rate': 0.0002323092256359145, 'epoch': 2.72}
{'loss': 1.0998, 'grad_norm': 18.332073211669922, 'learning_rate': 0.0002321792260692464, 'epoch': 2.73}
{'loss': 1.1005, 'grad_norm': 452.7109680175781, 'learning_rate': 0.0002320492265025783, 'epoch': 2.73}
{'loss': 1.0991, 'grad_norm': 1509.3482666015625, 'learning_rate': 0.0002319192269359102, 'epoch': 2.73}
{'loss': 1.1011, 'grad_norm': 141.587890625, 'learning_rate': 0.0002317892273692421, 'epoch': 2.74}
{'loss': 1.0984, 'grad_norm': 1711.49462890625, 'learning_rate': 0.00023165922780257398, 'epoch': 2.74}
{'loss': 1.0985, 'grad_norm': 2208.833984375, 'learning_rate': 0.00023152922823590585, 'epoch': 2.75}
{'loss': 1.0979, 'grad_norm': 2595.728759765625, 'learning_rate': 0.00023139922866923776, 'epoch': 2.75}
{'loss': 1.0989, 'grad_norm': 36941.79296875, 'learning_rate': 0.00023126922910256964, 'epoch': 2.75}
{'loss': 1.0998, 'grad_norm': 8747.0166015625, 'learning_rate': 0.00023113922953590151, 'epoch': 2.76}
{'loss': 1.0998, 'grad_norm': 1157.090087890625, 'learning_rate': 0.0002310092299692334, 'epoch': 2.76}
{'loss': 1.1018, 'grad_norm': 1016.4500732421875, 'learning_rate': 0.00023087923040256532, 'epoch': 2.77}
{'loss': 1.0996, 'grad_norm': 44312208.0, 'learning_rate': 0.0002307492308358972, 'epoch': 2.77}
{'loss': 1.0983, 'grad_norm': 18132.84375, 'learning_rate': 0.00023061923126922908, 'epoch': 2.77}
{'loss': 1.1002, 'grad_norm': 5462.0859375, 'learning_rate': 0.00023048923170256098, 'epoch': 2.78}
{'loss': 1.0994, 'grad_norm': 3672.841796875, 'learning_rate': 0.00023035923213589286, 'epoch': 2.78}
{'loss': 1.1005, 'grad_norm': 40588.30859375, 'learning_rate': 0.00023022923256922474, 'epoch': 2.79}
{'loss': 1.1004, 'grad_norm': 2159.196044921875, 'learning_rate': 0.00023009923300255661, 'epoch': 2.79}
{'loss': 1.1001, 'grad_norm': 6127.91455078125, 'learning_rate': 0.00022996923343588852, 'epoch': 2.79}
{'loss': 1.0985, 'grad_norm': 231.23187255859375, 'learning_rate': 0.00022983923386922042, 'epoch': 2.8}
{'loss': 1.0995, 'grad_norm': 596.8592529296875, 'learning_rate': 0.0002297092343025523, 'epoch': 2.8}
{'loss': 1.099, 'grad_norm': 332.7311706542969, 'learning_rate': 0.0002295792347358842, 'epoch': 2.81}
{'loss': 1.101, 'grad_norm': 29282.95703125, 'learning_rate': 0.00022944923516921608, 'epoch': 2.81}
{'loss': 1.0999, 'grad_norm': 2712.962158203125, 'learning_rate': 0.00022931923560254796, 'epoch': 2.81}
{'loss': 1.0984, 'grad_norm': 6643.50048828125, 'learning_rate': 0.00022918923603587986, 'epoch': 2.82}
{'loss': 1.0997, 'grad_norm': 7312.1201171875, 'learning_rate': 0.00022905923646921174, 'epoch': 2.82}
{'loss': 1.0993, 'grad_norm': 2539.719970703125, 'learning_rate': 0.00022892923690254362, 'epoch': 2.83}
{'loss': 1.0989, 'grad_norm': 58685.85546875, 'learning_rate': 0.00022879923733587555, 'epoch': 2.83}
{'loss': 1.1001, 'grad_norm': 746345.75, 'learning_rate': 0.00022866923776920743, 'epoch': 2.84}
{'loss': 1.0993, 'grad_norm': 13488.3076171875, 'learning_rate': 0.0002285392382025393, 'epoch': 2.84}
{'loss': 1.0992, 'grad_norm': 2367677.25, 'learning_rate': 0.00022840923863587118, 'epoch': 2.84}
{'loss': 1.1, 'grad_norm': 32661.712890625, 'learning_rate': 0.0002282792390692031, 'epoch': 2.85}
{'loss': 1.099, 'grad_norm': 3280.3759765625, 'learning_rate': 0.00022814923950253497, 'epoch': 2.85}
{'loss': 1.1003, 'grad_norm': 38690.09375, 'learning_rate': 0.00022801923993586684, 'epoch': 2.86}
{'loss': 1.0988, 'grad_norm': 34805.48828125, 'learning_rate': 0.00022788924036919872, 'epoch': 2.86}
{'loss': 1.1, 'grad_norm': 133195.71875, 'learning_rate': 0.00022775924080253065, 'epoch': 2.86}
{'loss': 1.099, 'grad_norm': 4744.01171875, 'learning_rate': 0.00022762924123586253, 'epoch': 2.87}
{'loss': 1.0984, 'grad_norm': 3600.08251953125, 'learning_rate': 0.00022749924166919443, 'epoch': 2.87}
{'loss': 1.0993, 'grad_norm': 108475.34375, 'learning_rate': 0.0002273692421025263, 'epoch': 2.88}
{'loss': 1.0984, 'grad_norm': 156338.375, 'learning_rate': 0.0002272392425358582, 'epoch': 2.88}
{'loss': 1.099, 'grad_norm': 9683.279296875, 'learning_rate': 0.00022710924296919007, 'epoch': 2.88}
{'loss': 1.0989, 'grad_norm': 9484.734375, 'learning_rate': 0.00022697924340252197, 'epoch': 2.89}
{'loss': 1.0988, 'grad_norm': 1303.1776123046875, 'learning_rate': 0.00022684924383585385, 'epoch': 2.89}
{'loss': 1.0992, 'grad_norm': 18160.580078125, 'learning_rate': 0.00022671924426918575, 'epoch': 2.9}
{'loss': 1.0989, 'grad_norm': 35433.81640625, 'learning_rate': 0.00022658924470251766, 'epoch': 2.9}
{'loss': 1.0964, 'grad_norm': 9795.880859375, 'learning_rate': 0.00022645924513584953, 'epoch': 2.9}
{'loss': 1.1009, 'grad_norm': 303.5465087890625, 'learning_rate': 0.0002263292455691814, 'epoch': 2.91}
{'loss': 1.0989, 'grad_norm': 3066.9619140625, 'learning_rate': 0.0002261992460025133, 'epoch': 2.91}
{'loss': 1.0993, 'grad_norm': 2796.5966796875, 'learning_rate': 0.0002260692464358452, 'epoch': 2.92}
{'loss': 1.0988, 'grad_norm': 490727.6875, 'learning_rate': 0.00022593924686917707, 'epoch': 2.92}
{'loss': 1.0999, 'grad_norm': 3282.528564453125, 'learning_rate': 0.00022580924730250895, 'epoch': 2.92}
{'loss': 1.0986, 'grad_norm': 15701.810546875, 'learning_rate': 0.00022567924773584088, 'epoch': 2.93}
{'loss': 1.0995, 'grad_norm': 6485.7978515625, 'learning_rate': 0.00022554924816917276, 'epoch': 2.93}
{'loss': 1.0989, 'grad_norm': 1074741.0, 'learning_rate': 0.00022541924860250463, 'epoch': 2.94}
{'loss': 1.0972, 'grad_norm': 26241.50390625, 'learning_rate': 0.00022528924903583654, 'epoch': 2.94}
{'loss': 1.0994, 'grad_norm': 1276620.125, 'learning_rate': 0.00022515924946916842, 'epoch': 2.95}
{'loss': 1.0996, 'grad_norm': 167611.96875, 'learning_rate': 0.0002250292499025003, 'epoch': 2.95}
{'loss': 1.1, 'grad_norm': 4933.103515625, 'learning_rate': 0.00022489925033583217, 'epoch': 2.95}
{'loss': 1.0999, 'grad_norm': 5664.74267578125, 'learning_rate': 0.00022476925076916408, 'epoch': 2.96}
{'loss': 1.1009, 'grad_norm': 25363.111328125, 'learning_rate': 0.00022463925120249598, 'epoch': 2.96}
{'loss': 1.0997, 'grad_norm': 17018.751953125, 'learning_rate': 0.00022450925163582786, 'epoch': 2.97}
{'loss': 1.1, 'grad_norm': 24555.943359375, 'learning_rate': 0.00022437925206915976, 'epoch': 2.97}
{'loss': 1.0984, 'grad_norm': 17235.244140625, 'learning_rate': 0.00022424925250249164, 'epoch': 2.97}
{'loss': 1.0986, 'grad_norm': 15303.08203125, 'learning_rate': 0.00022411925293582352, 'epoch': 2.98}
{'loss': 1.0987, 'grad_norm': 5395.33154296875, 'learning_rate': 0.0002239892533691554, 'epoch': 2.98}
{'loss': 1.0992, 'grad_norm': 1177634.5, 'learning_rate': 0.0002238592538024873, 'epoch': 2.99}
{'loss': 1.0995, 'grad_norm': 419903.09375, 'learning_rate': 0.0002237292542358192, 'epoch': 2.99}
{'loss': 1.0987, 'grad_norm': 2973221.25, 'learning_rate': 0.0002235992546691511, 'epoch': 2.99}
{'loss': 1.0998, 'grad_norm': 6450899.5, 'learning_rate': 0.00022346925510248299, 'epoch': 3.0}
{'eval_loss': 1.0978779792785645, 'eval_accuracy': 0.31818644931227713, 'eval_runtime': 5.4157, 'eval_samples_per_second': 1812.311, 'eval_steps_per_second': 11.448, 'epoch': 3.0}
{'loss': 1.0997, 'grad_norm': 6422.9072265625, 'learning_rate': 0.00022333925553581486, 'epoch': 3.0}
{'loss': 1.0983, 'grad_norm': 5746.65478515625, 'learning_rate': 0.00022320925596914674, 'epoch': 3.01}
{'loss': 1.0981, 'grad_norm': 2180.04931640625, 'learning_rate': 0.00022307925640247865, 'epoch': 3.01}
{'loss': 1.0993, 'grad_norm': 112321.8671875, 'learning_rate': 0.00022294925683581052, 'epoch': 3.01}
{'loss': 1.0997, 'grad_norm': 8714.451171875, 'learning_rate': 0.0002228192572691424, 'epoch': 3.02}
{'loss': 1.0986, 'grad_norm': 30928.74609375, 'learning_rate': 0.00022268925770247433, 'epoch': 3.02}
{'loss': 1.0989, 'grad_norm': 478.30487060546875, 'learning_rate': 0.0002225592581358062, 'epoch': 3.03}
{'loss': 1.0974, 'grad_norm': 2180.602294921875, 'learning_rate': 0.0002224292585691381, 'epoch': 3.03}
{'loss': 1.1001, 'grad_norm': 318748.25, 'learning_rate': 0.00022229925900246996, 'epoch': 3.03}
{'loss': 1.0978, 'grad_norm': 4453.7578125, 'learning_rate': 0.00022216925943580187, 'epoch': 3.04}
{'loss': 1.0988, 'grad_norm': 21529.435546875, 'learning_rate': 0.00022203925986913375, 'epoch': 3.04}
{'loss': 1.0984, 'grad_norm': 705.1968994140625, 'learning_rate': 0.00022190926030246562, 'epoch': 3.05}
{'loss': 1.0995, 'grad_norm': 14161.6962890625, 'learning_rate': 0.00022177926073579753, 'epoch': 3.05}
{'loss': 1.0975, 'grad_norm': 186.90512084960938, 'learning_rate': 0.00022164926116912943, 'epoch': 3.05}
{'loss': 1.0993, 'grad_norm': 5302.3681640625, 'learning_rate': 0.0002215192616024613, 'epoch': 3.06}
{'loss': 1.1007, 'grad_norm': 915.1488037109375, 'learning_rate': 0.00022138926203579321, 'epoch': 3.06}
{'loss': 1.0992, 'grad_norm': 1359.5545654296875, 'learning_rate': 0.0002212592624691251, 'epoch': 3.07}
{'loss': 1.0975, 'grad_norm': 302.376220703125, 'learning_rate': 0.00022112926290245697, 'epoch': 3.07}
{'loss': 1.0982, 'grad_norm': 22788.919921875, 'learning_rate': 0.00022099926333578885, 'epoch': 3.08}
{'loss': 1.0993, 'grad_norm': 1202.0087890625, 'learning_rate': 0.00022086926376912075, 'epoch': 3.08}
{'loss': 1.0982, 'grad_norm': 1394.6700439453125, 'learning_rate': 0.00022073926420245263, 'epoch': 3.08}
{'loss': 1.0987, 'grad_norm': 1087.3629150390625, 'learning_rate': 0.00022060926463578453, 'epoch': 3.09}
{'loss': 1.101, 'grad_norm': 203.701904296875, 'learning_rate': 0.00022047926506911644, 'epoch': 3.09}
{'loss': 1.0993, 'grad_norm': 11351.2451171875, 'learning_rate': 0.00022034926550244832, 'epoch': 3.1}
{'loss': 1.0984, 'grad_norm': 4095.727294921875, 'learning_rate': 0.0002202192659357802, 'epoch': 3.1}
{'loss': 1.0987, 'grad_norm': 31.079057693481445, 'learning_rate': 0.00022008926636911207, 'epoch': 3.1}
{'loss': 1.099, 'grad_norm': 1677.1904296875, 'learning_rate': 0.00021995926680244397, 'epoch': 3.11}
{'loss': 1.1013, 'grad_norm': 298.08770751953125, 'learning_rate': 0.00021982926723577585, 'epoch': 3.11}
{'loss': 1.0993, 'grad_norm': 10196.462890625, 'learning_rate': 0.00021969926766910773, 'epoch': 3.12}
{'loss': 1.0996, 'grad_norm': 719.4823608398438, 'learning_rate': 0.00021956926810243966, 'epoch': 3.12}
{'loss': 1.0983, 'grad_norm': 952.8914794921875, 'learning_rate': 0.00021943926853577154, 'epoch': 3.12}
{'loss': 1.0977, 'grad_norm': 84772.671875, 'learning_rate': 0.00021930926896910342, 'epoch': 3.13}
{'loss': 1.099, 'grad_norm': 16714.90234375, 'learning_rate': 0.00021917926940243532, 'epoch': 3.13}
{'loss': 1.099, 'grad_norm': 5165.69482421875, 'learning_rate': 0.0002190492698357672, 'epoch': 3.14}
{'loss': 1.0975, 'grad_norm': 76.04713439941406, 'learning_rate': 0.00021891927026909908, 'epoch': 3.14}
{'loss': 1.0999, 'grad_norm': 1163.5963134765625, 'learning_rate': 0.00021878927070243095, 'epoch': 3.14}
{'loss': 1.1004, 'grad_norm': 781.9253540039062, 'learning_rate': 0.00021865927113576286, 'epoch': 3.15}
{'loss': 1.0979, 'grad_norm': 557.1343383789062, 'learning_rate': 0.00021852927156909476, 'epoch': 3.15}
{'loss': 1.0995, 'grad_norm': 5165.97705078125, 'learning_rate': 0.00021839927200242664, 'epoch': 3.16}
{'loss': 1.1004, 'grad_norm': 1039.710693359375, 'learning_rate': 0.00021826927243575854, 'epoch': 3.16}
{'loss': 1.0979, 'grad_norm': 1125.0439453125, 'learning_rate': 0.00021813927286909042, 'epoch': 3.16}
{'loss': 1.1005, 'grad_norm': 11218.626953125, 'learning_rate': 0.0002180092733024223, 'epoch': 3.17}
{'loss': 1.0981, 'grad_norm': 2635.755859375, 'learning_rate': 0.0002178792737357542, 'epoch': 3.17}
{'loss': 1.1011, 'grad_norm': 17619.08984375, 'learning_rate': 0.00021774927416908608, 'epoch': 3.18}
{'loss': 1.0984, 'grad_norm': 1917.3514404296875, 'learning_rate': 0.00021761927460241796, 'epoch': 3.18}
{'loss': 1.0989, 'grad_norm': 3059.08447265625, 'learning_rate': 0.0002174892750357499, 'epoch': 3.19}
{'loss': 1.0987, 'grad_norm': 62216.28125, 'learning_rate': 0.00021735927546908177, 'epoch': 3.19}
{'loss': 1.1003, 'grad_norm': 607.3385009765625, 'learning_rate': 0.00021722927590241364, 'epoch': 3.19}
{'loss': 1.0993, 'grad_norm': 2797.486083984375, 'learning_rate': 0.00021709927633574552, 'epoch': 3.2}
{'loss': 1.1001, 'grad_norm': 2504.590576171875, 'learning_rate': 0.00021696927676907743, 'epoch': 3.2}
{'loss': 1.0982, 'grad_norm': 173.21852111816406, 'learning_rate': 0.0002168392772024093, 'epoch': 3.21}
{'loss': 1.0993, 'grad_norm': 807.359375, 'learning_rate': 0.00021670927763574118, 'epoch': 3.21}
{'loss': 1.0994, 'grad_norm': 1028.5841064453125, 'learning_rate': 0.00021657927806907306, 'epoch': 3.21}
{'loss': 1.1006, 'grad_norm': 491.057861328125, 'learning_rate': 0.000216449278502405, 'epoch': 3.22}
{'loss': 1.098, 'grad_norm': 1128.699951171875, 'learning_rate': 0.00021631927893573687, 'epoch': 3.22}
{'loss': 1.0985, 'grad_norm': 3608.3681640625, 'learning_rate': 0.00021618927936906874, 'epoch': 3.23}
{'loss': 1.0994, 'grad_norm': 825.0305786132812, 'learning_rate': 0.00021605927980240065, 'epoch': 3.23}
{'loss': 1.1002, 'grad_norm': 2043.249267578125, 'learning_rate': 0.00021592928023573253, 'epoch': 3.23}
{'loss': 1.0991, 'grad_norm': 3128.585205078125, 'learning_rate': 0.0002157992806690644, 'epoch': 3.24}
{'loss': 1.099, 'grad_norm': 518300.8125, 'learning_rate': 0.0002156692811023963, 'epoch': 3.24}
{'loss': 1.0996, 'grad_norm': 3984.1904296875, 'learning_rate': 0.00021553928153572819, 'epoch': 3.25}
{'loss': 1.0999, 'grad_norm': 1075.150634765625, 'learning_rate': 0.0002154092819690601, 'epoch': 3.25}
{'loss': 1.0991, 'grad_norm': 6632.462890625, 'learning_rate': 0.000215279282402392, 'epoch': 3.25}
{'loss': 1.1007, 'grad_norm': 5216.0498046875, 'learning_rate': 0.00021514928283572387, 'epoch': 3.26}
{'loss': 1.0992, 'grad_norm': 10978.5830078125, 'learning_rate': 0.00021501928326905575, 'epoch': 3.26}
{'loss': 1.0996, 'grad_norm': 62319.10546875, 'learning_rate': 0.00021488928370238763, 'epoch': 3.27}
{'loss': 1.0998, 'grad_norm': 1662.117431640625, 'learning_rate': 0.00021475928413571953, 'epoch': 3.27}
{'loss': 1.0987, 'grad_norm': 40291.02734375, 'learning_rate': 0.0002146292845690514, 'epoch': 3.27}
{'loss': 1.0991, 'grad_norm': 5971.02001953125, 'learning_rate': 0.0002144992850023833, 'epoch': 3.28}
{'loss': 1.098, 'grad_norm': 3348.701171875, 'learning_rate': 0.00021436928543571522, 'epoch': 3.28}
{'loss': 1.0992, 'grad_norm': 6587.29931640625, 'learning_rate': 0.0002142392858690471, 'epoch': 3.29}
{'loss': 1.0981, 'grad_norm': 3263.947998046875, 'learning_rate': 0.00021410928630237897, 'epoch': 3.29}
{'loss': 1.0995, 'grad_norm': 1261.663818359375, 'learning_rate': 0.00021397928673571088, 'epoch': 3.3}
{'loss': 1.1001, 'grad_norm': 2877.656005859375, 'learning_rate': 0.00021384928716904276, 'epoch': 3.3}
{'loss': 1.1001, 'grad_norm': 228482.796875, 'learning_rate': 0.00021371928760237463, 'epoch': 3.3}
{'loss': 1.0979, 'grad_norm': 1024.9105224609375, 'learning_rate': 0.0002135892880357065, 'epoch': 3.31}
{'loss': 1.098, 'grad_norm': 1239.4029541015625, 'learning_rate': 0.00021345928846903841, 'epoch': 3.31}
{'loss': 1.0996, 'grad_norm': 9358.0947265625, 'learning_rate': 0.00021332928890237032, 'epoch': 3.32}
{'loss': 1.0995, 'grad_norm': 1278.706787109375, 'learning_rate': 0.0002131992893357022, 'epoch': 3.32}
{'loss': 1.1004, 'grad_norm': 2634.73291015625, 'learning_rate': 0.0002130692897690341, 'epoch': 3.32}
{'loss': 1.0999, 'grad_norm': 1634.2830810546875, 'learning_rate': 0.00021293929020236598, 'epoch': 3.33}
{'loss': 1.0984, 'grad_norm': 319.52288818359375, 'learning_rate': 0.00021280929063569786, 'epoch': 3.33}
{'loss': 1.0996, 'grad_norm': 239.10333251953125, 'learning_rate': 0.00021267929106902973, 'epoch': 3.34}
{'loss': 1.0975, 'grad_norm': 714.7467041015625, 'learning_rate': 0.00021254929150236164, 'epoch': 3.34}
{'loss': 1.0995, 'grad_norm': 102.7023696899414, 'learning_rate': 0.00021241929193569352, 'epoch': 3.34}
{'loss': 1.1007, 'grad_norm': 3349.341552734375, 'learning_rate': 0.00021228929236902542, 'epoch': 3.35}
{'loss': 1.1021, 'grad_norm': 13254.6220703125, 'learning_rate': 0.00021215929280235732, 'epoch': 3.35}
{'loss': 1.0987, 'grad_norm': 155.36245727539062, 'learning_rate': 0.0002120292932356892, 'epoch': 3.36}
{'loss': 1.0977, 'grad_norm': 594.25537109375, 'learning_rate': 0.00021189929366902108, 'epoch': 3.36}
{'loss': 1.1004, 'grad_norm': 401.5958251953125, 'learning_rate': 0.00021176929410235298, 'epoch': 3.36}
{'loss': 1.1043, 'grad_norm': 740.8869018554688, 'learning_rate': 0.00021163929453568486, 'epoch': 3.37}
{'loss': 1.1, 'grad_norm': 6553.33642578125, 'learning_rate': 0.00021150929496901674, 'epoch': 3.37}
{'loss': 1.103, 'grad_norm': 1224.9627685546875, 'learning_rate': 0.00021137929540234862, 'epoch': 3.38}
{'loss': 1.1036, 'grad_norm': 1982.57568359375, 'learning_rate': 0.00021124929583568055, 'epoch': 3.38}
{'loss': 1.0995, 'grad_norm': 1571.013671875, 'learning_rate': 0.00021111929626901242, 'epoch': 3.38}
{'loss': 1.1, 'grad_norm': 767.5235595703125, 'learning_rate': 0.0002109892967023443, 'epoch': 3.39}
{'loss': 1.1, 'grad_norm': 19.93989372253418, 'learning_rate': 0.0002108592971356762, 'epoch': 3.39}
{'loss': 1.099, 'grad_norm': 1104.50927734375, 'learning_rate': 0.00021072929756900808, 'epoch': 3.4}
{'loss': 1.0994, 'grad_norm': 24587.927734375, 'learning_rate': 0.00021059929800233996, 'epoch': 3.4}
{'loss': 1.1001, 'grad_norm': 179.9015655517578, 'learning_rate': 0.00021046929843567184, 'epoch': 3.41}
{'loss': 1.0994, 'grad_norm': 3151.70849609375, 'learning_rate': 0.00021033929886900374, 'epoch': 3.41}
{'loss': 1.0998, 'grad_norm': 5364.8876953125, 'learning_rate': 0.00021020929930233565, 'epoch': 3.41}
{'loss': 1.0988, 'grad_norm': 195759.6875, 'learning_rate': 0.00021007929973566753, 'epoch': 3.42}
{'loss': 1.1, 'grad_norm': 81.28227233886719, 'learning_rate': 0.00020994930016899943, 'epoch': 3.42}
{'loss': 1.0989, 'grad_norm': 512.4609985351562, 'learning_rate': 0.0002098193006023313, 'epoch': 3.43}
{'loss': 1.1003, 'grad_norm': 94.08795928955078, 'learning_rate': 0.00020968930103566318, 'epoch': 3.43}
{'loss': 1.1, 'grad_norm': 2278.675048828125, 'learning_rate': 0.0002095593014689951, 'epoch': 3.43}
{'loss': 1.1007, 'grad_norm': 478.74786376953125, 'learning_rate': 0.00020942930190232697, 'epoch': 3.44}
{'loss': 1.099, 'grad_norm': 4432.41015625, 'learning_rate': 0.00020929930233565884, 'epoch': 3.44}
{'loss': 1.1011, 'grad_norm': 185.03982543945312, 'learning_rate': 0.00020916930276899078, 'epoch': 3.45}
{'loss': 1.098, 'grad_norm': 1129.53271484375, 'learning_rate': 0.00020903930320232265, 'epoch': 3.45}
{'loss': 1.0993, 'grad_norm': 149.80047607421875, 'learning_rate': 0.00020890930363565453, 'epoch': 3.45}
{'loss': 1.0988, 'grad_norm': 484.12615966796875, 'learning_rate': 0.0002087793040689864, 'epoch': 3.46}
{'loss': 1.0999, 'grad_norm': 842.2816772460938, 'learning_rate': 0.0002086493045023183, 'epoch': 3.46}
{'loss': 1.0991, 'grad_norm': 1389.2462158203125, 'learning_rate': 0.0002085193049356502, 'epoch': 3.47}
{'loss': 1.1031, 'grad_norm': 75.18833923339844, 'learning_rate': 0.00020838930536898207, 'epoch': 3.47}
{'loss': 1.1017, 'grad_norm': 1383.9373779296875, 'learning_rate': 0.00020825930580231394, 'epoch': 3.47}
{'loss': 1.1018, 'grad_norm': 26.089895248413086, 'learning_rate': 0.00020812930623564588, 'epoch': 3.48}
{'loss': 1.1027, 'grad_norm': 344.9488220214844, 'learning_rate': 0.00020799930666897775, 'epoch': 3.48}
{'loss': 1.1024, 'grad_norm': 504.4400329589844, 'learning_rate': 0.00020786930710230966, 'epoch': 3.49}
{'loss': 1.0978, 'grad_norm': 389.0498962402344, 'learning_rate': 0.00020773930753564154, 'epoch': 3.49}
{'loss': 1.1014, 'grad_norm': 19152.15625, 'learning_rate': 0.0002076093079689734, 'epoch': 3.49}
{'loss': 1.1012, 'grad_norm': 1887.305908203125, 'learning_rate': 0.0002074793084023053, 'epoch': 3.5}
{'loss': 1.0995, 'grad_norm': 1008.9620971679688, 'learning_rate': 0.0002073493088356372, 'epoch': 3.5}
{'loss': 1.1013, 'grad_norm': 142.22117614746094, 'learning_rate': 0.00020721930926896907, 'epoch': 3.51}
{'loss': 1.0988, 'grad_norm': 73.9451675415039, 'learning_rate': 0.00020708930970230098, 'epoch': 3.51}
{'loss': 1.0994, 'grad_norm': 506.9985046386719, 'learning_rate': 0.00020695931013563288, 'epoch': 3.52}
{'loss': 1.0992, 'grad_norm': 4314.46142578125, 'learning_rate': 0.00020682931056896476, 'epoch': 3.52}
{'loss': 1.0984, 'grad_norm': 3558.65478515625, 'learning_rate': 0.00020669931100229664, 'epoch': 3.52}
{'loss': 1.0983, 'grad_norm': 164.67596435546875, 'learning_rate': 0.00020656931143562851, 'epoch': 3.53}
{'loss': 1.1057, 'grad_norm': 175.3828125, 'learning_rate': 0.00020643931186896042, 'epoch': 3.53}
{'loss': 1.1003, 'grad_norm': 3226.232666015625, 'learning_rate': 0.0002063093123022923, 'epoch': 3.54}
{'loss': 1.103, 'grad_norm': 186.5998077392578, 'learning_rate': 0.00020617931273562417, 'epoch': 3.54}
{'loss': 1.0999, 'grad_norm': 1787.187744140625, 'learning_rate': 0.0002060493131689561, 'epoch': 3.54}
{'loss': 1.1003, 'grad_norm': 13081.943359375, 'learning_rate': 0.00020591931360228798, 'epoch': 3.55}
{'loss': 1.0969, 'grad_norm': 87.68669891357422, 'learning_rate': 0.00020578931403561986, 'epoch': 3.55}
{'loss': 1.0989, 'grad_norm': 355.6083068847656, 'learning_rate': 0.00020565931446895176, 'epoch': 3.56}
{'loss': 1.1025, 'grad_norm': 734.592041015625, 'learning_rate': 0.00020552931490228364, 'epoch': 3.56}
{'loss': 1.0985, 'grad_norm': 1268.4537353515625, 'learning_rate': 0.00020539931533561552, 'epoch': 3.56}
{'loss': 1.0978, 'grad_norm': 184.70066833496094, 'learning_rate': 0.0002052693157689474, 'epoch': 3.57}
{'loss': 1.0983, 'grad_norm': 4364.537109375, 'learning_rate': 0.0002051393162022793, 'epoch': 3.57}
{'loss': 1.0988, 'grad_norm': 600.2794799804688, 'learning_rate': 0.0002050093166356112, 'epoch': 3.58}
{'loss': 1.1003, 'grad_norm': 1385.9219970703125, 'learning_rate': 0.00020487931706894308, 'epoch': 3.58}
{'loss': 1.1005, 'grad_norm': 26.232080459594727, 'learning_rate': 0.000204749317502275, 'epoch': 3.58}
{'loss': 1.0978, 'grad_norm': 420.1224670410156, 'learning_rate': 0.00020461931793560686, 'epoch': 3.59}
{'loss': 1.1002, 'grad_norm': 28927.626953125, 'learning_rate': 0.00020448931836893874, 'epoch': 3.59}
{'loss': 1.0995, 'grad_norm': 591.8467407226562, 'learning_rate': 0.00020435931880227062, 'epoch': 3.6}
{'loss': 1.0998, 'grad_norm': 52081.99609375, 'learning_rate': 0.00020422931923560252, 'epoch': 3.6}
{'loss': 1.0993, 'grad_norm': 5608.224609375, 'learning_rate': 0.0002040993196689344, 'epoch': 3.6}
{'loss': 1.0979, 'grad_norm': 312.8893737792969, 'learning_rate': 0.00020396932010226633, 'epoch': 3.61}
{'loss': 1.0994, 'grad_norm': 62.61078643798828, 'learning_rate': 0.0002038393205355982, 'epoch': 3.61}
{'loss': 1.0975, 'grad_norm': 877.6589965820312, 'learning_rate': 0.0002037093209689301, 'epoch': 3.62}
{'loss': 1.0996, 'grad_norm': 992.3470458984375, 'learning_rate': 0.00020357932140226197, 'epoch': 3.62}
{'loss': 1.0993, 'grad_norm': 87.30243682861328, 'learning_rate': 0.00020344932183559387, 'epoch': 3.63}
{'loss': 1.0981, 'grad_norm': 377.98095703125, 'learning_rate': 0.00020331932226892575, 'epoch': 3.63}
{'loss': 1.1, 'grad_norm': 1096.8970947265625, 'learning_rate': 0.00020318932270225763, 'epoch': 3.63}
{'loss': 1.101, 'grad_norm': 171.4669189453125, 'learning_rate': 0.00020305932313558956, 'epoch': 3.64}
{'loss': 1.1003, 'grad_norm': 995.8238525390625, 'learning_rate': 0.00020292932356892143, 'epoch': 3.64}
{'loss': 1.0994, 'grad_norm': 478.4693908691406, 'learning_rate': 0.0002027993240022533, 'epoch': 3.65}
{'loss': 1.1009, 'grad_norm': 1940.9527587890625, 'learning_rate': 0.0002026693244355852, 'epoch': 3.65}
{'loss': 1.0985, 'grad_norm': 23.123641967773438, 'learning_rate': 0.0002025393248689171, 'epoch': 3.65}
{'loss': 1.0989, 'grad_norm': 752.513427734375, 'learning_rate': 0.00020240932530224897, 'epoch': 3.66}
{'loss': 1.0986, 'grad_norm': 3990.700927734375, 'learning_rate': 0.00020227932573558085, 'epoch': 3.66}
{'loss': 1.1009, 'grad_norm': 38.1318473815918, 'learning_rate': 0.00020214932616891275, 'epoch': 3.67}
{'loss': 1.0986, 'grad_norm': 14957.330078125, 'learning_rate': 0.00020201932660224466, 'epoch': 3.67}
{'loss': 1.1007, 'grad_norm': 3358.528564453125, 'learning_rate': 0.00020188932703557653, 'epoch': 3.67}
{'loss': 1.0987, 'grad_norm': 144.19903564453125, 'learning_rate': 0.00020175932746890844, 'epoch': 3.68}
{'loss': 1.1025, 'grad_norm': 113819.515625, 'learning_rate': 0.00020162932790224032, 'epoch': 3.68}
{'loss': 1.1025, 'grad_norm': 187.33718872070312, 'learning_rate': 0.0002014993283355722, 'epoch': 3.69}
{'loss': 1.0981, 'grad_norm': 1300500.625, 'learning_rate': 0.00020136932876890407, 'epoch': 3.69}
{'loss': 1.0981, 'grad_norm': 266.3699645996094, 'learning_rate': 0.00020123932920223598, 'epoch': 3.69}
{'loss': 1.0996, 'grad_norm': 5201.7060546875, 'learning_rate': 0.00020110932963556785, 'epoch': 3.7}
{'loss': 1.0973, 'grad_norm': 5770.81640625, 'learning_rate': 0.00020097933006889976, 'epoch': 3.7}
{'loss': 1.0998, 'grad_norm': 357.21112060546875, 'learning_rate': 0.00020084933050223166, 'epoch': 3.71}
{'loss': 1.0985, 'grad_norm': 1145.1729736328125, 'learning_rate': 0.00020071933093556354, 'epoch': 3.71}
{'loss': 1.0986, 'grad_norm': 130.08180236816406, 'learning_rate': 0.00020058933136889542, 'epoch': 3.71}
{'loss': 1.1013, 'grad_norm': 1440.8572998046875, 'learning_rate': 0.0002004593318022273, 'epoch': 3.72}
{'loss': 1.1004, 'grad_norm': 644.0031127929688, 'learning_rate': 0.0002003293322355592, 'epoch': 3.72}
{'loss': 1.1013, 'grad_norm': 3550.54638671875, 'learning_rate': 0.00020019933266889108, 'epoch': 3.73}
{'loss': 1.0988, 'grad_norm': 72.19271850585938, 'learning_rate': 0.00020006933310222295, 'epoch': 3.73}
{'loss': 1.099, 'grad_norm': 31.808374404907227, 'learning_rate': 0.00019993933353555489, 'epoch': 3.74}
{'loss': 1.0983, 'grad_norm': 506.4920959472656, 'learning_rate': 0.00019980933396888676, 'epoch': 3.74}
{'loss': 1.1009, 'grad_norm': 3405.3125, 'learning_rate': 0.00019967933440221864, 'epoch': 3.74}
{'loss': 1.1006, 'grad_norm': 792.3046875, 'learning_rate': 0.00019954933483555055, 'epoch': 3.75}
{'loss': 1.1022, 'grad_norm': 118.76380920410156, 'learning_rate': 0.00019941933526888242, 'epoch': 3.75}
{'loss': 1.0996, 'grad_norm': 4700.83056640625, 'learning_rate': 0.0001992893357022143, 'epoch': 3.76}
{'loss': 1.0992, 'grad_norm': 638.4961547851562, 'learning_rate': 0.00019915933613554618, 'epoch': 3.76}
{'loss': 1.1015, 'grad_norm': 8282.5751953125, 'learning_rate': 0.00019902933656887808, 'epoch': 3.76}
{'loss': 1.0994, 'grad_norm': 284.7354431152344, 'learning_rate': 0.00019889933700220999, 'epoch': 3.77}
{'loss': 1.0991, 'grad_norm': 1010.7774658203125, 'learning_rate': 0.00019876933743554186, 'epoch': 3.77}
{'loss': 1.1011, 'grad_norm': 9951.7861328125, 'learning_rate': 0.00019863933786887377, 'epoch': 3.78}
{'loss': 1.0989, 'grad_norm': 429.7512512207031, 'learning_rate': 0.00019850933830220565, 'epoch': 3.78}
{'loss': 1.1019, 'grad_norm': 1833.7279052734375, 'learning_rate': 0.00019837933873553752, 'epoch': 3.78}
{'loss': 1.1008, 'grad_norm': 2152.802978515625, 'learning_rate': 0.00019824933916886943, 'epoch': 3.79}
{'loss': 1.1005, 'grad_norm': 384.30267333984375, 'learning_rate': 0.0001981193396022013, 'epoch': 3.79}
{'loss': 1.0993, 'grad_norm': 268.7261047363281, 'learning_rate': 0.00019798934003553318, 'epoch': 3.8}
{'loss': 1.0993, 'grad_norm': 11094.654296875, 'learning_rate': 0.00019785934046886511, 'epoch': 3.8}
{'loss': 1.0982, 'grad_norm': 805.3195190429688, 'learning_rate': 0.000197729340902197, 'epoch': 3.8}
{'loss': 1.0993, 'grad_norm': 223.0296173095703, 'learning_rate': 0.00019759934133552887, 'epoch': 3.81}
{'loss': 1.0999, 'grad_norm': 889.8494873046875, 'learning_rate': 0.00019746934176886075, 'epoch': 3.81}
{'loss': 1.099, 'grad_norm': 1303.0650634765625, 'learning_rate': 0.00019733934220219265, 'epoch': 3.82}
{'loss': 1.0978, 'grad_norm': 1127.94677734375, 'learning_rate': 0.00019720934263552453, 'epoch': 3.82}
{'loss': 1.0983, 'grad_norm': 797.6007690429688, 'learning_rate': 0.0001970793430688564, 'epoch': 3.82}
{'loss': 1.0984, 'grad_norm': 673.6476440429688, 'learning_rate': 0.00019694934350218828, 'epoch': 3.83}
{'loss': 1.1, 'grad_norm': 183.65518188476562, 'learning_rate': 0.00019681934393552021, 'epoch': 3.83}
{'loss': 1.1007, 'grad_norm': 2694.108154296875, 'learning_rate': 0.0001966893443688521, 'epoch': 3.84}
{'loss': 1.0987, 'grad_norm': 20.60422134399414, 'learning_rate': 0.00019655934480218397, 'epoch': 3.84}
{'loss': 1.0979, 'grad_norm': 919.5035400390625, 'learning_rate': 0.00019642934523551587, 'epoch': 3.85}
{'loss': 1.1007, 'grad_norm': 27.562654495239258, 'learning_rate': 0.00019629934566884775, 'epoch': 3.85}
{'loss': 1.1012, 'grad_norm': 21.754901885986328, 'learning_rate': 0.00019616934610217963, 'epoch': 3.85}
{'loss': 1.1008, 'grad_norm': 110.56519317626953, 'learning_rate': 0.00019603934653551153, 'epoch': 3.86}
{'loss': 1.1008, 'grad_norm': 747.4630126953125, 'learning_rate': 0.0001959093469688434, 'epoch': 3.86}
{'loss': 1.0993, 'grad_norm': 946.9974365234375, 'learning_rate': 0.00019577934740217532, 'epoch': 3.87}
{'loss': 1.0998, 'grad_norm': 74.0838623046875, 'learning_rate': 0.00019564934783550722, 'epoch': 3.87}
{'loss': 1.1019, 'grad_norm': 6962.375, 'learning_rate': 0.0001955193482688391, 'epoch': 3.87}
{'loss': 1.1014, 'grad_norm': 2318.816162109375, 'learning_rate': 0.00019538934870217097, 'epoch': 3.88}
{'loss': 1.0996, 'grad_norm': 96.23416137695312, 'learning_rate': 0.00019525934913550285, 'epoch': 3.88}
{'loss': 1.0994, 'grad_norm': 4525.19189453125, 'learning_rate': 0.00019512934956883476, 'epoch': 3.89}
{'loss': 1.1009, 'grad_norm': 76.61190032958984, 'learning_rate': 0.00019499935000216663, 'epoch': 3.89}
{'loss': 1.1018, 'grad_norm': 2419.273681640625, 'learning_rate': 0.0001948693504354985, 'epoch': 3.89}
{'loss': 1.0974, 'grad_norm': 178.2152557373047, 'learning_rate': 0.00019473935086883044, 'epoch': 3.9}
{'loss': 1.1001, 'grad_norm': 128.76014709472656, 'learning_rate': 0.00019460935130216232, 'epoch': 3.9}
{'loss': 1.0979, 'grad_norm': 1526.5552978515625, 'learning_rate': 0.0001944793517354942, 'epoch': 3.91}
{'loss': 1.1035, 'grad_norm': 148627.734375, 'learning_rate': 0.0001943493521688261, 'epoch': 3.91}
{'loss': 1.0995, 'grad_norm': 8791.0732421875, 'learning_rate': 0.00019421935260215798, 'epoch': 3.91}
{'loss': 1.1002, 'grad_norm': 78.33341979980469, 'learning_rate': 0.00019408935303548986, 'epoch': 3.92}
{'loss': 1.1009, 'grad_norm': 483.0299987792969, 'learning_rate': 0.00019395935346882173, 'epoch': 3.92}
{'loss': 1.0987, 'grad_norm': 3752.765380859375, 'learning_rate': 0.00019382935390215364, 'epoch': 3.93}
{'loss': 1.0977, 'grad_norm': 136.36859130859375, 'learning_rate': 0.00019369935433548554, 'epoch': 3.93}
{'loss': 1.1005, 'grad_norm': 168.86695861816406, 'learning_rate': 0.00019356935476881742, 'epoch': 3.93}
{'loss': 1.1012, 'grad_norm': 177.81820678710938, 'learning_rate': 0.00019343935520214933, 'epoch': 3.94}
{'loss': 1.1001, 'grad_norm': 177.80191040039062, 'learning_rate': 0.0001933093556354812, 'epoch': 3.94}
{'loss': 1.1008, 'grad_norm': 399.68487548828125, 'learning_rate': 0.00019317935606881308, 'epoch': 3.95}
{'loss': 1.1013, 'grad_norm': 1203.8504638671875, 'learning_rate': 0.00019304935650214496, 'epoch': 3.95}
{'loss': 1.0981, 'grad_norm': 55.07851791381836, 'learning_rate': 0.00019291935693547686, 'epoch': 3.96}
{'loss': 1.1002, 'grad_norm': 33.19650650024414, 'learning_rate': 0.00019278935736880874, 'epoch': 3.96}
{'loss': 1.1007, 'grad_norm': 175.32901000976562, 'learning_rate': 0.00019265935780214064, 'epoch': 3.96}
{'loss': 1.0997, 'grad_norm': 47.74339294433594, 'learning_rate': 0.00019252935823547255, 'epoch': 3.97}
{'loss': 1.1005, 'grad_norm': 150.480224609375, 'learning_rate': 0.00019239935866880443, 'epoch': 3.97}
{'loss': 1.0989, 'grad_norm': 198.95706176757812, 'learning_rate': 0.0001922693591021363, 'epoch': 3.98}
{'loss': 1.1002, 'grad_norm': 4420.26220703125, 'learning_rate': 0.0001921393595354682, 'epoch': 3.98}
{'loss': 1.1007, 'grad_norm': 520.0309448242188, 'learning_rate': 0.00019200935996880009, 'epoch': 3.98}
{'loss': 1.1011, 'grad_norm': 1920.39990234375, 'learning_rate': 0.00019187936040213196, 'epoch': 3.99}
{'loss': 1.1038, 'grad_norm': 57851.30078125, 'learning_rate': 0.00019174936083546384, 'epoch': 3.99}
{'loss': 1.1009, 'grad_norm': 232.2328643798828, 'learning_rate': 0.00019161936126879577, 'epoch': 4.0}
{'loss': 1.1005, 'grad_norm': 284.4908752441406, 'learning_rate': 0.00019148936170212765, 'epoch': 4.0}
{'eval_loss': 1.0971018075942993, 'eval_accuracy': 0.3544574630667346, 'eval_runtime': 5.3904, 'eval_samples_per_second': 1820.822, 'eval_steps_per_second': 11.502, 'epoch': 4.0}
{'loss': 1.1085, 'grad_norm': 2171.962890625, 'learning_rate': 0.00019135936213545953, 'epoch': 4.0}
{'loss': 1.1248, 'grad_norm': 204.6683349609375, 'learning_rate': 0.00019122936256879143, 'epoch': 4.01}
{'loss': 1.1114, 'grad_norm': 515.24267578125, 'learning_rate': 0.0001910993630021233, 'epoch': 4.01}
{'loss': 1.1027, 'grad_norm': 7240.2626953125, 'learning_rate': 0.0001909693634354552, 'epoch': 4.02}
{'loss': 1.1008, 'grad_norm': 15464.751953125, 'learning_rate': 0.00019083936386878706, 'epoch': 4.02}
{'loss': 1.1012, 'grad_norm': 114.5369644165039, 'learning_rate': 0.00019070936430211897, 'epoch': 4.02}
{'loss': 1.1011, 'grad_norm': 2878.2255859375, 'learning_rate': 0.00019057936473545087, 'epoch': 4.03}
{'loss': 1.102, 'grad_norm': 3156.156982421875, 'learning_rate': 0.00019044936516878275, 'epoch': 4.03}
{'loss': 1.1012, 'grad_norm': 234.94271850585938, 'learning_rate': 0.00019031936560211465, 'epoch': 4.04}
{'loss': 1.0991, 'grad_norm': 3409.418212890625, 'learning_rate': 0.00019018936603544653, 'epoch': 4.04}
{'loss': 1.1003, 'grad_norm': 1235.958740234375, 'learning_rate': 0.0001900593664687784, 'epoch': 4.04}
{'loss': 1.1004, 'grad_norm': 259.891357421875, 'learning_rate': 0.00018992936690211031, 'epoch': 4.05}
{'loss': 1.1, 'grad_norm': 269307.59375, 'learning_rate': 0.0001897993673354422, 'epoch': 4.05}
{'loss': 1.1039, 'grad_norm': 112.54847717285156, 'learning_rate': 0.00018966936776877407, 'epoch': 4.06}
{'loss': 1.0995, 'grad_norm': 13126.0478515625, 'learning_rate': 0.000189539368202106, 'epoch': 4.06}
{'loss': 1.0987, 'grad_norm': 81993.0859375, 'learning_rate': 0.00018940936863543788, 'epoch': 4.07}
{'loss': 1.0995, 'grad_norm': 91155.984375, 'learning_rate': 0.00018927936906876976, 'epoch': 4.07}
{'loss': 1.0996, 'grad_norm': 311.607666015625, 'learning_rate': 0.00018914936950210163, 'epoch': 4.07}
{'loss': 1.0989, 'grad_norm': 63.73005676269531, 'learning_rate': 0.00018901936993543354, 'epoch': 4.08}
{'loss': 1.1002, 'grad_norm': 24645.841796875, 'learning_rate': 0.00018888937036876541, 'epoch': 4.08}
{'loss': 1.0997, 'grad_norm': 394.5324401855469, 'learning_rate': 0.0001887593708020973, 'epoch': 4.09}
{'loss': 1.0993, 'grad_norm': 1238.8013916015625, 'learning_rate': 0.00018862937123542917, 'epoch': 4.09}
{'loss': 1.1, 'grad_norm': 4869.04443359375, 'learning_rate': 0.0001884993716687611, 'epoch': 4.09}
{'loss': 1.0991, 'grad_norm': 88871.0859375, 'learning_rate': 0.00018836937210209298, 'epoch': 4.1}
{'loss': 1.0979, 'grad_norm': 1206.3670654296875, 'learning_rate': 0.00018823937253542488, 'epoch': 4.1}
{'loss': 1.0979, 'grad_norm': 675.1392211914062, 'learning_rate': 0.00018810937296875676, 'epoch': 4.11}
{'loss': 1.1, 'grad_norm': 3527.999755859375, 'learning_rate': 0.00018797937340208864, 'epoch': 4.11}
{'loss': 1.0996, 'grad_norm': 1825.5125732421875, 'learning_rate': 0.00018784937383542052, 'epoch': 4.11}
{'loss': 1.0994, 'grad_norm': 20978.39453125, 'learning_rate': 0.00018771937426875242, 'epoch': 4.12}
{'loss': 1.0989, 'grad_norm': 49952.69921875, 'learning_rate': 0.0001875893747020843, 'epoch': 4.12}
{'loss': 1.0999, 'grad_norm': 475.80926513671875, 'learning_rate': 0.0001874593751354162, 'epoch': 4.13}
{'loss': 1.1002, 'grad_norm': 36024.57421875, 'learning_rate': 0.0001873293755687481, 'epoch': 4.13}
{'loss': 1.0991, 'grad_norm': 38420.56640625, 'learning_rate': 0.00018719937600207998, 'epoch': 4.13}
{'loss': 1.0985, 'grad_norm': 207.9063262939453, 'learning_rate': 0.00018706937643541186, 'epoch': 4.14}
{'loss': 1.0992, 'grad_norm': 3932.887451171875, 'learning_rate': 0.00018693937686874374, 'epoch': 4.14}
{'loss': 1.0997, 'grad_norm': 3165.097412109375, 'learning_rate': 0.00018680937730207564, 'epoch': 4.15}
{'loss': 1.1, 'grad_norm': 7553.4794921875, 'learning_rate': 0.00018667937773540752, 'epoch': 4.15}
{'loss': 1.1003, 'grad_norm': 1091.120361328125, 'learning_rate': 0.0001865493781687394, 'epoch': 4.15}
{'loss': 1.0999, 'grad_norm': 202664.75, 'learning_rate': 0.00018641937860207133, 'epoch': 4.16}
{'loss': 1.0989, 'grad_norm': 10042.1328125, 'learning_rate': 0.0001862893790354032, 'epoch': 4.16}
{'loss': 1.0999, 'grad_norm': 174.83485412597656, 'learning_rate': 0.00018615937946873508, 'epoch': 4.17}
{'loss': 1.0971, 'grad_norm': 4139.3515625, 'learning_rate': 0.000186029379902067, 'epoch': 4.17}
{'loss': 1.0982, 'grad_norm': 1698.4813232421875, 'learning_rate': 0.00018589938033539887, 'epoch': 4.18}
{'loss': 1.0997, 'grad_norm': 12828.419921875, 'learning_rate': 0.00018576938076873074, 'epoch': 4.18}
{'loss': 1.1006, 'grad_norm': 3194.551025390625, 'learning_rate': 0.00018563938120206262, 'epoch': 4.18}
{'loss': 1.0999, 'grad_norm': 418.87213134765625, 'learning_rate': 0.00018550938163539453, 'epoch': 4.19}
{'loss': 1.1005, 'grad_norm': 35184.56640625, 'learning_rate': 0.00018537938206872643, 'epoch': 4.19}
{'loss': 1.099, 'grad_norm': 94.29362487792969, 'learning_rate': 0.0001852493825020583, 'epoch': 4.2}
{'loss': 1.0991, 'grad_norm': 103475.078125, 'learning_rate': 0.0001851193829353902, 'epoch': 4.2}
{'loss': 1.0978, 'grad_norm': 1385.266845703125, 'learning_rate': 0.0001849893833687221, 'epoch': 4.2}
{'loss': 1.0965, 'grad_norm': 1821.5875244140625, 'learning_rate': 0.00018485938380205397, 'epoch': 4.21}
{'loss': 1.0992, 'grad_norm': 9978.349609375, 'learning_rate': 0.00018472938423538584, 'epoch': 4.21}
{'loss': 1.1008, 'grad_norm': 52445.79296875, 'learning_rate': 0.00018459938466871775, 'epoch': 4.22}
{'loss': 1.0997, 'grad_norm': 16081.314453125, 'learning_rate': 0.00018446938510204963, 'epoch': 4.22}
{'loss': 1.0995, 'grad_norm': 955.3705444335938, 'learning_rate': 0.00018433938553538156, 'epoch': 4.22}
{'loss': 1.0997, 'grad_norm': 2031.585693359375, 'learning_rate': 0.00018420938596871344, 'epoch': 4.23}
{'loss': 1.0979, 'grad_norm': 20910.90234375, 'learning_rate': 0.0001840793864020453, 'epoch': 4.23}
{'loss': 1.0986, 'grad_norm': 52545.17578125, 'learning_rate': 0.0001839493868353772, 'epoch': 4.24}
{'loss': 1.0991, 'grad_norm': 7357.09716796875, 'learning_rate': 0.0001838193872687091, 'epoch': 4.24}
{'loss': 1.1019, 'grad_norm': 41997.70703125, 'learning_rate': 0.00018368938770204097, 'epoch': 4.24}
{'loss': 1.1012, 'grad_norm': 766.8067016601562, 'learning_rate': 0.00018355938813537285, 'epoch': 4.25}
{'loss': 1.1015, 'grad_norm': 174610.09375, 'learning_rate': 0.00018342938856870478, 'epoch': 4.25}
{'loss': 1.0999, 'grad_norm': 7785.185546875, 'learning_rate': 0.00018329938900203666, 'epoch': 4.26}
{'loss': 1.1, 'grad_norm': 375.89471435546875, 'learning_rate': 0.00018316938943536854, 'epoch': 4.26}
{'loss': 1.0997, 'grad_norm': 3932.38720703125, 'learning_rate': 0.00018303938986870041, 'epoch': 4.26}
{'loss': 1.0995, 'grad_norm': 60680.171875, 'learning_rate': 0.00018290939030203232, 'epoch': 4.27}
{'loss': 1.1, 'grad_norm': 5752.6552734375, 'learning_rate': 0.0001827793907353642, 'epoch': 4.27}
{'loss': 1.0998, 'grad_norm': 4141.5, 'learning_rate': 0.00018264939116869607, 'epoch': 4.28}
{'loss': 1.1004, 'grad_norm': 1774.292724609375, 'learning_rate': 0.00018251939160202798, 'epoch': 4.28}
{'loss': 1.098, 'grad_norm': 4118.8505859375, 'learning_rate': 0.00018238939203535988, 'epoch': 4.29}
{'loss': 1.1007, 'grad_norm': 183.44424438476562, 'learning_rate': 0.00018225939246869176, 'epoch': 4.29}
{'loss': 1.0994, 'grad_norm': 591.7567138671875, 'learning_rate': 0.00018212939290202366, 'epoch': 4.29}
{'loss': 1.1015, 'grad_norm': 333.84600830078125, 'learning_rate': 0.00018199939333535554, 'epoch': 4.3}
{'loss': 1.0998, 'grad_norm': 1197.9742431640625, 'learning_rate': 0.00018186939376868742, 'epoch': 4.3}
{'loss': 1.0995, 'grad_norm': 93792.171875, 'learning_rate': 0.0001817393942020193, 'epoch': 4.31}
{'loss': 1.0988, 'grad_norm': 22002.099609375, 'learning_rate': 0.0001816093946353512, 'epoch': 4.31}
{'loss': 1.0989, 'grad_norm': 1554.97314453125, 'learning_rate': 0.00018147939506868308, 'epoch': 4.31}
{'loss': 1.0991, 'grad_norm': 24537.435546875, 'learning_rate': 0.00018134939550201498, 'epoch': 4.32}
{'loss': 1.0987, 'grad_norm': 19859.376953125, 'learning_rate': 0.0001812193959353469, 'epoch': 4.32}
{'loss': 1.0988, 'grad_norm': 66961.015625, 'learning_rate': 0.00018108939636867876, 'epoch': 4.33}
{'loss': 1.0998, 'grad_norm': 47598.984375, 'learning_rate': 0.00018095939680201064, 'epoch': 4.33}
{'loss': 1.1012, 'grad_norm': 1461.7548828125, 'learning_rate': 0.00018082939723534252, 'epoch': 4.33}
{'loss': 1.1014, 'grad_norm': 14968.125, 'learning_rate': 0.00018069939766867442, 'epoch': 4.34}
{'loss': 1.0979, 'grad_norm': 1816.2369384765625, 'learning_rate': 0.0001805693981020063, 'epoch': 4.34}
{'loss': 1.0999, 'grad_norm': 256.3465881347656, 'learning_rate': 0.00018043939853533818, 'epoch': 4.35}
{'loss': 1.099, 'grad_norm': 9873.302734375, 'learning_rate': 0.0001803093989686701, 'epoch': 4.35}
{'loss': 1.0999, 'grad_norm': 2764.02880859375, 'learning_rate': 0.000180179399402002, 'epoch': 4.35}
{'loss': 1.0981, 'grad_norm': 6622.49462890625, 'learning_rate': 0.00018004939983533387, 'epoch': 4.36}
{'loss': 1.0979, 'grad_norm': 1521.8309326171875, 'learning_rate': 0.00017991940026866577, 'epoch': 4.36}
{'loss': 1.0994, 'grad_norm': 63935.63671875, 'learning_rate': 0.00017978940070199765, 'epoch': 4.37}
{'loss': 1.0991, 'grad_norm': 621256.25, 'learning_rate': 0.00017965940113532952, 'epoch': 4.37}
{'loss': 1.0986, 'grad_norm': 10224.736328125, 'learning_rate': 0.0001795294015686614, 'epoch': 4.37}
{'loss': 1.0999, 'grad_norm': 6638.06494140625, 'learning_rate': 0.0001793994020019933, 'epoch': 4.38}
{'loss': 1.0981, 'grad_norm': 62014.72265625, 'learning_rate': 0.0001792694024353252, 'epoch': 4.38}
{'loss': 1.0982, 'grad_norm': 2950.867431640625, 'learning_rate': 0.0001791394028686571, 'epoch': 4.39}
{'loss': 1.0993, 'grad_norm': 2865.622802734375, 'learning_rate': 0.000179009403301989, 'epoch': 4.39}
{'loss': 1.1002, 'grad_norm': 2307.169189453125, 'learning_rate': 0.00017887940373532087, 'epoch': 4.4}
{'loss': 1.1013, 'grad_norm': 272.2221374511719, 'learning_rate': 0.00017874940416865275, 'epoch': 4.4}
{'loss': 1.0975, 'grad_norm': 1179716.5, 'learning_rate': 0.00017861940460198465, 'epoch': 4.4}
{'loss': 1.1006, 'grad_norm': 20748.8515625, 'learning_rate': 0.00017848940503531653, 'epoch': 4.41}
{'loss': 1.1001, 'grad_norm': 4541.00927734375, 'learning_rate': 0.0001783594054686484, 'epoch': 4.41}
{'loss': 1.099, 'grad_norm': 78044.0625, 'learning_rate': 0.00017822940590198034, 'epoch': 4.42}
{'loss': 1.0991, 'grad_norm': 726.8689575195312, 'learning_rate': 0.00017809940633531222, 'epoch': 4.42}
{'loss': 1.0997, 'grad_norm': 2402.123291015625, 'learning_rate': 0.0001779694067686441, 'epoch': 4.42}
{'loss': 1.1001, 'grad_norm': 5072.166015625, 'learning_rate': 0.00017783940720197597, 'epoch': 4.43}
{'loss': 1.1002, 'grad_norm': 1085.7296142578125, 'learning_rate': 0.00017770940763530788, 'epoch': 4.43}
{'loss': 1.0996, 'grad_norm': 148857.546875, 'learning_rate': 0.00017757940806863975, 'epoch': 4.44}
{'loss': 1.0982, 'grad_norm': 88937736.0, 'learning_rate': 0.00017744940850197163, 'epoch': 4.44}
{'loss': 1.0999, 'grad_norm': 5502.79833984375, 'learning_rate': 0.0001773194089353035, 'epoch': 4.44}
{'loss': 1.0986, 'grad_norm': 2794.05419921875, 'learning_rate': 0.00017718940936863544, 'epoch': 4.45}
{'loss': 1.0993, 'grad_norm': 105850.8359375, 'learning_rate': 0.00017705940980196732, 'epoch': 4.45}
{'loss': 1.0987, 'grad_norm': 30365.451171875, 'learning_rate': 0.0001769294102352992, 'epoch': 4.46}
{'loss': 1.0994, 'grad_norm': 11641.7958984375, 'learning_rate': 0.0001767994106686311, 'epoch': 4.46}
{'loss': 1.0984, 'grad_norm': 2865.3251953125, 'learning_rate': 0.00017666941110196298, 'epoch': 4.46}
{'loss': 1.1009, 'grad_norm': 354746.40625, 'learning_rate': 0.00017653941153529485, 'epoch': 4.47}
{'loss': 1.1003, 'grad_norm': 985.4796752929688, 'learning_rate': 0.00017640941196862676, 'epoch': 4.47}
{'loss': 1.0987, 'grad_norm': 139255.453125, 'learning_rate': 0.00017627941240195864, 'epoch': 4.48}
{'loss': 1.1002, 'grad_norm': 94222.4765625, 'learning_rate': 0.00017614941283529054, 'epoch': 4.48}
{'loss': 1.1001, 'grad_norm': 1169971.0, 'learning_rate': 0.00017601941326862244, 'epoch': 4.48}
{'loss': 1.0988, 'grad_norm': 782681.625, 'learning_rate': 0.00017588941370195432, 'epoch': 4.49}
{'loss': 1.0997, 'grad_norm': 1892.7825927734375, 'learning_rate': 0.0001757594141352862, 'epoch': 4.49}
{'loss': 1.0974, 'grad_norm': 224368.859375, 'learning_rate': 0.00017562941456861808, 'epoch': 4.5}
{'loss': 1.1002, 'grad_norm': 493908.3125, 'learning_rate': 0.00017549941500194998, 'epoch': 4.5}
{'loss': 1.0996, 'grad_norm': 2175.47119140625, 'learning_rate': 0.00017536941543528186, 'epoch': 4.51}
{'loss': 1.1003, 'grad_norm': 1021172.8125, 'learning_rate': 0.00017523941586861374, 'epoch': 4.51}
{'loss': 1.0992, 'grad_norm': 38256.3984375, 'learning_rate': 0.00017510941630194567, 'epoch': 4.51}
{'loss': 1.0982, 'grad_norm': 14647.76171875, 'learning_rate': 0.00017497941673527755, 'epoch': 4.52}
{'loss': 1.0987, 'grad_norm': 21236610.0, 'learning_rate': 0.00017484941716860942, 'epoch': 4.52}
{'loss': 1.0978, 'grad_norm': 68860.59375, 'learning_rate': 0.00017471941760194133, 'epoch': 4.53}
{'loss': 1.0991, 'grad_norm': 2709.29541015625, 'learning_rate': 0.0001745894180352732, 'epoch': 4.53}
{'loss': 1.1003, 'grad_norm': 17502.30078125, 'learning_rate': 0.00017445941846860508, 'epoch': 4.53}
{'loss': 1.0984, 'grad_norm': 33982.578125, 'learning_rate': 0.00017432941890193696, 'epoch': 4.54}
{'loss': 1.0988, 'grad_norm': 31950.16796875, 'learning_rate': 0.00017419941933526886, 'epoch': 4.54}
{'loss': 1.1001, 'grad_norm': 133.885498046875, 'learning_rate': 0.00017406941976860077, 'epoch': 4.55}
{'loss': 1.0998, 'grad_norm': 4389955.5, 'learning_rate': 0.00017393942020193265, 'epoch': 4.55}
{'loss': 1.0983, 'grad_norm': 4157.39697265625, 'learning_rate': 0.00017380942063526455, 'epoch': 4.55}
{'loss': 1.1001, 'grad_norm': 11928.6494140625, 'learning_rate': 0.00017367942106859643, 'epoch': 4.56}
{'loss': 1.0992, 'grad_norm': 3279.302734375, 'learning_rate': 0.0001735494215019283, 'epoch': 4.56}
{'loss': 1.0978, 'grad_norm': 14804.216796875, 'learning_rate': 0.00017341942193526018, 'epoch': 4.57}
{'loss': 1.0977, 'grad_norm': 3749.14501953125, 'learning_rate': 0.0001732894223685921, 'epoch': 4.57}
{'loss': 1.0991, 'grad_norm': 90588.2578125, 'learning_rate': 0.00017315942280192396, 'epoch': 4.57}
{'loss': 1.098, 'grad_norm': 184007.390625, 'learning_rate': 0.00017302942323525587, 'epoch': 4.58}
{'loss': 1.099, 'grad_norm': 2380.230712890625, 'learning_rate': 0.00017289942366858777, 'epoch': 4.58}
{'loss': 1.0992, 'grad_norm': 11185.556640625, 'learning_rate': 0.00017276942410191965, 'epoch': 4.59}
{'loss': 1.0986, 'grad_norm': 5384.3662109375, 'learning_rate': 0.00017263942453525153, 'epoch': 4.59}
{'loss': 1.0999, 'grad_norm': 12846.125, 'learning_rate': 0.00017250942496858343, 'epoch': 4.59}
{'loss': 1.0987, 'grad_norm': 79097.3359375, 'learning_rate': 0.0001723794254019153, 'epoch': 4.6}
{'loss': 1.0991, 'grad_norm': 7137.71484375, 'learning_rate': 0.0001722494258352472, 'epoch': 4.6}
{'loss': 1.0985, 'grad_norm': 111466.7890625, 'learning_rate': 0.00017211942626857907, 'epoch': 4.61}
{'loss': 1.0978, 'grad_norm': 2732.083984375, 'learning_rate': 0.000171989426701911, 'epoch': 4.61}
{'loss': 1.0991, 'grad_norm': 24599.53125, 'learning_rate': 0.00017185942713524287, 'epoch': 4.62}
{'loss': 1.0991, 'grad_norm': 21350.375, 'learning_rate': 0.00017172942756857475, 'epoch': 4.62}
{'loss': 1.099, 'grad_norm': 7477.63720703125, 'learning_rate': 0.00017159942800190666, 'epoch': 4.62}
{'loss': 1.0989, 'grad_norm': 12014.2255859375, 'learning_rate': 0.00017146942843523853, 'epoch': 4.63}
{'loss': 1.0987, 'grad_norm': 24680.908203125, 'learning_rate': 0.0001713394288685704, 'epoch': 4.63}
{'loss': 1.0983, 'grad_norm': 1714.1048583984375, 'learning_rate': 0.0001712094293019023, 'epoch': 4.64}
{'loss': 1.0987, 'grad_norm': 1556.60693359375, 'learning_rate': 0.0001710794297352342, 'epoch': 4.64}
{'loss': 1.0999, 'grad_norm': 13717.3330078125, 'learning_rate': 0.0001709494301685661, 'epoch': 4.64}
{'loss': 1.0991, 'grad_norm': 21181.44140625, 'learning_rate': 0.00017081943060189798, 'epoch': 4.65}
{'loss': 1.0989, 'grad_norm': 4190.13623046875, 'learning_rate': 0.00017068943103522988, 'epoch': 4.65}
{'loss': 1.0983, 'grad_norm': 2557.349609375, 'learning_rate': 0.00017055943146856176, 'epoch': 4.66}
{'loss': 1.0996, 'grad_norm': 27504.666015625, 'learning_rate': 0.00017042943190189363, 'epoch': 4.66}
{'loss': 1.0991, 'grad_norm': 3848.03564453125, 'learning_rate': 0.00017029943233522554, 'epoch': 4.66}
{'loss': 1.0998, 'grad_norm': 858.820068359375, 'learning_rate': 0.00017016943276855742, 'epoch': 4.67}
{'loss': 1.1039, 'grad_norm': 592.8037109375, 'learning_rate': 0.0001700394332018893, 'epoch': 4.67}
{'loss': 1.1035, 'grad_norm': 78929.78125, 'learning_rate': 0.00016990943363522123, 'epoch': 4.68}
{'loss': 1.1029, 'grad_norm': 780.874267578125, 'learning_rate': 0.0001697794340685531, 'epoch': 4.68}
{'loss': 1.1019, 'grad_norm': 513.1002807617188, 'learning_rate': 0.00016964943450188498, 'epoch': 4.68}
{'loss': 1.0994, 'grad_norm': 695.2036743164062, 'learning_rate': 0.00016951943493521686, 'epoch': 4.69}
{'loss': 1.0999, 'grad_norm': 316407.03125, 'learning_rate': 0.00016938943536854876, 'epoch': 4.69}
{'loss': 1.0993, 'grad_norm': 4080.74365234375, 'learning_rate': 0.00016925943580188064, 'epoch': 4.7}
{'loss': 1.0988, 'grad_norm': 432.8184814453125, 'learning_rate': 0.00016912943623521252, 'epoch': 4.7}
{'loss': 1.1007, 'grad_norm': 1000.844482421875, 'learning_rate': 0.0001689994366685444, 'epoch': 4.7}
{'loss': 1.0998, 'grad_norm': 1239.330322265625, 'learning_rate': 0.00016886943710187633, 'epoch': 4.71}
{'loss': 1.0982, 'grad_norm': 2055.531982421875, 'learning_rate': 0.0001687394375352082, 'epoch': 4.71}
{'loss': 1.0983, 'grad_norm': 52423.921875, 'learning_rate': 0.0001686094379685401, 'epoch': 4.72}
{'loss': 1.0996, 'grad_norm': 10394.966796875, 'learning_rate': 0.00016847943840187199, 'epoch': 4.72}
{'loss': 1.0992, 'grad_norm': 7239.9150390625, 'learning_rate': 0.00016834943883520386, 'epoch': 4.73}
{'loss': 1.0993, 'grad_norm': 46030.671875, 'learning_rate': 0.00016821943926853574, 'epoch': 4.73}
{'loss': 1.0992, 'grad_norm': 10016.5537109375, 'learning_rate': 0.00016808943970186764, 'epoch': 4.73}
{'loss': 1.0998, 'grad_norm': 332.3296813964844, 'learning_rate': 0.00016795944013519952, 'epoch': 4.74}
{'loss': 1.0985, 'grad_norm': 9146.734375, 'learning_rate': 0.00016782944056853143, 'epoch': 4.74}
{'loss': 1.0989, 'grad_norm': 4028.546875, 'learning_rate': 0.00016769944100186333, 'epoch': 4.75}
{'loss': 1.0992, 'grad_norm': 5326.3701171875, 'learning_rate': 0.0001675694414351952, 'epoch': 4.75}
{'loss': 1.0999, 'grad_norm': 967.1685180664062, 'learning_rate': 0.00016743944186852709, 'epoch': 4.75}
{'loss': 1.1001, 'grad_norm': 51135.98828125, 'learning_rate': 0.00016730944230185896, 'epoch': 4.76}
{'loss': 1.0982, 'grad_norm': 7916.568359375, 'learning_rate': 0.00016717944273519087, 'epoch': 4.76}
{'loss': 1.0993, 'grad_norm': 836.80322265625, 'learning_rate': 0.00016704944316852275, 'epoch': 4.77}
{'loss': 1.1, 'grad_norm': 881.319580078125, 'learning_rate': 0.00016691944360185462, 'epoch': 4.77}
{'loss': 1.0993, 'grad_norm': 6762572.0, 'learning_rate': 0.00016678944403518655, 'epoch': 4.77}
{'loss': 1.1, 'grad_norm': 7770.11376953125, 'learning_rate': 0.00016665944446851843, 'epoch': 4.78}
{'loss': 1.1024, 'grad_norm': 44.89048385620117, 'learning_rate': 0.0001665294449018503, 'epoch': 4.78}
{'loss': 1.0995, 'grad_norm': 6286.0751953125, 'learning_rate': 0.00016639944533518221, 'epoch': 4.79}
{'loss': 1.1009, 'grad_norm': 260.53076171875, 'learning_rate': 0.0001662694457685141, 'epoch': 4.79}
{'loss': 1.0997, 'grad_norm': 27709.419921875, 'learning_rate': 0.00016613944620184597, 'epoch': 4.79}
{'loss': 1.1046, 'grad_norm': 1762.673095703125, 'learning_rate': 0.00016600944663517785, 'epoch': 4.8}
{'loss': 1.0996, 'grad_norm': 4213.3837890625, 'learning_rate': 0.00016587944706850975, 'epoch': 4.8}
{'loss': 1.0992, 'grad_norm': 23.207273483276367, 'learning_rate': 0.00016574944750184166, 'epoch': 4.81}
{'loss': 1.1005, 'grad_norm': 55819.390625, 'learning_rate': 0.00016561944793517353, 'epoch': 4.81}
{'loss': 1.1019, 'grad_norm': 1080.027099609375, 'learning_rate': 0.00016548944836850544, 'epoch': 4.81}
{'loss': 1.1031, 'grad_norm': 317.81378173828125, 'learning_rate': 0.00016535944880183731, 'epoch': 4.82}
{'loss': 1.1022, 'grad_norm': 33253.625, 'learning_rate': 0.0001652294492351692, 'epoch': 4.82}
{'loss': 1.1016, 'grad_norm': 1536.5311279296875, 'learning_rate': 0.00016509944966850107, 'epoch': 4.83}
{'loss': 1.0995, 'grad_norm': 865.6865234375, 'learning_rate': 0.00016496945010183297, 'epoch': 4.83}
{'loss': 1.1023, 'grad_norm': 148.6259002685547, 'learning_rate': 0.00016483945053516485, 'epoch': 4.84}
{'loss': 1.0983, 'grad_norm': 40.41869354248047, 'learning_rate': 0.00016470945096849678, 'epoch': 4.84}
{'loss': 1.1009, 'grad_norm': 12593.1015625, 'learning_rate': 0.00016457945140182866, 'epoch': 4.84}
{'loss': 1.1, 'grad_norm': 28876.728515625, 'learning_rate': 0.00016444945183516054, 'epoch': 4.85}
{'loss': 1.1024, 'grad_norm': 642.3536987304688, 'learning_rate': 0.00016431945226849242, 'epoch': 4.85}
{'loss': 1.0996, 'grad_norm': 276.2447204589844, 'learning_rate': 0.00016418945270182432, 'epoch': 4.86}
{'loss': 1.1041, 'grad_norm': 74.12577819824219, 'learning_rate': 0.0001640594531351562, 'epoch': 4.86}
{'loss': 1.1, 'grad_norm': 1913.8873291015625, 'learning_rate': 0.00016392945356848807, 'epoch': 4.86}
{'loss': 1.1053, 'grad_norm': 1191.9288330078125, 'learning_rate': 0.00016379945400181995, 'epoch': 4.87}
{'loss': 1.1127, 'grad_norm': 910.28271484375, 'learning_rate': 0.00016366945443515188, 'epoch': 4.87}
{'loss': 1.1221, 'grad_norm': 292.5114440917969, 'learning_rate': 0.00016353945486848376, 'epoch': 4.88}
{'loss': 1.1106, 'grad_norm': 66.50426483154297, 'learning_rate': 0.00016340945530181564, 'epoch': 4.88}
{'loss': 1.1075, 'grad_norm': 2935.43603515625, 'learning_rate': 0.00016327945573514754, 'epoch': 4.88}
{'loss': 1.1017, 'grad_norm': 914.4641723632812, 'learning_rate': 0.00016314945616847942, 'epoch': 4.89}
{'loss': 1.1031, 'grad_norm': 84502.59375, 'learning_rate': 0.0001630194566018113, 'epoch': 4.89}
{'loss': 1.1019, 'grad_norm': 362.8294372558594, 'learning_rate': 0.0001628894570351432, 'epoch': 4.9}
{'loss': 1.1014, 'grad_norm': 2101.35302734375, 'learning_rate': 0.0001627594574684751, 'epoch': 4.9}
{'loss': 1.1056, 'grad_norm': 1297.0941162109375, 'learning_rate': 0.00016262945790180698, 'epoch': 4.9}
{'loss': 1.1013, 'grad_norm': 9777.2587890625, 'learning_rate': 0.0001624994583351389, 'epoch': 4.91}
{'loss': 1.1065, 'grad_norm': 2328.262451171875, 'learning_rate': 0.00016236945876847077, 'epoch': 4.91}
{'loss': 1.1021, 'grad_norm': 1197286.0, 'learning_rate': 0.00016223945920180264, 'epoch': 4.92}
{'loss': 1.1042, 'grad_norm': 1017.994140625, 'learning_rate': 0.00016210945963513452, 'epoch': 4.92}
{'loss': 1.1004, 'grad_norm': 1245.840087890625, 'learning_rate': 0.00016197946006846643, 'epoch': 4.92}
{'loss': 1.1013, 'grad_norm': 997.8922119140625, 'learning_rate': 0.0001618494605017983, 'epoch': 4.93}
{'loss': 1.0989, 'grad_norm': 3199.704345703125, 'learning_rate': 0.0001617194609351302, 'epoch': 4.93}
{'loss': 1.1033, 'grad_norm': 1987.934326171875, 'learning_rate': 0.0001615894613684621, 'epoch': 4.94}
{'loss': 1.1015, 'grad_norm': 395.8311767578125, 'learning_rate': 0.000161459461801794, 'epoch': 4.94}
{'loss': 1.1004, 'grad_norm': 195.3129425048828, 'learning_rate': 0.00016132946223512587, 'epoch': 4.95}
{'loss': 1.0987, 'grad_norm': 22499.16796875, 'learning_rate': 0.00016119946266845774, 'epoch': 4.95}
{'loss': 1.1, 'grad_norm': 444.7035217285156, 'learning_rate': 0.00016106946310178965, 'epoch': 4.95}
{'loss': 1.1001, 'grad_norm': 106.773681640625, 'learning_rate': 0.00016093946353512153, 'epoch': 4.96}
{'loss': 1.0972, 'grad_norm': 1464.9254150390625, 'learning_rate': 0.0001608094639684534, 'epoch': 4.96}
{'loss': 1.0991, 'grad_norm': 8194.0009765625, 'learning_rate': 0.00016067946440178534, 'epoch': 4.97}
{'loss': 1.1036, 'grad_norm': 20959.306640625, 'learning_rate': 0.0001605494648351172, 'epoch': 4.97}
{'loss': 1.0991, 'grad_norm': 173.77801513671875, 'learning_rate': 0.0001604194652684491, 'epoch': 4.97}
{'loss': 1.099, 'grad_norm': 131.6382598876953, 'learning_rate': 0.000160289465701781, 'epoch': 4.98}
{'loss': 1.0997, 'grad_norm': 20639.126953125, 'learning_rate': 0.00016015946613511287, 'epoch': 4.98}
{'loss': 1.0993, 'grad_norm': 134.19264221191406, 'learning_rate': 0.00016002946656844475, 'epoch': 4.99}
{'loss': 1.1013, 'grad_norm': 202.50045776367188, 'learning_rate': 0.00015989946700177663, 'epoch': 4.99}
{'loss': 1.1009, 'grad_norm': 218.9170379638672, 'learning_rate': 0.00015976946743510853, 'epoch': 4.99}
{'loss': 1.1045, 'grad_norm': 34752.59765625, 'learning_rate': 0.00015963946786844044, 'epoch': 5.0}
{'eval_loss': 1.1021133661270142, 'eval_accuracy': 0.31818644931227713, 'eval_runtime': 5.5044, 'eval_samples_per_second': 1783.117, 'eval_steps_per_second': 11.264, 'epoch': 5.0}
{'loss': 1.1042, 'grad_norm': 3080.7607421875, 'learning_rate': 0.0001595094683017723, 'epoch': 5.0}
{'loss': 1.1033, 'grad_norm': 2066.555908203125, 'learning_rate': 0.00015937946873510422, 'epoch': 5.01}
{'loss': 1.0988, 'grad_norm': 2121.555908203125, 'learning_rate': 0.0001592494691684361, 'epoch': 5.01}
{'loss': 1.0993, 'grad_norm': 223271.640625, 'learning_rate': 0.00015911946960176797, 'epoch': 5.01}
{'loss': 1.101, 'grad_norm': 965.6807861328125, 'learning_rate': 0.00015898947003509988, 'epoch': 5.02}
{'loss': 1.1006, 'grad_norm': 50.82534408569336, 'learning_rate': 0.00015885947046843175, 'epoch': 5.02}
{'loss': 1.1005, 'grad_norm': 174.13365173339844, 'learning_rate': 0.00015872947090176363, 'epoch': 5.03}
{'loss': 1.1046, 'grad_norm': 1718.19873046875, 'learning_rate': 0.00015859947133509556, 'epoch': 5.03}
{'loss': 1.103, 'grad_norm': 33.19874954223633, 'learning_rate': 0.00015846947176842744, 'epoch': 5.03}
{'loss': 1.1016, 'grad_norm': 167.52676391601562, 'learning_rate': 0.00015833947220175932, 'epoch': 5.04}
{'loss': 1.0998, 'grad_norm': 255005.828125, 'learning_rate': 0.0001582094726350912, 'epoch': 5.04}
{'loss': 1.1, 'grad_norm': 2257.658935546875, 'learning_rate': 0.0001580794730684231, 'epoch': 5.05}
{'loss': 1.0995, 'grad_norm': 25622.576171875, 'learning_rate': 0.00015794947350175498, 'epoch': 5.05}
{'loss': 1.101, 'grad_norm': 999.0930786132812, 'learning_rate': 0.00015781947393508686, 'epoch': 5.05}
{'loss': 1.1007, 'grad_norm': 1021.8381958007812, 'learning_rate': 0.00015768947436841873, 'epoch': 5.06}
{'loss': 1.1021, 'grad_norm': 4797.61669921875, 'learning_rate': 0.00015755947480175066, 'epoch': 5.06}
{'loss': 1.0981, 'grad_norm': 227415.15625, 'learning_rate': 0.00015742947523508254, 'epoch': 5.07}
{'loss': 1.1019, 'grad_norm': 601.6552124023438, 'learning_rate': 0.00015729947566841442, 'epoch': 5.07}
{'loss': 1.1008, 'grad_norm': 40647.67578125, 'learning_rate': 0.00015716947610174632, 'epoch': 5.08}
{'loss': 1.1024, 'grad_norm': 51770.76953125, 'learning_rate': 0.0001570394765350782, 'epoch': 5.08}
{'loss': 1.1013, 'grad_norm': 2195.96875, 'learning_rate': 0.00015690947696841008, 'epoch': 5.08}
{'loss': 1.1021, 'grad_norm': 4678.12255859375, 'learning_rate': 0.00015677947740174198, 'epoch': 5.09}
{'loss': 1.1016, 'grad_norm': 911.9075317382812, 'learning_rate': 0.00015664947783507386, 'epoch': 5.09}
{'loss': 1.1024, 'grad_norm': 185.4596710205078, 'learning_rate': 0.00015651947826840577, 'epoch': 5.1}
{'loss': 1.0997, 'grad_norm': 3076.016845703125, 'learning_rate': 0.00015638947870173767, 'epoch': 5.1}
{'loss': 1.0994, 'grad_norm': 3812.9296875, 'learning_rate': 0.00015625947913506955, 'epoch': 5.1}
{'loss': 1.0994, 'grad_norm': 106.02169799804688, 'learning_rate': 0.00015612947956840142, 'epoch': 5.11}
{'loss': 1.1008, 'grad_norm': 449.6429748535156, 'learning_rate': 0.0001559994800017333, 'epoch': 5.11}
{'loss': 1.0982, 'grad_norm': 476.9042663574219, 'learning_rate': 0.0001558694804350652, 'epoch': 5.12}
{'loss': 1.1008, 'grad_norm': 1039.68017578125, 'learning_rate': 0.00015573948086839708, 'epoch': 5.12}
{'loss': 1.0971, 'grad_norm': 10655.5634765625, 'learning_rate': 0.00015560948130172896, 'epoch': 5.12}
{'loss': 1.0999, 'grad_norm': 3234.693603515625, 'learning_rate': 0.0001554794817350609, 'epoch': 5.13}
{'loss': 1.1026, 'grad_norm': 689.4810180664062, 'learning_rate': 0.00015534948216839277, 'epoch': 5.13}
{'loss': 1.1079, 'grad_norm': 45142.0078125, 'learning_rate': 0.00015521948260172465, 'epoch': 5.14}
{'loss': 1.1032, 'grad_norm': 1965.611083984375, 'learning_rate': 0.00015508948303505655, 'epoch': 5.14}
{'loss': 1.1013, 'grad_norm': 17.00179100036621, 'learning_rate': 0.00015495948346838843, 'epoch': 5.14}
{'loss': 1.0984, 'grad_norm': 1893.703369140625, 'learning_rate': 0.0001548294839017203, 'epoch': 5.15}
{'loss': 1.1038, 'grad_norm': 15396.91015625, 'learning_rate': 0.00015469948433505218, 'epoch': 5.15}
{'loss': 1.1089, 'grad_norm': 944.307373046875, 'learning_rate': 0.0001545694847683841, 'epoch': 5.16}
{'loss': 1.1147, 'grad_norm': 458.4618225097656, 'learning_rate': 0.000154439485201716, 'epoch': 5.16}
{'loss': 1.1226, 'grad_norm': 1066.3394775390625, 'learning_rate': 0.00015430948563504787, 'epoch': 5.16}
{'loss': 1.0985, 'grad_norm': 538239.25, 'learning_rate': 0.00015417948606837978, 'epoch': 5.17}
{'loss': 1.1039, 'grad_norm': 279.41314697265625, 'learning_rate': 0.00015404948650171165, 'epoch': 5.17}
{'loss': 1.0997, 'grad_norm': 701.0781860351562, 'learning_rate': 0.00015391948693504353, 'epoch': 5.18}
{'loss': 1.1033, 'grad_norm': 437.0156555175781, 'learning_rate': 0.0001537894873683754, 'epoch': 5.18}
{'loss': 1.101, 'grad_norm': 253.109619140625, 'learning_rate': 0.0001536594878017073, 'epoch': 5.19}
{'loss': 1.0993, 'grad_norm': 2668.423583984375, 'learning_rate': 0.0001535294882350392, 'epoch': 5.19}
{'loss': 1.0998, 'grad_norm': 496018.96875, 'learning_rate': 0.0001533994886683711, 'epoch': 5.19}
{'loss': 1.0997, 'grad_norm': 52739.890625, 'learning_rate': 0.000153269489101703, 'epoch': 5.2}
{'loss': 1.1078, 'grad_norm': 23763.8046875, 'learning_rate': 0.00015313948953503488, 'epoch': 5.2}
{'loss': 1.116, 'grad_norm': 390.53436279296875, 'learning_rate': 0.00015300948996836675, 'epoch': 5.21}
{'loss': 1.1203, 'grad_norm': 156.51351928710938, 'learning_rate': 0.00015287949040169866, 'epoch': 5.21}
{'loss': 1.112, 'grad_norm': 147.42800903320312, 'learning_rate': 0.00015274949083503054, 'epoch': 5.21}
{'loss': 1.1014, 'grad_norm': 431.33697509765625, 'learning_rate': 0.0001526194912683624, 'epoch': 5.22}
{'loss': 1.1004, 'grad_norm': 1330.310791015625, 'learning_rate': 0.0001524894917016943, 'epoch': 5.22}
{'loss': 1.0987, 'grad_norm': 24348.716796875, 'learning_rate': 0.00015235949213502622, 'epoch': 5.23}
{'loss': 1.102, 'grad_norm': 2733.457763671875, 'learning_rate': 0.0001522294925683581, 'epoch': 5.23}
{'loss': 1.1091, 'grad_norm': 787.471923828125, 'learning_rate': 0.00015209949300168998, 'epoch': 5.23}
{'loss': 1.1034, 'grad_norm': 23736.3984375, 'learning_rate': 0.00015196949343502188, 'epoch': 5.24}
{'loss': 1.0993, 'grad_norm': 480147.8125, 'learning_rate': 0.00015183949386835376, 'epoch': 5.24}
{'loss': 1.1017, 'grad_norm': 632.6743774414062, 'learning_rate': 0.00015170949430168564, 'epoch': 5.25}
{'loss': 1.1002, 'grad_norm': 247.2656707763672, 'learning_rate': 0.00015157949473501751, 'epoch': 5.25}
{'loss': 1.0996, 'grad_norm': 305.18634033203125, 'learning_rate': 0.00015144949516834942, 'epoch': 5.25}
{'loss': 1.0994, 'grad_norm': 790461.8125, 'learning_rate': 0.00015131949560168132, 'epoch': 5.26}
{'loss': 1.0985, 'grad_norm': 1148.65576171875, 'learning_rate': 0.0001511894960350132, 'epoch': 5.26}
{'loss': 1.1006, 'grad_norm': 213.8092803955078, 'learning_rate': 0.0001510594964683451, 'epoch': 5.27}
{'loss': 1.1079, 'grad_norm': 391.8812561035156, 'learning_rate': 0.00015092949690167698, 'epoch': 5.27}
{'loss': 1.1176, 'grad_norm': 11516.119140625, 'learning_rate': 0.00015079949733500886, 'epoch': 5.27}
{'loss': 1.1114, 'grad_norm': 1677.1571044921875, 'learning_rate': 0.00015066949776834076, 'epoch': 5.28}
{'loss': 1.1075, 'grad_norm': 6013.20751953125, 'learning_rate': 0.00015053949820167264, 'epoch': 5.28}
{'loss': 1.1006, 'grad_norm': 118.27003479003906, 'learning_rate': 0.00015040949863500452, 'epoch': 5.29}
{'loss': 1.1009, 'grad_norm': 740.5927124023438, 'learning_rate': 0.00015027949906833645, 'epoch': 5.29}
{'loss': 1.1006, 'grad_norm': 6527686.5, 'learning_rate': 0.00015014949950166833, 'epoch': 5.3}
{'loss': 1.101, 'grad_norm': 298.34112548828125, 'learning_rate': 0.0001500194999350002, 'epoch': 5.3}
{'loss': 1.1004, 'grad_norm': 4262.9775390625, 'learning_rate': 0.00014988950036833208, 'epoch': 5.3}
{'loss': 1.1005, 'grad_norm': 9805.4560546875, 'learning_rate': 0.000149759500801664, 'epoch': 5.31}
{'loss': 1.099, 'grad_norm': 60.846038818359375, 'learning_rate': 0.00014962950123499586, 'epoch': 5.31}
{'loss': 1.0977, 'grad_norm': 47770.578125, 'learning_rate': 0.00014949950166832777, 'epoch': 5.32}
{'loss': 1.0986, 'grad_norm': 82.59085845947266, 'learning_rate': 0.00014936950210165965, 'epoch': 5.32}
{'loss': 1.1008, 'grad_norm': 10624.154296875, 'learning_rate': 0.00014923950253499152, 'epoch': 5.32}
{'loss': 1.1013, 'grad_norm': 56.13926696777344, 'learning_rate': 0.00014910950296832343, 'epoch': 5.33}
{'loss': 1.0988, 'grad_norm': 12735.7734375, 'learning_rate': 0.00014897950340165533, 'epoch': 5.33}
{'loss': 1.1004, 'grad_norm': 18771174.0, 'learning_rate': 0.0001488495038349872, 'epoch': 5.34}
{'loss': 1.0988, 'grad_norm': 32729.87890625, 'learning_rate': 0.0001487195042683191, 'epoch': 5.34}
{'loss': 1.0986, 'grad_norm': 674.317138671875, 'learning_rate': 0.00014858950470165097, 'epoch': 5.34}
{'loss': 1.0991, 'grad_norm': 22167.955078125, 'learning_rate': 0.00014845950513498287, 'epoch': 5.35}
{'loss': 1.1007, 'grad_norm': 336.63409423828125, 'learning_rate': 0.00014832950556831475, 'epoch': 5.35}
{'loss': 1.101, 'grad_norm': 46.09502410888672, 'learning_rate': 0.00014819950600164665, 'epoch': 5.36}
{'loss': 1.1001, 'grad_norm': 410027.40625, 'learning_rate': 0.00014806950643497853, 'epoch': 5.36}
{'loss': 1.1019, 'grad_norm': 89.2096939086914, 'learning_rate': 0.00014793950686831043, 'epoch': 5.36}
{'loss': 1.1031, 'grad_norm': 416.99627685546875, 'learning_rate': 0.0001478095073016423, 'epoch': 5.37}
{'loss': 1.1006, 'grad_norm': 149195.5, 'learning_rate': 0.0001476795077349742, 'epoch': 5.37}
{'loss': 1.0981, 'grad_norm': 8028.97021484375, 'learning_rate': 0.0001475495081683061, 'epoch': 5.38}
{'loss': 1.1007, 'grad_norm': 574.3360595703125, 'learning_rate': 0.000147419508601638, 'epoch': 5.38}
{'loss': 1.1012, 'grad_norm': 255.44570922851562, 'learning_rate': 0.00014728950903496987, 'epoch': 5.38}
{'loss': 1.1013, 'grad_norm': 6199.2939453125, 'learning_rate': 0.00014715950946830175, 'epoch': 5.39}
{'loss': 1.0997, 'grad_norm': 46968.796875, 'learning_rate': 0.00014702950990163363, 'epoch': 5.39}
{'loss': 1.0977, 'grad_norm': 2861.110595703125, 'learning_rate': 0.00014689951033496553, 'epoch': 5.4}
{'loss': 1.0992, 'grad_norm': 86.6321029663086, 'learning_rate': 0.00014676951076829744, 'epoch': 5.4}
{'loss': 1.098, 'grad_norm': 882055.625, 'learning_rate': 0.00014663951120162932, 'epoch': 5.41}
{'loss': 1.1001, 'grad_norm': 5392.15380859375, 'learning_rate': 0.0001465095116349612, 'epoch': 5.41}
{'loss': 1.0992, 'grad_norm': 1870.6368408203125, 'learning_rate': 0.0001463795120682931, 'epoch': 5.41}
{'loss': 1.0983, 'grad_norm': 139404.8125, 'learning_rate': 0.00014624951250162498, 'epoch': 5.42}
{'loss': 1.0992, 'grad_norm': 293862.625, 'learning_rate': 0.00014611951293495688, 'epoch': 5.42}
{'loss': 1.0986, 'grad_norm': 39065.99609375, 'learning_rate': 0.00014598951336828876, 'epoch': 5.43}
{'loss': 1.0991, 'grad_norm': 6611.24072265625, 'learning_rate': 0.00014585951380162066, 'epoch': 5.43}
{'loss': 1.0995, 'grad_norm': 1248.72314453125, 'learning_rate': 0.00014572951423495254, 'epoch': 5.43}
{'loss': 1.103, 'grad_norm': 3178.64111328125, 'learning_rate': 0.00014559951466828442, 'epoch': 5.44}
{'loss': 1.104, 'grad_norm': 358.3228759765625, 'learning_rate': 0.0001454695151016163, 'epoch': 5.44}
{'loss': 1.1023, 'grad_norm': 2459.30419921875, 'learning_rate': 0.0001453395155349482, 'epoch': 5.45}
{'loss': 1.0993, 'grad_norm': 562.4050903320312, 'learning_rate': 0.0001452095159682801, 'epoch': 5.45}
{'loss': 1.0996, 'grad_norm': 1664.4130859375, 'learning_rate': 0.00014507951640161198, 'epoch': 5.45}
{'loss': 1.1007, 'grad_norm': 8414.8076171875, 'learning_rate': 0.00014494951683494386, 'epoch': 5.46}
{'loss': 1.0998, 'grad_norm': 1765.898681640625, 'learning_rate': 0.00014481951726827576, 'epoch': 5.46}
{'loss': 1.0993, 'grad_norm': 28.670608520507812, 'learning_rate': 0.00014468951770160764, 'epoch': 5.47}
{'loss': 1.1012, 'grad_norm': 269653.4375, 'learning_rate': 0.00014455951813493954, 'epoch': 5.47}
{'loss': 1.1063, 'grad_norm': 128939.4296875, 'learning_rate': 0.00014442951856827142, 'epoch': 5.47}
{'loss': 1.1057, 'grad_norm': 45238.9609375, 'learning_rate': 0.00014429951900160333, 'epoch': 5.48}
{'loss': 1.1037, 'grad_norm': 131357.28125, 'learning_rate': 0.0001441695194349352, 'epoch': 5.48}
{'loss': 1.1054, 'grad_norm': 14789.208984375, 'learning_rate': 0.00014403951986826708, 'epoch': 5.49}
{'loss': 1.0995, 'grad_norm': 335.2704772949219, 'learning_rate': 0.00014390952030159899, 'epoch': 5.49}
{'loss': 1.1023, 'grad_norm': 151790.59375, 'learning_rate': 0.00014377952073493086, 'epoch': 5.49}
{'loss': 1.1039, 'grad_norm': 17209.681640625, 'learning_rate': 0.00014364952116826277, 'epoch': 5.5}
{'loss': 1.1016, 'grad_norm': 11160.798828125, 'learning_rate': 0.00014351952160159465, 'epoch': 5.5}
{'loss': 1.1008, 'grad_norm': 1306.457275390625, 'learning_rate': 0.00014338952203492652, 'epoch': 5.51}
{'loss': 1.1002, 'grad_norm': 22122.09375, 'learning_rate': 0.00014325952246825843, 'epoch': 5.51}
{'loss': 1.1046, 'grad_norm': 1991392.875, 'learning_rate': 0.0001431295229015903, 'epoch': 5.52}
{'loss': 1.1036, 'grad_norm': 1286.1142578125, 'learning_rate': 0.0001429995233349222, 'epoch': 5.52}
{'loss': 1.1092, 'grad_norm': 933.3286743164062, 'learning_rate': 0.0001428695237682541, 'epoch': 5.52}
{'loss': 1.1099, 'grad_norm': 3202.029052734375, 'learning_rate': 0.000142739524201586, 'epoch': 5.53}
{'loss': 1.1044, 'grad_norm': 6452.94482421875, 'learning_rate': 0.00014260952463491787, 'epoch': 5.53}
{'loss': 1.0991, 'grad_norm': 157.2643280029297, 'learning_rate': 0.00014247952506824975, 'epoch': 5.54}
{'loss': 1.1013, 'grad_norm': 2247.3515625, 'learning_rate': 0.00014234952550158165, 'epoch': 5.54}
{'loss': 1.0993, 'grad_norm': 181.7969512939453, 'learning_rate': 0.00014221952593491356, 'epoch': 5.54}
{'loss': 1.1025, 'grad_norm': 1481.2625732421875, 'learning_rate': 0.00014208952636824543, 'epoch': 5.55}
{'loss': 1.1017, 'grad_norm': 260.6043395996094, 'learning_rate': 0.0001419595268015773, 'epoch': 5.55}
{'loss': 1.1033, 'grad_norm': 2414.69775390625, 'learning_rate': 0.00014182952723490921, 'epoch': 5.56}
{'loss': 1.0998, 'grad_norm': 398308.78125, 'learning_rate': 0.0001416995276682411, 'epoch': 5.56}
{'loss': 1.1001, 'grad_norm': 8671.2734375, 'learning_rate': 0.00014156952810157297, 'epoch': 5.56}
{'loss': 1.0993, 'grad_norm': 7647.7509765625, 'learning_rate': 0.00014143952853490487, 'epoch': 5.57}
{'loss': 1.0998, 'grad_norm': 82929.9609375, 'learning_rate': 0.00014130952896823678, 'epoch': 5.57}
{'loss': 1.0998, 'grad_norm': 6914.72607421875, 'learning_rate': 0.00014117952940156866, 'epoch': 5.58}
{'loss': 1.0998, 'grad_norm': 7048.427734375, 'learning_rate': 0.00014104952983490053, 'epoch': 5.58}
{'loss': 1.1022, 'grad_norm': 199.28590393066406, 'learning_rate': 0.0001409195302682324, 'epoch': 5.58}
{'loss': 1.107, 'grad_norm': 197.3983917236328, 'learning_rate': 0.00014078953070156432, 'epoch': 5.59}
{'loss': 1.0983, 'grad_norm': 363.9690856933594, 'learning_rate': 0.00014065953113489622, 'epoch': 5.59}
{'loss': 1.0971, 'grad_norm': 342.8221130371094, 'learning_rate': 0.0001405295315682281, 'epoch': 5.6}
{'loss': 1.1011, 'grad_norm': 1447.3651123046875, 'learning_rate': 0.00014039953200155997, 'epoch': 5.6}
{'loss': 1.0995, 'grad_norm': 2731250.25, 'learning_rate': 0.00014026953243489188, 'epoch': 5.6}
{'loss': 1.1037, 'grad_norm': 828.791015625, 'learning_rate': 0.00014013953286822376, 'epoch': 5.61}
{'loss': 1.0997, 'grad_norm': 2043.77978515625, 'learning_rate': 0.00014000953330155566, 'epoch': 5.61}
{'loss': 1.1009, 'grad_norm': 2254.441162109375, 'learning_rate': 0.00013987953373488754, 'epoch': 5.62}
{'loss': 1.1011, 'grad_norm': 30812.712890625, 'learning_rate': 0.00013974953416821944, 'epoch': 5.62}
{'loss': 1.0997, 'grad_norm': 1249.255126953125, 'learning_rate': 0.00013961953460155132, 'epoch': 5.63}
{'loss': 1.0987, 'grad_norm': 42256.4921875, 'learning_rate': 0.0001394895350348832, 'epoch': 5.63}
{'loss': 1.1011, 'grad_norm': 385.02978515625, 'learning_rate': 0.0001393595354682151, 'epoch': 5.63}
{'loss': 1.099, 'grad_norm': 1171.575439453125, 'learning_rate': 0.00013922953590154698, 'epoch': 5.64}
{'loss': 1.0995, 'grad_norm': 1965.9215087890625, 'learning_rate': 0.00013909953633487888, 'epoch': 5.64}
{'loss': 1.0999, 'grad_norm': 1105.6441650390625, 'learning_rate': 0.00013896953676821076, 'epoch': 5.65}
{'loss': 1.1014, 'grad_norm': 14724.4287109375, 'learning_rate': 0.00013883953720154264, 'epoch': 5.65}
{'loss': 1.0997, 'grad_norm': 3124.51904296875, 'learning_rate': 0.00013870953763487454, 'epoch': 5.65}
{'loss': 1.103, 'grad_norm': 364.3898010253906, 'learning_rate': 0.00013857953806820642, 'epoch': 5.66}
{'loss': 1.1111, 'grad_norm': 94160.6484375, 'learning_rate': 0.00013844953850153833, 'epoch': 5.66}
{'loss': 1.1049, 'grad_norm': 27485.2109375, 'learning_rate': 0.0001383195389348702, 'epoch': 5.67}
{'loss': 1.1022, 'grad_norm': 2894.80078125, 'learning_rate': 0.0001381895393682021, 'epoch': 5.67}
{'loss': 1.1011, 'grad_norm': 14808.1455078125, 'learning_rate': 0.00013805953980153398, 'epoch': 5.67}
{'loss': 1.103, 'grad_norm': 1123.6956787109375, 'learning_rate': 0.00013792954023486586, 'epoch': 5.68}
{'loss': 1.1077, 'grad_norm': 187.83087158203125, 'learning_rate': 0.00013779954066819777, 'epoch': 5.68}
{'loss': 1.1069, 'grad_norm': 49235.48046875, 'learning_rate': 0.00013766954110152964, 'epoch': 5.69}
{'loss': 1.1049, 'grad_norm': 2993.4169921875, 'learning_rate': 0.00013753954153486155, 'epoch': 5.69}
{'loss': 1.0997, 'grad_norm': 197.52108764648438, 'learning_rate': 0.00013740954196819343, 'epoch': 5.69}
{'loss': 1.101, 'grad_norm': 73477.53125, 'learning_rate': 0.0001372795424015253, 'epoch': 5.7}
{'loss': 1.1017, 'grad_norm': 155.74685668945312, 'learning_rate': 0.0001371495428348572, 'epoch': 5.7}
{'loss': 1.0986, 'grad_norm': 6908.322265625, 'learning_rate': 0.00013701954326818909, 'epoch': 5.71}
{'loss': 1.1003, 'grad_norm': 921.0231323242188, 'learning_rate': 0.000136889543701521, 'epoch': 5.71}
{'loss': 1.1006, 'grad_norm': 1794.1593017578125, 'learning_rate': 0.00013675954413485287, 'epoch': 5.71}
{'loss': 1.1002, 'grad_norm': 2398.10693359375, 'learning_rate': 0.00013662954456818477, 'epoch': 5.72}
{'loss': 1.1011, 'grad_norm': 2673.72900390625, 'learning_rate': 0.00013649954500151665, 'epoch': 5.72}
{'loss': 1.1024, 'grad_norm': 194768.0, 'learning_rate': 0.00013636954543484853, 'epoch': 5.73}
{'loss': 1.0982, 'grad_norm': 1080.40478515625, 'learning_rate': 0.00013623954586818043, 'epoch': 5.73}
{'loss': 1.1017, 'grad_norm': 12496.2216796875, 'learning_rate': 0.00013610954630151234, 'epoch': 5.74}
{'loss': 1.0974, 'grad_norm': 30.637758255004883, 'learning_rate': 0.0001359795467348442, 'epoch': 5.74}
{'loss': 1.1031, 'grad_norm': 145071.96875, 'learning_rate': 0.0001358495471681761, 'epoch': 5.74}
{'loss': 1.1022, 'grad_norm': 12953.099609375, 'learning_rate': 0.00013571954760150797, 'epoch': 5.75}
{'loss': 1.1016, 'grad_norm': 209.7593536376953, 'learning_rate': 0.00013558954803483987, 'epoch': 5.75}
{'loss': 1.1002, 'grad_norm': 367.7087707519531, 'learning_rate': 0.00013545954846817175, 'epoch': 5.76}
{'loss': 1.1015, 'grad_norm': 34320.98828125, 'learning_rate': 0.00013532954890150365, 'epoch': 5.76}
{'loss': 1.0996, 'grad_norm': 47790.85546875, 'learning_rate': 0.00013519954933483553, 'epoch': 5.76}
{'loss': 1.1043, 'grad_norm': 3564.159423828125, 'learning_rate': 0.00013506954976816744, 'epoch': 5.77}
{'loss': 1.0977, 'grad_norm': 164.38778686523438, 'learning_rate': 0.00013493955020149931, 'epoch': 5.77}
{'loss': 1.1026, 'grad_norm': 146.79342651367188, 'learning_rate': 0.0001348095506348312, 'epoch': 5.78}
{'loss': 1.1012, 'grad_norm': 1990.23291015625, 'learning_rate': 0.0001346795510681631, 'epoch': 5.78}
{'loss': 1.0997, 'grad_norm': 2989.7919921875, 'learning_rate': 0.000134549551501495, 'epoch': 5.78}
{'loss': 1.0997, 'grad_norm': 255001.9375, 'learning_rate': 0.00013441955193482688, 'epoch': 5.79}
{'loss': 1.0975, 'grad_norm': 2011.0723876953125, 'learning_rate': 0.00013428955236815876, 'epoch': 5.79}
{'loss': 1.0986, 'grad_norm': 3939.915283203125, 'learning_rate': 0.00013415955280149063, 'epoch': 5.8}
{'loss': 1.1024, 'grad_norm': 90.36302185058594, 'learning_rate': 0.00013402955323482254, 'epoch': 5.8}
{'loss': 1.101, 'grad_norm': 5274.28759765625, 'learning_rate': 0.00013389955366815444, 'epoch': 5.8}
{'loss': 1.0995, 'grad_norm': 2875.638916015625, 'learning_rate': 0.00013376955410148632, 'epoch': 5.81}
{'loss': 1.1, 'grad_norm': 6763.49609375, 'learning_rate': 0.0001336395545348182, 'epoch': 5.81}
{'loss': 1.1012, 'grad_norm': 5508.8525390625, 'learning_rate': 0.0001335095549681501, 'epoch': 5.82}
{'loss': 1.0999, 'grad_norm': 10619.2255859375, 'learning_rate': 0.00013337955540148198, 'epoch': 5.82}
{'loss': 1.0994, 'grad_norm': 796.40966796875, 'learning_rate': 0.00013324955583481388, 'epoch': 5.82}
{'loss': 1.1046, 'grad_norm': 145.4147186279297, 'learning_rate': 0.00013311955626814576, 'epoch': 5.83}
{'loss': 1.104, 'grad_norm': 1977.31591796875, 'learning_rate': 0.00013298955670147766, 'epoch': 5.83}
{'loss': 1.1005, 'grad_norm': 1850.423583984375, 'learning_rate': 0.00013285955713480954, 'epoch': 5.84}
{'loss': 1.1024, 'grad_norm': 1049.75048828125, 'learning_rate': 0.00013272955756814142, 'epoch': 5.84}
{'loss': 1.099, 'grad_norm': 11562.7158203125, 'learning_rate': 0.0001325995580014733, 'epoch': 5.85}
{'loss': 1.1004, 'grad_norm': 7211.6240234375, 'learning_rate': 0.0001324695584348052, 'epoch': 5.85}
{'loss': 1.0986, 'grad_norm': 33.878746032714844, 'learning_rate': 0.0001323395588681371, 'epoch': 5.85}
{'loss': 1.099, 'grad_norm': 367992.15625, 'learning_rate': 0.00013220955930146898, 'epoch': 5.86}
{'loss': 1.1031, 'grad_norm': 151812.859375, 'learning_rate': 0.00013207955973480086, 'epoch': 5.86}
{'loss': 1.1033, 'grad_norm': 292.7672119140625, 'learning_rate': 0.00013194956016813277, 'epoch': 5.87}
{'loss': 1.0996, 'grad_norm': 1463.551025390625, 'learning_rate': 0.00013181956060146464, 'epoch': 5.87}
{'loss': 1.1033, 'grad_norm': 411.57525634765625, 'learning_rate': 0.00013168956103479655, 'epoch': 5.87}
{'loss': 1.0997, 'grad_norm': 389.7000732421875, 'learning_rate': 0.00013155956146812842, 'epoch': 5.88}
{'loss': 1.1001, 'grad_norm': 12006.0888671875, 'learning_rate': 0.00013142956190146033, 'epoch': 5.88}
{'loss': 1.1002, 'grad_norm': 496.545654296875, 'learning_rate': 0.0001312995623347922, 'epoch': 5.89}
{'loss': 1.0997, 'grad_norm': 1940.01708984375, 'learning_rate': 0.00013116956276812408, 'epoch': 5.89}
{'loss': 1.0996, 'grad_norm': 226.0027313232422, 'learning_rate': 0.000131039563201456, 'epoch': 5.89}
{'loss': 1.1011, 'grad_norm': 8523.19140625, 'learning_rate': 0.00013090956363478787, 'epoch': 5.9}
{'loss': 1.0994, 'grad_norm': 503.278564453125, 'learning_rate': 0.00013077956406811977, 'epoch': 5.9}
{'loss': 1.1007, 'grad_norm': 1326.0948486328125, 'learning_rate': 0.00013064956450145165, 'epoch': 5.91}
{'loss': 1.1007, 'grad_norm': 337.8992919921875, 'learning_rate': 0.00013051956493478353, 'epoch': 5.91}
{'loss': 1.101, 'grad_norm': 333445.46875, 'learning_rate': 0.00013038956536811543, 'epoch': 5.91}
{'loss': 1.0989, 'grad_norm': 208970.109375, 'learning_rate': 0.0001302595658014473, 'epoch': 5.92}
{'loss': 1.0988, 'grad_norm': 2879.26318359375, 'learning_rate': 0.0001301295662347792, 'epoch': 5.92}
{'loss': 1.1008, 'grad_norm': 69314.0859375, 'learning_rate': 0.0001299995666681111, 'epoch': 5.93}
{'loss': 1.0989, 'grad_norm': 235600.046875, 'learning_rate': 0.000129869567101443, 'epoch': 5.93}
{'loss': 1.1017, 'grad_norm': 11887939.0, 'learning_rate': 0.00012973956753477487, 'epoch': 5.93}
{'loss': 1.101, 'grad_norm': 916.8009033203125, 'learning_rate': 0.00012960956796810675, 'epoch': 5.94}
{'loss': 1.1041, 'grad_norm': 3490.0361328125, 'learning_rate': 0.00012947956840143865, 'epoch': 5.94}
{'loss': 1.0983, 'grad_norm': 9497701.0, 'learning_rate': 0.00012934956883477056, 'epoch': 5.95}
{'loss': 1.1033, 'grad_norm': 2061.41455078125, 'learning_rate': 0.00012921956926810244, 'epoch': 5.95}
{'loss': 1.0989, 'grad_norm': 192.5865936279297, 'learning_rate': 0.0001290895697014343, 'epoch': 5.96}
{'loss': 1.1004, 'grad_norm': 11282.12109375, 'learning_rate': 0.0001289595701347662, 'epoch': 5.96}
{'loss': 1.1008, 'grad_norm': 1835.7635498046875, 'learning_rate': 0.0001288295705680981, 'epoch': 5.96}
{'loss': 1.1024, 'grad_norm': 283.0398864746094, 'learning_rate': 0.00012869957100142997, 'epoch': 5.97}
{'loss': 1.1013, 'grad_norm': 30509.4609375, 'learning_rate': 0.00012856957143476188, 'epoch': 5.97}
{'loss': 1.1002, 'grad_norm': 4367.52783203125, 'learning_rate': 0.00012843957186809375, 'epoch': 5.98}
{'loss': 1.0994, 'grad_norm': 30219.95703125, 'learning_rate': 0.00012830957230142566, 'epoch': 5.98}
{'loss': 1.1013, 'grad_norm': 12194.29296875, 'learning_rate': 0.00012817957273475754, 'epoch': 5.98}
{'loss': 1.098, 'grad_norm': 38.764041900634766, 'learning_rate': 0.0001280495731680894, 'epoch': 5.99}
{'loss': 1.1025, 'grad_norm': 6207.73046875, 'learning_rate': 0.00012791957360142132, 'epoch': 5.99}
{'loss': 1.1006, 'grad_norm': 115276.203125, 'learning_rate': 0.00012778957403475322, 'epoch': 6.0}
{'loss': 1.0998, 'grad_norm': 1466009.625, 'learning_rate': 0.0001276595744680851, 'epoch': 6.0}
{'eval_loss': 1.0963135957717896, 'eval_accuracy': 0.3273560876209883, 'eval_runtime': 5.6187, 'eval_samples_per_second': 1746.861, 'eval_steps_per_second': 11.035, 'epoch': 6.0}
{'loss': 1.1002, 'grad_norm': 26593.8515625, 'learning_rate': 0.00012752957490141698, 'epoch': 6.0}
{'loss': 1.1023, 'grad_norm': 1735.6937255859375, 'learning_rate': 0.00012739957533474885, 'epoch': 6.01}
{'loss': 1.0994, 'grad_norm': 9366.7548828125, 'learning_rate': 0.00012726957576808076, 'epoch': 6.01}
{'loss': 1.0982, 'grad_norm': 2238.369384765625, 'learning_rate': 0.00012713957620141266, 'epoch': 6.02}
{'loss': 1.1005, 'grad_norm': 1587.7855224609375, 'learning_rate': 0.00012700957663474454, 'epoch': 6.02}
{'loss': 1.0989, 'grad_norm': 1936.70849609375, 'learning_rate': 0.00012687957706807642, 'epoch': 6.02}
{'loss': 1.0994, 'grad_norm': 981.252197265625, 'learning_rate': 0.00012674957750140832, 'epoch': 6.03}
{'loss': 1.0977, 'grad_norm': 7930.0849609375, 'learning_rate': 0.0001266195779347402, 'epoch': 6.03}
{'loss': 1.1025, 'grad_norm': 1544.2344970703125, 'learning_rate': 0.0001264895783680721, 'epoch': 6.04}
{'loss': 1.0975, 'grad_norm': 1652.098876953125, 'learning_rate': 0.00012635957880140398, 'epoch': 6.04}
{'loss': 1.103, 'grad_norm': 2803.01025390625, 'learning_rate': 0.0001262295792347359, 'epoch': 6.04}
{'loss': 1.0998, 'grad_norm': 119.60762786865234, 'learning_rate': 0.00012609957966806776, 'epoch': 6.05}
{'loss': 1.0987, 'grad_norm': 1968.593505859375, 'learning_rate': 0.00012596958010139964, 'epoch': 6.05}
{'loss': 1.0991, 'grad_norm': 609.0888671875, 'learning_rate': 0.00012583958053473152, 'epoch': 6.06}
{'loss': 1.1005, 'grad_norm': 7527.8037109375, 'learning_rate': 0.00012570958096806342, 'epoch': 6.06}
{'loss': 1.1014, 'grad_norm': 10470.3095703125, 'learning_rate': 0.00012557958140139533, 'epoch': 6.07}
{'loss': 1.1001, 'grad_norm': 1068.520263671875, 'learning_rate': 0.0001254495818347272, 'epoch': 6.07}
{'loss': 1.0987, 'grad_norm': 137.5792236328125, 'learning_rate': 0.00012531958226805908, 'epoch': 6.07}
{'loss': 1.0991, 'grad_norm': 64519.4609375, 'learning_rate': 0.000125189582701391, 'epoch': 6.08}
{'loss': 1.0998, 'grad_norm': 31873.330078125, 'learning_rate': 0.00012505958313472287, 'epoch': 6.08}
{'loss': 1.0992, 'grad_norm': 3245693.75, 'learning_rate': 0.00012492958356805477, 'epoch': 6.09}
{'loss': 1.0995, 'grad_norm': 204929.328125, 'learning_rate': 0.00012479958400138665, 'epoch': 6.09}
{'loss': 1.0993, 'grad_norm': 609.5272827148438, 'learning_rate': 0.00012466958443471855, 'epoch': 6.09}
{'loss': 1.098, 'grad_norm': 944.1818237304688, 'learning_rate': 0.00012453958486805043, 'epoch': 6.1}
{'loss': 1.0992, 'grad_norm': 3566.61181640625, 'learning_rate': 0.0001244095853013823, 'epoch': 6.1}
{'loss': 1.0977, 'grad_norm': 531.3637084960938, 'learning_rate': 0.0001242795857347142, 'epoch': 6.11}
{'loss': 1.099, 'grad_norm': 125.81608581542969, 'learning_rate': 0.0001241495861680461, 'epoch': 6.11}
{'loss': 1.0988, 'grad_norm': 1747.9345703125, 'learning_rate': 0.000124019586601378, 'epoch': 6.11}
{'loss': 1.1003, 'grad_norm': 19937.263671875, 'learning_rate': 0.00012388958703470987, 'epoch': 6.12}
{'loss': 1.0981, 'grad_norm': 3365.5634765625, 'learning_rate': 0.00012375958746804175, 'epoch': 6.12}
{'loss': 1.0995, 'grad_norm': 851.6581420898438, 'learning_rate': 0.00012362958790137365, 'epoch': 6.13}
{'loss': 1.098, 'grad_norm': 7469.78369140625, 'learning_rate': 0.00012349958833470553, 'epoch': 6.13}
{'loss': 1.0991, 'grad_norm': 62638.79296875, 'learning_rate': 0.00012336958876803743, 'epoch': 6.13}
{'loss': 1.1015, 'grad_norm': 18832.67578125, 'learning_rate': 0.0001232395892013693, 'epoch': 6.14}
{'loss': 1.0993, 'grad_norm': 3591.98876953125, 'learning_rate': 0.00012310958963470122, 'epoch': 6.14}
{'loss': 1.0999, 'grad_norm': 8731.6806640625, 'learning_rate': 0.0001229795900680331, 'epoch': 6.15}
{'loss': 1.0993, 'grad_norm': 3250790.75, 'learning_rate': 0.00012284959050136497, 'epoch': 6.15}
{'loss': 1.0995, 'grad_norm': 4696.662109375, 'learning_rate': 0.00012271959093469688, 'epoch': 6.15}
{'loss': 1.1, 'grad_norm': 438032.84375, 'learning_rate': 0.00012258959136802878, 'epoch': 6.16}
{'loss': 1.0999, 'grad_norm': 15294.4208984375, 'learning_rate': 0.00012245959180136066, 'epoch': 6.16}
{'loss': 1.0994, 'grad_norm': 49992788.0, 'learning_rate': 0.00012232959223469253, 'epoch': 6.17}
{'loss': 1.0988, 'grad_norm': 5626.40673828125, 'learning_rate': 0.00012219959266802444, 'epoch': 6.17}
{'loss': 1.0989, 'grad_norm': 247.7176971435547, 'learning_rate': 0.00012206959310135632, 'epoch': 6.18}
{'loss': 1.0988, 'grad_norm': 535.4238891601562, 'learning_rate': 0.00012193959353468821, 'epoch': 6.18}
{'loss': 1.0981, 'grad_norm': 23278.029296875, 'learning_rate': 0.00012180959396802009, 'epoch': 6.18}
{'loss': 1.0983, 'grad_norm': 25628.177734375, 'learning_rate': 0.00012167959440135199, 'epoch': 6.19}
{'loss': 1.0986, 'grad_norm': 437530.25, 'learning_rate': 0.00012154959483468388, 'epoch': 6.19}
{'loss': 1.0982, 'grad_norm': 11078.8115234375, 'learning_rate': 0.00012141959526801576, 'epoch': 6.2}
{'loss': 1.0989, 'grad_norm': 7224.28369140625, 'learning_rate': 0.00012128959570134765, 'epoch': 6.2}
{'loss': 1.0976, 'grad_norm': 73915.328125, 'learning_rate': 0.00012115959613467955, 'epoch': 6.2}
{'loss': 1.099, 'grad_norm': 830934.5, 'learning_rate': 0.00012102959656801143, 'epoch': 6.21}
{'loss': 1.0991, 'grad_norm': 146162.203125, 'learning_rate': 0.00012089959700134332, 'epoch': 6.21}
{'loss': 1.0983, 'grad_norm': 2126.8037109375, 'learning_rate': 0.0001207695974346752, 'epoch': 6.22}
{'loss': 1.0993, 'grad_norm': 8228136.0, 'learning_rate': 0.0001206395978680071, 'epoch': 6.22}
{'loss': 1.0994, 'grad_norm': 331031.375, 'learning_rate': 0.000120509598301339, 'epoch': 6.22}
{'loss': 1.0987, 'grad_norm': 158130.265625, 'learning_rate': 0.00012037959873467087, 'epoch': 6.23}
{'loss': 1.0982, 'grad_norm': 13519.1220703125, 'learning_rate': 0.00012024959916800276, 'epoch': 6.23}
{'loss': 1.0989, 'grad_norm': 173381.8125, 'learning_rate': 0.00012011959960133465, 'epoch': 6.24}
{'loss': 1.0996, 'grad_norm': 16115.81640625, 'learning_rate': 0.00011998960003466655, 'epoch': 6.24}
{'loss': 1.0989, 'grad_norm': 16584.69921875, 'learning_rate': 0.00011985960046799842, 'epoch': 6.24}
{'loss': 1.0996, 'grad_norm': 969276.5625, 'learning_rate': 0.00011972960090133031, 'epoch': 6.25}
{'loss': 1.1, 'grad_norm': 24989.08203125, 'learning_rate': 0.00011959960133466222, 'epoch': 6.25}
{'loss': 1.099, 'grad_norm': 2541.76806640625, 'learning_rate': 0.0001194696017679941, 'epoch': 6.26}
{'loss': 1.0986, 'grad_norm': 119177.6015625, 'learning_rate': 0.00011933960220132599, 'epoch': 6.26}
{'loss': 1.0987, 'grad_norm': 127824.3203125, 'learning_rate': 0.00011920960263465786, 'epoch': 6.26}
{'loss': 1.0985, 'grad_norm': 905.3805541992188, 'learning_rate': 0.00011907960306798977, 'epoch': 6.27}
{'loss': 1.0977, 'grad_norm': 10178.779296875, 'learning_rate': 0.00011894960350132166, 'epoch': 6.27}
{'loss': 1.1006, 'grad_norm': 1998.415283203125, 'learning_rate': 0.00011881960393465354, 'epoch': 6.28}
{'loss': 1.0985, 'grad_norm': 11121.501953125, 'learning_rate': 0.00011868960436798543, 'epoch': 6.28}
{'loss': 1.1009, 'grad_norm': 16580.3046875, 'learning_rate': 0.00011855960480131733, 'epoch': 6.29}
{'loss': 1.1, 'grad_norm': 8346.1865234375, 'learning_rate': 0.00011842960523464921, 'epoch': 6.29}
{'loss': 1.0985, 'grad_norm': 1078.78173828125, 'learning_rate': 0.0001182996056679811, 'epoch': 6.29}
{'loss': 1.0998, 'grad_norm': 7476849.0, 'learning_rate': 0.00011816960610131298, 'epoch': 6.3}
{'loss': 1.0996, 'grad_norm': 5675.47265625, 'learning_rate': 0.00011803960653464488, 'epoch': 6.3}
{'loss': 1.0992, 'grad_norm': 3255.05126953125, 'learning_rate': 0.00011790960696797676, 'epoch': 6.31}
{'loss': 1.0998, 'grad_norm': 5448.41259765625, 'learning_rate': 0.00011777960740130865, 'epoch': 6.31}
{'loss': 1.0987, 'grad_norm': 255992.125, 'learning_rate': 0.00011764960783464054, 'epoch': 6.31}
{'loss': 1.0976, 'grad_norm': 6029628.0, 'learning_rate': 0.00011751960826797243, 'epoch': 6.32}
{'loss': 1.0994, 'grad_norm': 5826.51171875, 'learning_rate': 0.00011738960870130432, 'epoch': 6.32}
{'loss': 1.0989, 'grad_norm': 2825.574951171875, 'learning_rate': 0.0001172596091346362, 'epoch': 6.33}
{'loss': 1.0987, 'grad_norm': 292794.875, 'learning_rate': 0.00011712960956796809, 'epoch': 6.33}
{'loss': 1.0994, 'grad_norm': 189934.4375, 'learning_rate': 0.0001169996100013, 'epoch': 6.33}
{'loss': 1.0983, 'grad_norm': 13996.2548828125, 'learning_rate': 0.00011686961043463187, 'epoch': 6.34}
{'loss': 1.0993, 'grad_norm': 2632.841552734375, 'learning_rate': 0.00011673961086796377, 'epoch': 6.34}
{'loss': 1.0988, 'grad_norm': 10279.474609375, 'learning_rate': 0.00011660961130129564, 'epoch': 6.35}
{'loss': 1.0989, 'grad_norm': 12890.642578125, 'learning_rate': 0.00011647961173462755, 'epoch': 6.35}
{'loss': 1.0985, 'grad_norm': 7737838.0, 'learning_rate': 0.00011634961216795944, 'epoch': 6.35}
{'loss': 1.0988, 'grad_norm': 136361.546875, 'learning_rate': 0.00011621961260129132, 'epoch': 6.36}
{'loss': 1.0983, 'grad_norm': 4020.37646484375, 'learning_rate': 0.0001160896130346232, 'epoch': 6.36}
{'loss': 1.0994, 'grad_norm': 25384.271484375, 'learning_rate': 0.0001159596134679551, 'epoch': 6.37}
{'loss': 1.0987, 'grad_norm': 171719.078125, 'learning_rate': 0.00011582961390128699, 'epoch': 6.37}
{'loss': 1.0981, 'grad_norm': 238435.84375, 'learning_rate': 0.00011569961433461888, 'epoch': 6.37}
{'loss': 1.1, 'grad_norm': 30049892.0, 'learning_rate': 0.00011556961476795076, 'epoch': 6.38}
{'loss': 1.0994, 'grad_norm': 6802.6708984375, 'learning_rate': 0.00011543961520128266, 'epoch': 6.38}
{'loss': 1.0984, 'grad_norm': 153143.921875, 'learning_rate': 0.00011530961563461454, 'epoch': 6.39}
{'loss': 1.1, 'grad_norm': 1593512.75, 'learning_rate': 0.00011517961606794643, 'epoch': 6.39}
{'loss': 1.0997, 'grad_norm': 264607.625, 'learning_rate': 0.00011504961650127831, 'epoch': 6.4}
{'loss': 1.1009, 'grad_norm': 26914.4765625, 'learning_rate': 0.00011491961693461021, 'epoch': 6.4}
{'loss': 1.0989, 'grad_norm': 5614.5078125, 'learning_rate': 0.0001147896173679421, 'epoch': 6.4}
{'loss': 1.0992, 'grad_norm': 14068.904296875, 'learning_rate': 0.00011465961780127398, 'epoch': 6.41}
{'loss': 1.0988, 'grad_norm': 4622.68017578125, 'learning_rate': 0.00011452961823460587, 'epoch': 6.41}
{'loss': 1.1003, 'grad_norm': 14488.1728515625, 'learning_rate': 0.00011439961866793778, 'epoch': 6.42}
{'loss': 1.0981, 'grad_norm': 609738.5625, 'learning_rate': 0.00011426961910126965, 'epoch': 6.42}
{'loss': 1.0989, 'grad_norm': 10435791.0, 'learning_rate': 0.00011413961953460154, 'epoch': 6.42}
{'loss': 1.1004, 'grad_norm': 390297.5, 'learning_rate': 0.00011400961996793342, 'epoch': 6.43}
{'loss': 1.0985, 'grad_norm': 61813.83984375, 'learning_rate': 0.00011387962040126533, 'epoch': 6.43}
{'loss': 1.0991, 'grad_norm': 112430.5234375, 'learning_rate': 0.00011374962083459722, 'epoch': 6.44}
{'loss': 1.098, 'grad_norm': 28033.74609375, 'learning_rate': 0.0001136196212679291, 'epoch': 6.44}
{'loss': 1.0979, 'grad_norm': 382533.5, 'learning_rate': 0.00011348962170126099, 'epoch': 6.44}
{'loss': 1.0991, 'grad_norm': 56261.66796875, 'learning_rate': 0.00011335962213459288, 'epoch': 6.45}
{'loss': 1.0984, 'grad_norm': 110546.453125, 'learning_rate': 0.00011322962256792477, 'epoch': 6.45}
{'loss': 1.0991, 'grad_norm': 2022.54833984375, 'learning_rate': 0.00011309962300125664, 'epoch': 6.46}
{'loss': 1.0986, 'grad_norm': 1898584.875, 'learning_rate': 0.00011296962343458854, 'epoch': 6.46}
{'loss': 1.0988, 'grad_norm': 31740.22265625, 'learning_rate': 0.00011283962386792044, 'epoch': 6.46}
{'loss': 1.0981, 'grad_norm': 93078.1640625, 'learning_rate': 0.00011270962430125232, 'epoch': 6.47}
{'loss': 1.0974, 'grad_norm': 68556.734375, 'learning_rate': 0.00011257962473458421, 'epoch': 6.47}
{'loss': 1.1006, 'grad_norm': 12247.1953125, 'learning_rate': 0.00011244962516791609, 'epoch': 6.48}
{'loss': 1.0987, 'grad_norm': 28881.724609375, 'learning_rate': 0.00011231962560124799, 'epoch': 6.48}
{'loss': 1.0986, 'grad_norm': 2187726.75, 'learning_rate': 0.00011218962603457988, 'epoch': 6.48}
{'loss': 1.0993, 'grad_norm': 209826.359375, 'learning_rate': 0.00011205962646791176, 'epoch': 6.49}
{'loss': 1.0996, 'grad_norm': 211630.796875, 'learning_rate': 0.00011192962690124365, 'epoch': 6.49}
{'loss': 1.099, 'grad_norm': 23245.5703125, 'learning_rate': 0.00011179962733457555, 'epoch': 6.5}
{'loss': 1.098, 'grad_norm': 143145.78125, 'learning_rate': 0.00011166962776790743, 'epoch': 6.5}
{'loss': 1.098, 'grad_norm': 239544.53125, 'learning_rate': 0.00011153962820123932, 'epoch': 6.51}
{'loss': 1.0987, 'grad_norm': 215506.15625, 'learning_rate': 0.0001114096286345712, 'epoch': 6.51}
{'loss': 1.0992, 'grad_norm': 402349.15625, 'learning_rate': 0.0001112796290679031, 'epoch': 6.51}
{'loss': 1.0993, 'grad_norm': 2338410.75, 'learning_rate': 0.00011114962950123498, 'epoch': 6.52}
{'loss': 1.0974, 'grad_norm': 1610536.75, 'learning_rate': 0.00011101962993456687, 'epoch': 6.52}
{'loss': 1.0999, 'grad_norm': 507997.125, 'learning_rate': 0.00011088963036789876, 'epoch': 6.53}
{'loss': 1.0995, 'grad_norm': 20280.078125, 'learning_rate': 0.00011075963080123065, 'epoch': 6.53}
{'loss': 1.099, 'grad_norm': 4493142.5, 'learning_rate': 0.00011062963123456255, 'epoch': 6.53}
{'loss': 1.0983, 'grad_norm': 235374.28125, 'learning_rate': 0.00011049963166789442, 'epoch': 6.54}
{'loss': 1.099, 'grad_norm': 38112.15625, 'learning_rate': 0.00011036963210122631, 'epoch': 6.54}
{'loss': 1.0995, 'grad_norm': 34711.53125, 'learning_rate': 0.00011023963253455822, 'epoch': 6.55}
{'loss': 1.0982, 'grad_norm': 24391.025390625, 'learning_rate': 0.0001101096329678901, 'epoch': 6.55}
{'loss': 1.0977, 'grad_norm': 56983.8203125, 'learning_rate': 0.00010997963340122199, 'epoch': 6.55}
{'loss': 1.0994, 'grad_norm': 10989.4248046875, 'learning_rate': 0.00010984963383455386, 'epoch': 6.56}
{'loss': 1.1002, 'grad_norm': 47435.7734375, 'learning_rate': 0.00010971963426788577, 'epoch': 6.56}
{'loss': 1.0986, 'grad_norm': 1585.8389892578125, 'learning_rate': 0.00010958963470121766, 'epoch': 6.57}
{'loss': 1.0972, 'grad_norm': 21053.85546875, 'learning_rate': 0.00010945963513454954, 'epoch': 6.57}
{'loss': 1.0993, 'grad_norm': 100505.9765625, 'learning_rate': 0.00010932963556788143, 'epoch': 6.57}
{'loss': 1.0999, 'grad_norm': 10953.259765625, 'learning_rate': 0.00010919963600121332, 'epoch': 6.58}
{'loss': 1.0986, 'grad_norm': 4569.3935546875, 'learning_rate': 0.00010906963643454521, 'epoch': 6.58}
{'loss': 1.098, 'grad_norm': 303569.875, 'learning_rate': 0.0001089396368678771, 'epoch': 6.59}
{'loss': 1.0988, 'grad_norm': 43295.2890625, 'learning_rate': 0.00010880963730120898, 'epoch': 6.59}
{'loss': 1.0983, 'grad_norm': 55675.92578125, 'learning_rate': 0.00010867963773454088, 'epoch': 6.59}
{'loss': 1.099, 'grad_norm': 28390.265625, 'learning_rate': 0.00010854963816787276, 'epoch': 6.6}
{'loss': 1.0988, 'grad_norm': 38803.65234375, 'learning_rate': 0.00010841963860120465, 'epoch': 6.6}
{'loss': 1.0977, 'grad_norm': 4488.57275390625, 'learning_rate': 0.00010828963903453653, 'epoch': 6.61}
{'loss': 1.0983, 'grad_norm': 45626.4921875, 'learning_rate': 0.00010815963946786843, 'epoch': 6.61}
{'loss': 1.0979, 'grad_norm': 868939.5625, 'learning_rate': 0.00010802963990120032, 'epoch': 6.62}
{'loss': 1.098, 'grad_norm': 14480.09375, 'learning_rate': 0.0001078996403345322, 'epoch': 6.62}
{'loss': 1.0991, 'grad_norm': 489156.21875, 'learning_rate': 0.00010776964076786409, 'epoch': 6.62}
{'loss': 1.0987, 'grad_norm': 1229.1981201171875, 'learning_rate': 0.000107639641201196, 'epoch': 6.63}
{'loss': 1.0991, 'grad_norm': 178881.625, 'learning_rate': 0.00010750964163452787, 'epoch': 6.63}
{'loss': 1.0993, 'grad_norm': 14463.89453125, 'learning_rate': 0.00010737964206785977, 'epoch': 6.64}
{'loss': 1.0992, 'grad_norm': 17144.8046875, 'learning_rate': 0.00010724964250119164, 'epoch': 6.64}
{'loss': 1.098, 'grad_norm': 1156872.75, 'learning_rate': 0.00010711964293452355, 'epoch': 6.64}
{'loss': 1.0984, 'grad_norm': 9869.4443359375, 'learning_rate': 0.00010698964336785544, 'epoch': 6.65}
{'loss': 1.0987, 'grad_norm': 22479.12890625, 'learning_rate': 0.00010685964380118732, 'epoch': 6.65}
{'loss': 1.0986, 'grad_norm': 5022.56787109375, 'learning_rate': 0.00010672964423451921, 'epoch': 6.66}
{'loss': 1.0992, 'grad_norm': 62895.96484375, 'learning_rate': 0.0001065996446678511, 'epoch': 6.66}
{'loss': 1.0994, 'grad_norm': 18301.87890625, 'learning_rate': 0.00010646964510118299, 'epoch': 6.66}
{'loss': 1.0991, 'grad_norm': 1383.139892578125, 'learning_rate': 0.00010633964553451487, 'epoch': 6.67}
{'loss': 1.0979, 'grad_norm': 12315.6044921875, 'learning_rate': 0.00010620964596784676, 'epoch': 6.67}
{'loss': 1.0982, 'grad_norm': 25527.83203125, 'learning_rate': 0.00010607964640117866, 'epoch': 6.68}
{'loss': 1.0984, 'grad_norm': 85283.1796875, 'learning_rate': 0.00010594964683451054, 'epoch': 6.68}
{'loss': 1.0993, 'grad_norm': 17503.984375, 'learning_rate': 0.00010581964726784243, 'epoch': 6.68}
{'loss': 1.0991, 'grad_norm': 3071.678955078125, 'learning_rate': 0.00010568964770117431, 'epoch': 6.69}
{'loss': 1.099, 'grad_norm': 331760.21875, 'learning_rate': 0.00010555964813450621, 'epoch': 6.69}
{'loss': 1.0995, 'grad_norm': 8381056.0, 'learning_rate': 0.0001054296485678381, 'epoch': 6.7}
{'loss': 1.0985, 'grad_norm': 41062.40625, 'learning_rate': 0.00010529964900116998, 'epoch': 6.7}
{'loss': 1.0992, 'grad_norm': 25697616.0, 'learning_rate': 0.00010516964943450187, 'epoch': 6.7}
{'loss': 1.0987, 'grad_norm': 20507.75390625, 'learning_rate': 0.00010503964986783376, 'epoch': 6.71}
{'loss': 1.0984, 'grad_norm': 44690.33203125, 'learning_rate': 0.00010490965030116565, 'epoch': 6.71}
{'loss': 1.0997, 'grad_norm': 3133382.0, 'learning_rate': 0.00010477965073449754, 'epoch': 6.72}
{'loss': 1.0995, 'grad_norm': 18467.986328125, 'learning_rate': 0.00010464965116782942, 'epoch': 6.72}
{'loss': 1.099, 'grad_norm': 60783.3515625, 'learning_rate': 0.00010451965160116133, 'epoch': 6.73}
{'loss': 1.0979, 'grad_norm': 37372.62890625, 'learning_rate': 0.0001043896520344932, 'epoch': 6.73}
{'loss': 1.1008, 'grad_norm': 105463.734375, 'learning_rate': 0.0001042596524678251, 'epoch': 6.73}
{'loss': 1.0973, 'grad_norm': 10639.619140625, 'learning_rate': 0.00010412965290115697, 'epoch': 6.74}
{'loss': 1.0982, 'grad_norm': 6048.76416015625, 'learning_rate': 0.00010399965333448888, 'epoch': 6.74}
{'loss': 1.0992, 'grad_norm': 202572.1875, 'learning_rate': 0.00010386965376782077, 'epoch': 6.75}
{'loss': 1.0986, 'grad_norm': 1310276.125, 'learning_rate': 0.00010373965420115265, 'epoch': 6.75}
{'loss': 1.0992, 'grad_norm': 1278.9620361328125, 'learning_rate': 0.00010360965463448454, 'epoch': 6.75}
{'loss': 1.0992, 'grad_norm': 63528.6640625, 'learning_rate': 0.00010347965506781644, 'epoch': 6.76}
{'loss': 1.0981, 'grad_norm': 420463.1875, 'learning_rate': 0.00010334965550114832, 'epoch': 6.76}
{'loss': 1.0977, 'grad_norm': 1001321.4375, 'learning_rate': 0.00010321965593448021, 'epoch': 6.77}
{'loss': 1.0986, 'grad_norm': 100022.375, 'learning_rate': 0.00010308965636781209, 'epoch': 6.77}
{'loss': 1.0981, 'grad_norm': 6311.00927734375, 'learning_rate': 0.00010295965680114399, 'epoch': 6.77}
{'loss': 1.0978, 'grad_norm': 112262.21875, 'learning_rate': 0.00010282965723447588, 'epoch': 6.78}
{'loss': 1.0993, 'grad_norm': 64881.03125, 'learning_rate': 0.00010269965766780776, 'epoch': 6.78}
{'loss': 1.0989, 'grad_norm': 36502.6875, 'learning_rate': 0.00010256965810113965, 'epoch': 6.79}
{'loss': 1.0985, 'grad_norm': 5922.08349609375, 'learning_rate': 0.00010243965853447154, 'epoch': 6.79}
{'loss': 1.0994, 'grad_norm': 13532.771484375, 'learning_rate': 0.00010230965896780343, 'epoch': 6.79}
{'loss': 1.0988, 'grad_norm': 256242.859375, 'learning_rate': 0.00010217965940113531, 'epoch': 6.8}
{'loss': 1.1004, 'grad_norm': 34544.890625, 'learning_rate': 0.0001020496598344672, 'epoch': 6.8}
{'loss': 1.0998, 'grad_norm': 20853.62890625, 'learning_rate': 0.0001019196602677991, 'epoch': 6.81}
{'loss': 1.0982, 'grad_norm': 8475.4814453125, 'learning_rate': 0.00010178966070113098, 'epoch': 6.81}
{'loss': 1.0978, 'grad_norm': 2663.959716796875, 'learning_rate': 0.00010165966113446287, 'epoch': 6.81}
{'loss': 1.0988, 'grad_norm': 8789.265625, 'learning_rate': 0.00010152966156779478, 'epoch': 6.82}
{'loss': 1.0982, 'grad_norm': 2892.490234375, 'learning_rate': 0.00010139966200112666, 'epoch': 6.82}
{'loss': 1.0988, 'grad_norm': 21935.697265625, 'learning_rate': 0.00010126966243445855, 'epoch': 6.83}
{'loss': 1.0993, 'grad_norm': 59990.671875, 'learning_rate': 0.00010113966286779042, 'epoch': 6.83}
{'loss': 1.0997, 'grad_norm': 117709.9921875, 'learning_rate': 0.00010100966330112233, 'epoch': 6.84}
{'loss': 1.0986, 'grad_norm': 12331549.0, 'learning_rate': 0.00010087966373445422, 'epoch': 6.84}
{'loss': 1.0996, 'grad_norm': 4325.56298828125, 'learning_rate': 0.0001007496641677861, 'epoch': 6.84}
{'loss': 1.0992, 'grad_norm': 2588.822509765625, 'learning_rate': 0.00010061966460111799, 'epoch': 6.85}
{'loss': 1.0998, 'grad_norm': 25531.990234375, 'learning_rate': 0.00010048966503444988, 'epoch': 6.85}
{'loss': 1.0998, 'grad_norm': 24904.220703125, 'learning_rate': 0.00010035966546778177, 'epoch': 6.86}
{'loss': 1.0985, 'grad_norm': 7528.73974609375, 'learning_rate': 0.00010022966590111365, 'epoch': 6.86}
{'loss': 1.1, 'grad_norm': 3611.808837890625, 'learning_rate': 0.00010009966633444554, 'epoch': 6.86}
{'loss': 1.0986, 'grad_norm': 9031.462890625, 'learning_rate': 9.996966676777744e-05, 'epoch': 6.87}
{'loss': 1.0993, 'grad_norm': 100757.359375, 'learning_rate': 9.983966720110932e-05, 'epoch': 6.87}
{'loss': 1.099, 'grad_norm': 148533.25, 'learning_rate': 9.970966763444121e-05, 'epoch': 6.88}
{'loss': 1.0983, 'grad_norm': 28772.888671875, 'learning_rate': 9.957966806777309e-05, 'epoch': 6.88}
{'loss': 1.0988, 'grad_norm': 23611.91796875, 'learning_rate': 9.944966850110499e-05, 'epoch': 6.88}
{'loss': 1.0986, 'grad_norm': 3716.223876953125, 'learning_rate': 9.931966893443688e-05, 'epoch': 6.89}
{'loss': 1.0992, 'grad_norm': 47574.31640625, 'learning_rate': 9.918966936776876e-05, 'epoch': 6.89}
{'loss': 1.0989, 'grad_norm': 186064.90625, 'learning_rate': 9.905966980110065e-05, 'epoch': 6.9}
{'loss': 1.0986, 'grad_norm': 23549.7421875, 'learning_rate': 9.892967023443256e-05, 'epoch': 6.9}
{'loss': 1.0997, 'grad_norm': 547202.0625, 'learning_rate': 9.879967066776443e-05, 'epoch': 6.9}
{'loss': 1.0985, 'grad_norm': 58947.6328125, 'learning_rate': 9.866967110109633e-05, 'epoch': 6.91}
{'loss': 1.0993, 'grad_norm': 95071.3984375, 'learning_rate': 9.85396715344282e-05, 'epoch': 6.91}
{'loss': 1.0987, 'grad_norm': 2819.467529296875, 'learning_rate': 9.840967196776011e-05, 'epoch': 6.92}
{'loss': 1.0993, 'grad_norm': 38004.0859375, 'learning_rate': 9.827967240109198e-05, 'epoch': 6.92}
{'loss': 1.098, 'grad_norm': 1697139.25, 'learning_rate': 9.814967283442388e-05, 'epoch': 6.92}
{'loss': 1.1003, 'grad_norm': 27609.099609375, 'learning_rate': 9.801967326775577e-05, 'epoch': 6.93}
{'loss': 1.0982, 'grad_norm': 1309460.625, 'learning_rate': 9.788967370108766e-05, 'epoch': 6.93}
{'loss': 1.0999, 'grad_norm': 5894.60791015625, 'learning_rate': 9.775967413441955e-05, 'epoch': 6.94}
{'loss': 1.0991, 'grad_norm': 15530.3369140625, 'learning_rate': 9.762967456775143e-05, 'epoch': 6.94}
{'loss': 1.099, 'grad_norm': 302732.875, 'learning_rate': 9.749967500108332e-05, 'epoch': 6.95}
{'loss': 1.0985, 'grad_norm': 229468.375, 'learning_rate': 9.736967543441522e-05, 'epoch': 6.95}
{'loss': 1.0987, 'grad_norm': 8894.7880859375, 'learning_rate': 9.72396758677471e-05, 'epoch': 6.95}
{'loss': 1.1005, 'grad_norm': 2725.2392578125, 'learning_rate': 9.710967630107899e-05, 'epoch': 6.96}
{'loss': 1.0979, 'grad_norm': 16584.181640625, 'learning_rate': 9.697967673441087e-05, 'epoch': 6.96}
{'loss': 1.0983, 'grad_norm': 52515.49609375, 'learning_rate': 9.684967716774277e-05, 'epoch': 6.97}
{'loss': 1.098, 'grad_norm': 11150.943359375, 'learning_rate': 9.671967760107466e-05, 'epoch': 6.97}
{'loss': 1.0989, 'grad_norm': 117278.9453125, 'learning_rate': 9.658967803440654e-05, 'epoch': 6.97}
{'loss': 1.0998, 'grad_norm': 2863.38037109375, 'learning_rate': 9.645967846773843e-05, 'epoch': 6.98}
{'loss': 1.0982, 'grad_norm': 311171.75, 'learning_rate': 9.632967890107032e-05, 'epoch': 6.98}
{'loss': 1.1002, 'grad_norm': 31140.533203125, 'learning_rate': 9.619967933440221e-05, 'epoch': 6.99}
{'loss': 1.0985, 'grad_norm': 1088.8406982421875, 'learning_rate': 9.60696797677341e-05, 'epoch': 6.99}
{'loss': 1.0996, 'grad_norm': 1160.102783203125, 'learning_rate': 9.593968020106598e-05, 'epoch': 6.99}
{'loss': 1.099, 'grad_norm': 15681.6796875, 'learning_rate': 9.580968063439789e-05, 'epoch': 7.0}
{'eval_loss': 1.0980942249298096, 'eval_accuracy': 0.3544574630667346, 'eval_runtime': 5.3887, 'eval_samples_per_second': 1821.397, 'eval_steps_per_second': 11.506, 'epoch': 7.0}
{'loss': 1.0985, 'grad_norm': 8331.544921875, 'learning_rate': 9.567968106772976e-05, 'epoch': 7.0}
{'loss': 1.0985, 'grad_norm': 2284.181396484375, 'learning_rate': 9.554968150106165e-05, 'epoch': 7.01}
{'loss': 1.1001, 'grad_norm': 5587.98974609375, 'learning_rate': 9.541968193439353e-05, 'epoch': 7.01}
{'loss': 1.0985, 'grad_norm': 84376.078125, 'learning_rate': 9.528968236772544e-05, 'epoch': 7.01}
{'loss': 1.0989, 'grad_norm': 5264.8525390625, 'learning_rate': 9.515968280105733e-05, 'epoch': 7.02}
{'loss': 1.1001, 'grad_norm': 1101263.25, 'learning_rate': 9.50296832343892e-05, 'epoch': 7.02}
{'loss': 1.0988, 'grad_norm': 50806.35546875, 'learning_rate': 9.48996836677211e-05, 'epoch': 7.03}
{'loss': 1.099, 'grad_norm': 1419336.0, 'learning_rate': 9.4769684101053e-05, 'epoch': 7.03}
{'loss': 1.0991, 'grad_norm': 32527.873046875, 'learning_rate': 9.463968453438488e-05, 'epoch': 7.03}
{'loss': 1.0974, 'grad_norm': 1349989.0, 'learning_rate': 9.450968496771677e-05, 'epoch': 7.04}
{'loss': 1.0991, 'grad_norm': 8546.4814453125, 'learning_rate': 9.437968540104865e-05, 'epoch': 7.04}
{'loss': 1.0986, 'grad_norm': 1652449.375, 'learning_rate': 9.424968583438055e-05, 'epoch': 7.05}
{'loss': 1.0984, 'grad_norm': 14058.7109375, 'learning_rate': 9.411968626771244e-05, 'epoch': 7.05}
{'loss': 1.0999, 'grad_norm': 3642.178955078125, 'learning_rate': 9.398968670104432e-05, 'epoch': 7.05}
{'loss': 1.0996, 'grad_norm': 5684.7626953125, 'learning_rate': 9.385968713437621e-05, 'epoch': 7.06}
{'loss': 1.0995, 'grad_norm': 625.113037109375, 'learning_rate': 9.37296875677081e-05, 'epoch': 7.06}
{'loss': 1.0978, 'grad_norm': 99103.7734375, 'learning_rate': 9.359968800103999e-05, 'epoch': 7.07}
{'loss': 1.0992, 'grad_norm': 50449.79296875, 'learning_rate': 9.346968843437187e-05, 'epoch': 7.07}
{'loss': 1.0985, 'grad_norm': 42793.93359375, 'learning_rate': 9.333968886770376e-05, 'epoch': 7.08}
{'loss': 1.0992, 'grad_norm': 178136.5, 'learning_rate': 9.320968930103566e-05, 'epoch': 7.08}
{'loss': 1.0993, 'grad_norm': 16080.3505859375, 'learning_rate': 9.307968973436754e-05, 'epoch': 7.08}
{'loss': 1.0995, 'grad_norm': 16495.033203125, 'learning_rate': 9.294969016769943e-05, 'epoch': 7.09}
{'loss': 1.0995, 'grad_norm': 657502.5, 'learning_rate': 9.281969060103131e-05, 'epoch': 7.09}
{'loss': 1.0969, 'grad_norm': 156514.0625, 'learning_rate': 9.268969103436322e-05, 'epoch': 7.1}
{'loss': 1.0985, 'grad_norm': 113507.0859375, 'learning_rate': 9.25596914676951e-05, 'epoch': 7.1}
{'loss': 1.1, 'grad_norm': 2660764.0, 'learning_rate': 9.242969190102698e-05, 'epoch': 7.1}
{'loss': 1.0985, 'grad_norm': 23070.462890625, 'learning_rate': 9.229969233435887e-05, 'epoch': 7.11}
{'loss': 1.0993, 'grad_norm': 1722.2545166015625, 'learning_rate': 9.216969276769078e-05, 'epoch': 7.11}
{'loss': 1.0981, 'grad_norm': 2419.624755859375, 'learning_rate': 9.203969320102266e-05, 'epoch': 7.12}
{'loss': 1.0987, 'grad_norm': 174132.03125, 'learning_rate': 9.190969363435455e-05, 'epoch': 7.12}
{'loss': 1.0996, 'grad_norm': 1252.6195068359375, 'learning_rate': 9.177969406768642e-05, 'epoch': 7.12}
{'loss': 1.0964, 'grad_norm': 2605.108154296875, 'learning_rate': 9.164969450101833e-05, 'epoch': 7.13}
{'loss': 1.0995, 'grad_norm': 29700.416015625, 'learning_rate': 9.151969493435021e-05, 'epoch': 7.13}
{'loss': 1.1011, 'grad_norm': 1072240.625, 'learning_rate': 9.13896953676821e-05, 'epoch': 7.14}
{'loss': 1.0968, 'grad_norm': 22739.80078125, 'learning_rate': 9.125969580101399e-05, 'epoch': 7.14}
{'loss': 1.1001, 'grad_norm': 66217.234375, 'learning_rate': 9.112969623434588e-05, 'epoch': 7.14}
{'loss': 1.099, 'grad_norm': 597690.25, 'learning_rate': 9.099969666767777e-05, 'epoch': 7.15}
{'loss': 1.0988, 'grad_norm': 166409.53125, 'learning_rate': 9.086969710100965e-05, 'epoch': 7.15}
{'loss': 1.1, 'grad_norm': 40485.4921875, 'learning_rate': 9.073969753434154e-05, 'epoch': 7.16}
{'loss': 1.0986, 'grad_norm': 448037.125, 'learning_rate': 9.060969796767344e-05, 'epoch': 7.16}
{'loss': 1.0971, 'grad_norm': 36079.15625, 'learning_rate': 9.047969840100532e-05, 'epoch': 7.16}
{'loss': 1.0988, 'grad_norm': 10701.5, 'learning_rate': 9.034969883433721e-05, 'epoch': 7.17}
{'loss': 1.1004, 'grad_norm': 7780.8974609375, 'learning_rate': 9.021969926766909e-05, 'epoch': 7.17}
{'loss': 1.0973, 'grad_norm': 2413766.0, 'learning_rate': 9.0089699701001e-05, 'epoch': 7.18}
{'loss': 1.0983, 'grad_norm': 41842.36328125, 'learning_rate': 8.995970013433288e-05, 'epoch': 7.18}
{'loss': 1.0984, 'grad_norm': 20291.228515625, 'learning_rate': 8.982970056766476e-05, 'epoch': 7.19}
{'loss': 1.0993, 'grad_norm': 96377.4375, 'learning_rate': 8.969970100099665e-05, 'epoch': 7.19}
{'loss': 1.0999, 'grad_norm': 5964148.5, 'learning_rate': 8.956970143432854e-05, 'epoch': 7.19}
{'loss': 1.0977, 'grad_norm': 42119.2578125, 'learning_rate': 8.943970186766044e-05, 'epoch': 7.2}
{'loss': 1.0986, 'grad_norm': 16376.0009765625, 'learning_rate': 8.930970230099233e-05, 'epoch': 7.2}
{'loss': 1.1006, 'grad_norm': 58201.828125, 'learning_rate': 8.91797027343242e-05, 'epoch': 7.21}
{'loss': 1.0995, 'grad_norm': 1089686.125, 'learning_rate': 8.904970316765611e-05, 'epoch': 7.21}
{'loss': 1.0998, 'grad_norm': 11537.8046875, 'learning_rate': 8.891970360098799e-05, 'epoch': 7.21}
{'loss': 1.0988, 'grad_norm': 106588.5546875, 'learning_rate': 8.878970403431988e-05, 'epoch': 7.22}
{'loss': 1.0999, 'grad_norm': 994339.9375, 'learning_rate': 8.865970446765175e-05, 'epoch': 7.22}
{'loss': 1.0988, 'grad_norm': 52824.9921875, 'learning_rate': 8.852970490098366e-05, 'epoch': 7.23}
{'loss': 1.0997, 'grad_norm': 2820.78515625, 'learning_rate': 8.839970533431555e-05, 'epoch': 7.23}
{'loss': 1.0998, 'grad_norm': 18854.802734375, 'learning_rate': 8.826970576764743e-05, 'epoch': 7.23}
{'loss': 1.0998, 'grad_norm': 422081.8125, 'learning_rate': 8.813970620097932e-05, 'epoch': 7.24}
{'loss': 1.099, 'grad_norm': 1302076.875, 'learning_rate': 8.800970663431122e-05, 'epoch': 7.24}
{'loss': 1.1008, 'grad_norm': 329060.71875, 'learning_rate': 8.78797070676431e-05, 'epoch': 7.25}
{'loss': 1.0979, 'grad_norm': 2311738.5, 'learning_rate': 8.774970750097499e-05, 'epoch': 7.25}
{'loss': 1.0991, 'grad_norm': 133260.4375, 'learning_rate': 8.761970793430687e-05, 'epoch': 7.25}
{'loss': 1.0993, 'grad_norm': 334549.8125, 'learning_rate': 8.748970836763877e-05, 'epoch': 7.26}
{'loss': 1.0985, 'grad_norm': 77439.71875, 'learning_rate': 8.735970880097066e-05, 'epoch': 7.26}
{'loss': 1.098, 'grad_norm': 403739.90625, 'learning_rate': 8.722970923430254e-05, 'epoch': 7.27}
{'loss': 1.0987, 'grad_norm': 2892219.0, 'learning_rate': 8.709970966763443e-05, 'epoch': 7.27}
{'loss': 1.0996, 'grad_norm': 90536.7109375, 'learning_rate': 8.696971010096632e-05, 'epoch': 7.27}
{'loss': 1.0968, 'grad_norm': 138969.15625, 'learning_rate': 8.683971053429821e-05, 'epoch': 7.28}
{'loss': 1.0986, 'grad_norm': 47252.40625, 'learning_rate': 8.670971096763009e-05, 'epoch': 7.28}
{'loss': 1.0971, 'grad_norm': 15283.4111328125, 'learning_rate': 8.657971140096198e-05, 'epoch': 7.29}
{'loss': 1.0995, 'grad_norm': 444021.59375, 'learning_rate': 8.644971183429389e-05, 'epoch': 7.29}
{'loss': 1.0992, 'grad_norm': 595074.9375, 'learning_rate': 8.631971226762576e-05, 'epoch': 7.3}
{'loss': 1.1, 'grad_norm': 34217.30078125, 'learning_rate': 8.618971270095766e-05, 'epoch': 7.3}
{'loss': 1.1002, 'grad_norm': 30473.560546875, 'learning_rate': 8.605971313428953e-05, 'epoch': 7.3}
{'loss': 1.0986, 'grad_norm': 7970.1318359375, 'learning_rate': 8.592971356762144e-05, 'epoch': 7.31}
{'loss': 1.0994, 'grad_norm': 4614878.0, 'learning_rate': 8.579971400095333e-05, 'epoch': 7.31}
{'loss': 1.0981, 'grad_norm': 10591.6982421875, 'learning_rate': 8.56697144342852e-05, 'epoch': 7.32}
{'loss': 1.1, 'grad_norm': 5658.453125, 'learning_rate': 8.55397148676171e-05, 'epoch': 7.32}
{'loss': 1.0991, 'grad_norm': 83751.5078125, 'learning_rate': 8.540971530094899e-05, 'epoch': 7.32}
{'loss': 1.0998, 'grad_norm': 26207.958984375, 'learning_rate': 8.527971573428088e-05, 'epoch': 7.33}
{'loss': 1.0987, 'grad_norm': 11405.5849609375, 'learning_rate': 8.514971616761277e-05, 'epoch': 7.33}
{'loss': 1.0989, 'grad_norm': 3429.701904296875, 'learning_rate': 8.501971660094465e-05, 'epoch': 7.34}
{'loss': 1.1, 'grad_norm': 1158540.25, 'learning_rate': 8.488971703427655e-05, 'epoch': 7.34}
{'loss': 1.0997, 'grad_norm': 853934.9375, 'learning_rate': 8.475971746760843e-05, 'epoch': 7.34}
{'loss': 1.098, 'grad_norm': 626512.1875, 'learning_rate': 8.462971790094032e-05, 'epoch': 7.35}
{'loss': 1.0999, 'grad_norm': 996234.4375, 'learning_rate': 8.44997183342722e-05, 'epoch': 7.35}
{'loss': 1.1002, 'grad_norm': 1540648.875, 'learning_rate': 8.43697187676041e-05, 'epoch': 7.36}
{'loss': 1.1008, 'grad_norm': 538817.625, 'learning_rate': 8.423971920093599e-05, 'epoch': 7.36}
{'loss': 1.0989, 'grad_norm': 2484.641357421875, 'learning_rate': 8.410971963426787e-05, 'epoch': 7.36}
{'loss': 1.0997, 'grad_norm': 300299.28125, 'learning_rate': 8.397972006759976e-05, 'epoch': 7.37}
{'loss': 1.1002, 'grad_norm': 1358026.75, 'learning_rate': 8.384972050093167e-05, 'epoch': 7.37}
{'loss': 1.1005, 'grad_norm': 95417.65625, 'learning_rate': 8.371972093426354e-05, 'epoch': 7.38}
{'loss': 1.0983, 'grad_norm': 1011187.1875, 'learning_rate': 8.358972136759543e-05, 'epoch': 7.38}
{'loss': 1.0995, 'grad_norm': 1266783.75, 'learning_rate': 8.345972180092731e-05, 'epoch': 7.38}
{'loss': 1.0988, 'grad_norm': 130890.40625, 'learning_rate': 8.332972223425922e-05, 'epoch': 7.39}
{'loss': 1.0985, 'grad_norm': 6779.11279296875, 'learning_rate': 8.319972266759111e-05, 'epoch': 7.39}
{'loss': 1.0984, 'grad_norm': 137661.03125, 'learning_rate': 8.306972310092298e-05, 'epoch': 7.4}
{'loss': 1.0994, 'grad_norm': 110170.6171875, 'learning_rate': 8.293972353425488e-05, 'epoch': 7.4}
{'loss': 1.0979, 'grad_norm': 112429.578125, 'learning_rate': 8.280972396758677e-05, 'epoch': 7.41}
{'loss': 1.0993, 'grad_norm': 589.9164428710938, 'learning_rate': 8.267972440091866e-05, 'epoch': 7.41}
{'loss': 1.0993, 'grad_norm': 29153.193359375, 'learning_rate': 8.254972483425053e-05, 'epoch': 7.41}
{'loss': 1.098, 'grad_norm': 58384.34765625, 'learning_rate': 8.241972526758243e-05, 'epoch': 7.42}
{'loss': 1.0993, 'grad_norm': 290562.125, 'learning_rate': 8.228972570091433e-05, 'epoch': 7.42}
{'loss': 1.0993, 'grad_norm': 415319.0, 'learning_rate': 8.215972613424621e-05, 'epoch': 7.43}
{'loss': 1.099, 'grad_norm': 18484.970703125, 'learning_rate': 8.20297265675781e-05, 'epoch': 7.43}
{'loss': 1.0986, 'grad_norm': 23088.99609375, 'learning_rate': 8.189972700090998e-05, 'epoch': 7.43}
{'loss': 1.0984, 'grad_norm': 2948.166015625, 'learning_rate': 8.176972743424188e-05, 'epoch': 7.44}
{'loss': 1.0975, 'grad_norm': 6619.0771484375, 'learning_rate': 8.163972786757377e-05, 'epoch': 7.44}
{'loss': 1.0994, 'grad_norm': 46866.85546875, 'learning_rate': 8.150972830090565e-05, 'epoch': 7.45}
{'loss': 1.0995, 'grad_norm': 15962.1630859375, 'learning_rate': 8.137972873423755e-05, 'epoch': 7.45}
{'loss': 1.0988, 'grad_norm': 47117.98828125, 'learning_rate': 8.124972916756944e-05, 'epoch': 7.45}
{'loss': 1.098, 'grad_norm': 2121.909912109375, 'learning_rate': 8.111972960090132e-05, 'epoch': 7.46}
{'loss': 1.1003, 'grad_norm': 17517.828125, 'learning_rate': 8.098973003423321e-05, 'epoch': 7.46}
{'loss': 1.0983, 'grad_norm': 6274.15380859375, 'learning_rate': 8.08597304675651e-05, 'epoch': 7.47}
{'loss': 1.0996, 'grad_norm': 21377.86328125, 'learning_rate': 8.0729730900897e-05, 'epoch': 7.47}
{'loss': 1.0992, 'grad_norm': 111155.015625, 'learning_rate': 8.059973133422887e-05, 'epoch': 7.47}
{'loss': 1.0985, 'grad_norm': 41648.0703125, 'learning_rate': 8.046973176756076e-05, 'epoch': 7.48}
{'loss': 1.0981, 'grad_norm': 690.7640991210938, 'learning_rate': 8.033973220089267e-05, 'epoch': 7.48}
{'loss': 1.0985, 'grad_norm': 374.30938720703125, 'learning_rate': 8.020973263422455e-05, 'epoch': 7.49}
{'loss': 1.0993, 'grad_norm': 40724.48828125, 'learning_rate': 8.007973306755644e-05, 'epoch': 7.49}
{'loss': 1.099, 'grad_norm': 157502.296875, 'learning_rate': 7.994973350088831e-05, 'epoch': 7.49}
{'loss': 1.0968, 'grad_norm': 1036445.375, 'learning_rate': 7.981973393422022e-05, 'epoch': 7.5}
{'loss': 1.0989, 'grad_norm': 32413.908203125, 'learning_rate': 7.968973436755211e-05, 'epoch': 7.5}
{'loss': 1.097, 'grad_norm': 5918.23486328125, 'learning_rate': 7.955973480088399e-05, 'epoch': 7.51}
{'loss': 1.0987, 'grad_norm': 999.351806640625, 'learning_rate': 7.942973523421588e-05, 'epoch': 7.51}
{'loss': 1.1009, 'grad_norm': 154586.578125, 'learning_rate': 7.929973566754778e-05, 'epoch': 7.52}
{'loss': 1.0996, 'grad_norm': 65168.296875, 'learning_rate': 7.916973610087966e-05, 'epoch': 7.52}
{'loss': 1.0987, 'grad_norm': 388708.34375, 'learning_rate': 7.903973653421155e-05, 'epoch': 7.52}
{'loss': 1.0979, 'grad_norm': 3481.098876953125, 'learning_rate': 7.890973696754343e-05, 'epoch': 7.53}
{'loss': 1.0987, 'grad_norm': 4208890.5, 'learning_rate': 7.877973740087533e-05, 'epoch': 7.53}
{'loss': 1.0997, 'grad_norm': 52736.8125, 'learning_rate': 7.864973783420721e-05, 'epoch': 7.54}
{'loss': 1.0994, 'grad_norm': 30649990.0, 'learning_rate': 7.85197382675391e-05, 'epoch': 7.54}
{'loss': 1.0996, 'grad_norm': 43432.4375, 'learning_rate': 7.838973870087099e-05, 'epoch': 7.54}
{'loss': 1.0974, 'grad_norm': 246547.125, 'learning_rate': 7.825973913420288e-05, 'epoch': 7.55}
{'loss': 1.0976, 'grad_norm': 264227.5625, 'learning_rate': 7.812973956753477e-05, 'epoch': 7.55}
{'loss': 1.0994, 'grad_norm': 601986.6875, 'learning_rate': 7.799974000086665e-05, 'epoch': 7.56}
{'loss': 1.098, 'grad_norm': 18775.552734375, 'learning_rate': 7.786974043419854e-05, 'epoch': 7.56}
{'loss': 1.1005, 'grad_norm': 52253.234375, 'learning_rate': 7.773974086753045e-05, 'epoch': 7.56}
{'loss': 1.0986, 'grad_norm': 71284.90625, 'learning_rate': 7.760974130086232e-05, 'epoch': 7.57}
{'loss': 1.0985, 'grad_norm': 492339.6875, 'learning_rate': 7.747974173419421e-05, 'epoch': 7.57}
{'loss': 1.098, 'grad_norm': 361969.625, 'learning_rate': 7.734974216752609e-05, 'epoch': 7.58}
{'loss': 1.0994, 'grad_norm': 2142691.75, 'learning_rate': 7.7219742600858e-05, 'epoch': 7.58}
{'loss': 1.0999, 'grad_norm': 56669.33984375, 'learning_rate': 7.708974303418989e-05, 'epoch': 7.58}
{'loss': 1.0991, 'grad_norm': 30144.396484375, 'learning_rate': 7.695974346752177e-05, 'epoch': 7.59}
{'loss': 1.1001, 'grad_norm': 2177845.75, 'learning_rate': 7.682974390085366e-05, 'epoch': 7.59}
{'loss': 1.1007, 'grad_norm': 116872.890625, 'learning_rate': 7.669974433418555e-05, 'epoch': 7.6}
{'loss': 1.0989, 'grad_norm': 66448.703125, 'learning_rate': 7.656974476751744e-05, 'epoch': 7.6}
{'loss': 1.0987, 'grad_norm': 3190.703125, 'learning_rate': 7.643974520084933e-05, 'epoch': 7.6}
{'loss': 1.0987, 'grad_norm': 8266.111328125, 'learning_rate': 7.63097456341812e-05, 'epoch': 7.61}
{'loss': 1.0985, 'grad_norm': 68052.4453125, 'learning_rate': 7.617974606751311e-05, 'epoch': 7.61}
{'loss': 1.0997, 'grad_norm': 31935.166015625, 'learning_rate': 7.604974650084499e-05, 'epoch': 7.62}
{'loss': 1.0986, 'grad_norm': 83030.4609375, 'learning_rate': 7.591974693417688e-05, 'epoch': 7.62}
{'loss': 1.0986, 'grad_norm': 21077.9921875, 'learning_rate': 7.578974736750876e-05, 'epoch': 7.63}
{'loss': 1.0974, 'grad_norm': 55735.79296875, 'learning_rate': 7.565974780084066e-05, 'epoch': 7.63}
{'loss': 1.1002, 'grad_norm': 28162.05078125, 'learning_rate': 7.552974823417255e-05, 'epoch': 7.63}
{'loss': 1.0988, 'grad_norm': 4528.681640625, 'learning_rate': 7.539974866750443e-05, 'epoch': 7.64}
{'loss': 1.098, 'grad_norm': 63984.875, 'learning_rate': 7.526974910083632e-05, 'epoch': 7.64}
{'loss': 1.098, 'grad_norm': 114816.6171875, 'learning_rate': 7.513974953416823e-05, 'epoch': 7.65}
{'loss': 1.1001, 'grad_norm': 1188.3194580078125, 'learning_rate': 7.50097499675001e-05, 'epoch': 7.65}
{'loss': 1.0992, 'grad_norm': 778258.4375, 'learning_rate': 7.4879750400832e-05, 'epoch': 7.65}
{'loss': 1.0987, 'grad_norm': 215239.546875, 'learning_rate': 7.474975083416388e-05, 'epoch': 7.66}
{'loss': 1.0986, 'grad_norm': 601671.0625, 'learning_rate': 7.461975126749576e-05, 'epoch': 7.66}
{'loss': 1.0985, 'grad_norm': 78611.4375, 'learning_rate': 7.448975170082767e-05, 'epoch': 7.67}
{'loss': 1.0984, 'grad_norm': 8483.35546875, 'learning_rate': 7.435975213415954e-05, 'epoch': 7.67}
{'loss': 1.0984, 'grad_norm': 8006.755859375, 'learning_rate': 7.422975256749143e-05, 'epoch': 7.67}
{'loss': 1.0983, 'grad_norm': 9359.7470703125, 'learning_rate': 7.409975300082333e-05, 'epoch': 7.68}
{'loss': 1.0991, 'grad_norm': 34444.7578125, 'learning_rate': 7.396975343415522e-05, 'epoch': 7.68}
{'loss': 1.0999, 'grad_norm': 5386.0302734375, 'learning_rate': 7.38397538674871e-05, 'epoch': 7.69}
{'loss': 1.0987, 'grad_norm': 2069.20947265625, 'learning_rate': 7.3709754300819e-05, 'epoch': 7.69}
{'loss': 1.0994, 'grad_norm': 537689.5625, 'learning_rate': 7.357975473415088e-05, 'epoch': 7.69}
{'loss': 1.098, 'grad_norm': 10918830.0, 'learning_rate': 7.344975516748277e-05, 'epoch': 7.7}
{'loss': 1.0989, 'grad_norm': 70148.5, 'learning_rate': 7.331975560081466e-05, 'epoch': 7.7}
{'loss': 1.099, 'grad_norm': 9406.62109375, 'learning_rate': 7.318975603414655e-05, 'epoch': 7.71}
{'loss': 1.0997, 'grad_norm': 5340433.5, 'learning_rate': 7.305975646747844e-05, 'epoch': 7.71}
{'loss': 1.1005, 'grad_norm': 323.3279724121094, 'learning_rate': 7.292975690081033e-05, 'epoch': 7.71}
{'loss': 1.0985, 'grad_norm': 12493.4814453125, 'learning_rate': 7.279975733414221e-05, 'epoch': 7.72}
{'loss': 1.1, 'grad_norm': 1504455.25, 'learning_rate': 7.26697577674741e-05, 'epoch': 7.72}
{'loss': 1.0987, 'grad_norm': 12586.083984375, 'learning_rate': 7.253975820080599e-05, 'epoch': 7.73}
{'loss': 1.101, 'grad_norm': 223016.390625, 'learning_rate': 7.240975863413788e-05, 'epoch': 7.73}
{'loss': 1.0983, 'grad_norm': 41915.0390625, 'learning_rate': 7.227975906746977e-05, 'epoch': 7.74}
{'loss': 1.0984, 'grad_norm': 67674.40625, 'learning_rate': 7.214975950080166e-05, 'epoch': 7.74}
{'loss': 1.0988, 'grad_norm': 23702440.0, 'learning_rate': 7.201975993413354e-05, 'epoch': 7.74}
{'loss': 1.0994, 'grad_norm': 142630.203125, 'learning_rate': 7.188976036746543e-05, 'epoch': 7.75}
{'loss': 1.1023, 'grad_norm': 107151.2734375, 'learning_rate': 7.175976080079732e-05, 'epoch': 7.75}
{'loss': 1.1008, 'grad_norm': 925515.9375, 'learning_rate': 7.162976123412921e-05, 'epoch': 7.76}
{'loss': 1.0991, 'grad_norm': 29749.3828125, 'learning_rate': 7.14997616674611e-05, 'epoch': 7.76}
{'loss': 1.0996, 'grad_norm': 80480.0703125, 'learning_rate': 7.1369762100793e-05, 'epoch': 7.76}
{'loss': 1.101, 'grad_norm': 1126.2781982421875, 'learning_rate': 7.123976253412487e-05, 'epoch': 7.77}
{'loss': 1.0993, 'grad_norm': 14372.1376953125, 'learning_rate': 7.110976296745678e-05, 'epoch': 7.77}
{'loss': 1.0984, 'grad_norm': 5908.08056640625, 'learning_rate': 7.097976340078865e-05, 'epoch': 7.78}
{'loss': 1.0979, 'grad_norm': 3125.17578125, 'learning_rate': 7.084976383412055e-05, 'epoch': 7.78}
{'loss': 1.1006, 'grad_norm': 30215.75, 'learning_rate': 7.071976426745244e-05, 'epoch': 7.78}
{'loss': 1.1011, 'grad_norm': 192325.609375, 'learning_rate': 7.058976470078433e-05, 'epoch': 7.79}
{'loss': 1.1005, 'grad_norm': 47500.81640625, 'learning_rate': 7.04597651341162e-05, 'epoch': 7.79}
{'loss': 1.0977, 'grad_norm': 30418040.0, 'learning_rate': 7.032976556744811e-05, 'epoch': 7.8}
{'loss': 1.0981, 'grad_norm': 8592.416015625, 'learning_rate': 7.019976600077999e-05, 'epoch': 7.8}
{'loss': 1.102, 'grad_norm': 166268.375, 'learning_rate': 7.006976643411188e-05, 'epoch': 7.8}
{'loss': 1.0989, 'grad_norm': 14344.71875, 'learning_rate': 6.993976686744377e-05, 'epoch': 7.81}
{'loss': 1.0999, 'grad_norm': 2559.267578125, 'learning_rate': 6.980976730077566e-05, 'epoch': 7.81}
{'loss': 1.0995, 'grad_norm': 97012.5625, 'learning_rate': 6.967976773410755e-05, 'epoch': 7.82}
{'loss': 1.0984, 'grad_norm': 93779.1015625, 'learning_rate': 6.954976816743944e-05, 'epoch': 7.82}
{'loss': 1.1009, 'grad_norm': 96995.75, 'learning_rate': 6.941976860077132e-05, 'epoch': 7.82}
{'loss': 1.0999, 'grad_norm': 146626.546875, 'learning_rate': 6.928976903410321e-05, 'epoch': 7.83}
{'loss': 1.1026, 'grad_norm': 969.6891479492188, 'learning_rate': 6.91597694674351e-05, 'epoch': 7.83}
{'loss': 1.0994, 'grad_norm': 1588195.5, 'learning_rate': 6.902976990076699e-05, 'epoch': 7.84}
{'loss': 1.0981, 'grad_norm': 2667.256103515625, 'learning_rate': 6.889977033409888e-05, 'epoch': 7.84}
{'loss': 1.0999, 'grad_norm': 24528.25, 'learning_rate': 6.876977076743077e-05, 'epoch': 7.85}
{'loss': 1.1002, 'grad_norm': 1023504.75, 'learning_rate': 6.863977120076265e-05, 'epoch': 7.85}
{'loss': 1.0983, 'grad_norm': 24116.63671875, 'learning_rate': 6.850977163409454e-05, 'epoch': 7.85}
{'loss': 1.1001, 'grad_norm': 110926.0625, 'learning_rate': 6.837977206742643e-05, 'epoch': 7.86}
{'loss': 1.0999, 'grad_norm': 414.3542175292969, 'learning_rate': 6.824977250075832e-05, 'epoch': 7.86}
{'loss': 1.1009, 'grad_norm': 5367.6005859375, 'learning_rate': 6.811977293409022e-05, 'epoch': 7.87}
{'loss': 1.0994, 'grad_norm': 122575.3515625, 'learning_rate': 6.79897733674221e-05, 'epoch': 7.87}
{'loss': 1.0999, 'grad_norm': 17459.763671875, 'learning_rate': 6.785977380075398e-05, 'epoch': 7.87}
{'loss': 1.1002, 'grad_norm': 2650.9453125, 'learning_rate': 6.772977423408588e-05, 'epoch': 7.88}
{'loss': 1.0993, 'grad_norm': 3150.72265625, 'learning_rate': 6.759977466741777e-05, 'epoch': 7.88}
{'loss': 1.1, 'grad_norm': 5399.92431640625, 'learning_rate': 6.746977510074966e-05, 'epoch': 7.89}
{'loss': 1.099, 'grad_norm': 21425.990234375, 'learning_rate': 6.733977553408155e-05, 'epoch': 7.89}
{'loss': 1.1004, 'grad_norm': 31998.939453125, 'learning_rate': 6.720977596741344e-05, 'epoch': 7.89}
{'loss': 1.1, 'grad_norm': 61955.87109375, 'learning_rate': 6.707977640074532e-05, 'epoch': 7.9}
{'loss': 1.0997, 'grad_norm': 107597.578125, 'learning_rate': 6.694977683407722e-05, 'epoch': 7.9}
{'loss': 1.0996, 'grad_norm': 26518.388671875, 'learning_rate': 6.68197772674091e-05, 'epoch': 7.91}
{'loss': 1.0981, 'grad_norm': 84916072.0, 'learning_rate': 6.668977770074099e-05, 'epoch': 7.91}
{'loss': 1.0985, 'grad_norm': 224286.390625, 'learning_rate': 6.655977813407288e-05, 'epoch': 7.91}
{'loss': 1.0995, 'grad_norm': 595.1140747070312, 'learning_rate': 6.642977856740477e-05, 'epoch': 7.92}
{'loss': 1.099, 'grad_norm': 62697.6171875, 'learning_rate': 6.629977900073665e-05, 'epoch': 7.92}
{'loss': 1.0995, 'grad_norm': 118154.1640625, 'learning_rate': 6.616977943406855e-05, 'epoch': 7.93}
{'loss': 1.0993, 'grad_norm': 3980.57861328125, 'learning_rate': 6.603977986740043e-05, 'epoch': 7.93}
{'loss': 1.099, 'grad_norm': 2994.380859375, 'learning_rate': 6.590978030073232e-05, 'epoch': 7.93}
{'loss': 1.0993, 'grad_norm': 11454.8369140625, 'learning_rate': 6.577978073406421e-05, 'epoch': 7.94}
{'loss': 1.1, 'grad_norm': 104095.3671875, 'learning_rate': 6.56497811673961e-05, 'epoch': 7.94}
{'loss': 1.0989, 'grad_norm': 13597.0009765625, 'learning_rate': 6.5519781600728e-05, 'epoch': 7.95}
{'loss': 1.1001, 'grad_norm': 75773840.0, 'learning_rate': 6.538978203405989e-05, 'epoch': 7.95}
{'loss': 1.0998, 'grad_norm': 3143.546630859375, 'learning_rate': 6.525978246739176e-05, 'epoch': 7.96}
{'loss': 1.1004, 'grad_norm': 42186.828125, 'learning_rate': 6.512978290072365e-05, 'epoch': 7.96}
{'loss': 1.0993, 'grad_norm': 236506.96875, 'learning_rate': 6.499978333405554e-05, 'epoch': 7.96}
{'loss': 1.099, 'grad_norm': 15557.7724609375, 'learning_rate': 6.486978376738744e-05, 'epoch': 7.97}
{'loss': 1.0983, 'grad_norm': 41794.875, 'learning_rate': 6.473978420071933e-05, 'epoch': 7.97}
{'loss': 1.0994, 'grad_norm': 8418813.0, 'learning_rate': 6.460978463405122e-05, 'epoch': 7.98}
{'loss': 1.0991, 'grad_norm': 7350.1220703125, 'learning_rate': 6.44797850673831e-05, 'epoch': 7.98}
{'loss': 1.0988, 'grad_norm': 25133.720703125, 'learning_rate': 6.434978550071499e-05, 'epoch': 7.98}
{'loss': 1.101, 'grad_norm': 3687.647705078125, 'learning_rate': 6.421978593404688e-05, 'epoch': 7.99}
{'loss': 1.0995, 'grad_norm': 13899.384765625, 'learning_rate': 6.408978636737877e-05, 'epoch': 7.99}
{'loss': 1.0993, 'grad_norm': 2387.2265625, 'learning_rate': 6.395978680071066e-05, 'epoch': 8.0}
{'loss': 1.1, 'grad_norm': 8685.2314453125, 'learning_rate': 6.382978723404255e-05, 'epoch': 8.0}
{'eval_loss': 1.0973310470581055, 'eval_accuracy': 0.3544574630667346, 'eval_runtime': 5.3945, 'eval_samples_per_second': 1819.442, 'eval_steps_per_second': 11.493, 'epoch': 8.0}
{'loss': 1.0997, 'grad_norm': 2683.759765625, 'learning_rate': 6.369978766737443e-05, 'epoch': 8.0}
{'loss': 1.0997, 'grad_norm': 4867.6982421875, 'learning_rate': 6.356978810070633e-05, 'epoch': 8.01}
{'loss': 1.0989, 'grad_norm': 20982.28125, 'learning_rate': 6.343978853403821e-05, 'epoch': 8.01}
{'loss': 1.0997, 'grad_norm': 112626.3515625, 'learning_rate': 6.33097889673701e-05, 'epoch': 8.02}
{'loss': 1.1008, 'grad_norm': 75440.9375, 'learning_rate': 6.317978940070199e-05, 'epoch': 8.02}
{'loss': 1.0995, 'grad_norm': 1266866.0, 'learning_rate': 6.304978983403388e-05, 'epoch': 8.02}
{'loss': 1.1011, 'grad_norm': 1830.591796875, 'learning_rate': 6.291979026736576e-05, 'epoch': 8.03}
{'loss': 1.1001, 'grad_norm': 615307.4375, 'learning_rate': 6.278979070069766e-05, 'epoch': 8.03}
{'loss': 1.0971, 'grad_norm': 57182.12109375, 'learning_rate': 6.265979113402954e-05, 'epoch': 8.04}
{'loss': 1.0996, 'grad_norm': 49368.65625, 'learning_rate': 6.252979156736143e-05, 'epoch': 8.04}
{'loss': 1.1002, 'grad_norm': 289558.71875, 'learning_rate': 6.239979200069332e-05, 'epoch': 8.04}
{'loss': 1.0989, 'grad_norm': 4717.42724609375, 'learning_rate': 6.226979243402521e-05, 'epoch': 8.05}
{'loss': 1.0973, 'grad_norm': 31617.361328125, 'learning_rate': 6.21397928673571e-05, 'epoch': 8.05}
{'loss': 1.0995, 'grad_norm': 4024.724365234375, 'learning_rate': 6.2009793300689e-05, 'epoch': 8.06}
{'loss': 1.1014, 'grad_norm': 4919.037109375, 'learning_rate': 6.187979373402087e-05, 'epoch': 8.06}
{'loss': 1.1039, 'grad_norm': 26875.966796875, 'learning_rate': 6.174979416735276e-05, 'epoch': 8.07}
{'loss': 1.0998, 'grad_norm': 103366.0859375, 'learning_rate': 6.161979460068466e-05, 'epoch': 8.07}
{'loss': 1.0995, 'grad_norm': 49119.88671875, 'learning_rate': 6.148979503401655e-05, 'epoch': 8.07}
{'loss': 1.0997, 'grad_norm': 34123.28125, 'learning_rate': 6.135979546734844e-05, 'epoch': 8.08}
{'loss': 1.1022, 'grad_norm': 4781362.0, 'learning_rate': 6.122979590068033e-05, 'epoch': 8.08}
{'loss': 1.0978, 'grad_norm': 73363.609375, 'learning_rate': 6.109979633401222e-05, 'epoch': 8.09}
{'loss': 1.0995, 'grad_norm': 15627.96484375, 'learning_rate': 6.0969796767344104e-05, 'epoch': 8.09}
{'loss': 1.1006, 'grad_norm': 337509.75, 'learning_rate': 6.0839797200675995e-05, 'epoch': 8.09}
{'loss': 1.1016, 'grad_norm': 1381250.75, 'learning_rate': 6.070979763400788e-05, 'epoch': 8.1}
{'loss': 1.1001, 'grad_norm': 37185972.0, 'learning_rate': 6.057979806733978e-05, 'epoch': 8.1}
{'loss': 1.0991, 'grad_norm': 5060.89306640625, 'learning_rate': 6.044979850067166e-05, 'epoch': 8.11}
{'loss': 1.0989, 'grad_norm': 3417.31494140625, 'learning_rate': 6.031979893400355e-05, 'epoch': 8.11}
{'loss': 1.1003, 'grad_norm': 2830.614013671875, 'learning_rate': 6.0189799367335436e-05, 'epoch': 8.11}
{'loss': 1.0998, 'grad_norm': 8797.3896484375, 'learning_rate': 6.005979980066733e-05, 'epoch': 8.12}
{'loss': 1.0994, 'grad_norm': 3758.0009765625, 'learning_rate': 5.992980023399921e-05, 'epoch': 8.12}
{'loss': 1.0988, 'grad_norm': 176820.40625, 'learning_rate': 5.979980066733111e-05, 'epoch': 8.13}
{'loss': 1.1009, 'grad_norm': 1480.447509765625, 'learning_rate': 5.966980110066299e-05, 'epoch': 8.13}
{'loss': 1.0998, 'grad_norm': 2160.088623046875, 'learning_rate': 5.9539801533994884e-05, 'epoch': 8.13}
{'loss': 1.0991, 'grad_norm': 6961.20361328125, 'learning_rate': 5.940980196732677e-05, 'epoch': 8.14}
{'loss': 1.0986, 'grad_norm': 12068.5751953125, 'learning_rate': 5.9279802400658666e-05, 'epoch': 8.14}
{'loss': 1.101, 'grad_norm': 2737765.0, 'learning_rate': 5.914980283399055e-05, 'epoch': 8.15}
{'loss': 1.1016, 'grad_norm': 5602.65625, 'learning_rate': 5.901980326732244e-05, 'epoch': 8.15}
{'loss': 1.0984, 'grad_norm': 1425.579345703125, 'learning_rate': 5.8889803700654325e-05, 'epoch': 8.15}
{'loss': 1.0975, 'grad_norm': 29296.314453125, 'learning_rate': 5.8759804133986216e-05, 'epoch': 8.16}
{'loss': 1.0984, 'grad_norm': 17303.916015625, 'learning_rate': 5.86298045673181e-05, 'epoch': 8.16}
{'loss': 1.0974, 'grad_norm': 12323.455078125, 'learning_rate': 5.849980500065e-05, 'epoch': 8.17}
{'loss': 1.0988, 'grad_norm': 76393.1015625, 'learning_rate': 5.836980543398188e-05, 'epoch': 8.17}
{'loss': 1.0979, 'grad_norm': 6385.74853515625, 'learning_rate': 5.8239805867313774e-05, 'epoch': 8.18}
{'loss': 1.1001, 'grad_norm': 4406.38916015625, 'learning_rate': 5.810980630064566e-05, 'epoch': 8.18}
{'loss': 1.0999, 'grad_norm': 464607.21875, 'learning_rate': 5.797980673397755e-05, 'epoch': 8.18}
{'loss': 1.0996, 'grad_norm': 25093.70703125, 'learning_rate': 5.784980716730944e-05, 'epoch': 8.19}
{'loss': 1.0996, 'grad_norm': 26315.646484375, 'learning_rate': 5.771980760064133e-05, 'epoch': 8.19}
{'loss': 1.0988, 'grad_norm': 75738.921875, 'learning_rate': 5.7589808033973215e-05, 'epoch': 8.2}
{'loss': 1.0999, 'grad_norm': 2473.5869140625, 'learning_rate': 5.7459808467305106e-05, 'epoch': 8.2}
{'loss': 1.098, 'grad_norm': 89864.171875, 'learning_rate': 5.732980890063699e-05, 'epoch': 8.2}
{'loss': 1.1, 'grad_norm': 453.57049560546875, 'learning_rate': 5.719980933396889e-05, 'epoch': 8.21}
{'loss': 1.099, 'grad_norm': 81876.2265625, 'learning_rate': 5.706980976730077e-05, 'epoch': 8.21}
{'loss': 1.0996, 'grad_norm': 40712.81640625, 'learning_rate': 5.693981020063266e-05, 'epoch': 8.22}
{'loss': 1.0986, 'grad_norm': 176044.84375, 'learning_rate': 5.680981063396455e-05, 'epoch': 8.22}
{'loss': 1.0999, 'grad_norm': 61445.57421875, 'learning_rate': 5.667981106729644e-05, 'epoch': 8.22}
{'loss': 1.0989, 'grad_norm': 26612.205078125, 'learning_rate': 5.654981150062832e-05, 'epoch': 8.23}
{'loss': 1.0998, 'grad_norm': 191755.625, 'learning_rate': 5.641981193396022e-05, 'epoch': 8.23}
{'loss': 1.1002, 'grad_norm': 13622.7333984375, 'learning_rate': 5.6289812367292104e-05, 'epoch': 8.24}
{'loss': 1.0998, 'grad_norm': 269337.8125, 'learning_rate': 5.6159812800623995e-05, 'epoch': 8.24}
{'loss': 1.0986, 'grad_norm': 23062.671875, 'learning_rate': 5.602981323395588e-05, 'epoch': 8.24}
{'loss': 1.0993, 'grad_norm': 1305408.5, 'learning_rate': 5.589981366728778e-05, 'epoch': 8.25}
{'loss': 1.0982, 'grad_norm': 1596002.25, 'learning_rate': 5.576981410061966e-05, 'epoch': 8.25}
{'loss': 1.0996, 'grad_norm': 397134.03125, 'learning_rate': 5.563981453395155e-05, 'epoch': 8.26}
{'loss': 1.0999, 'grad_norm': 1971477.5, 'learning_rate': 5.5509814967283436e-05, 'epoch': 8.26}
{'loss': 1.098, 'grad_norm': 7002.0244140625, 'learning_rate': 5.537981540061533e-05, 'epoch': 8.26}
{'loss': 1.099, 'grad_norm': 9905.30859375, 'learning_rate': 5.524981583394721e-05, 'epoch': 8.27}
{'loss': 1.0997, 'grad_norm': 877.917724609375, 'learning_rate': 5.511981626727911e-05, 'epoch': 8.27}
{'loss': 1.0985, 'grad_norm': 1816.4544677734375, 'learning_rate': 5.4989816700610994e-05, 'epoch': 8.28}
{'loss': 1.1, 'grad_norm': 48271.13671875, 'learning_rate': 5.4859817133942885e-05, 'epoch': 8.28}
{'loss': 1.0995, 'grad_norm': 54975.0, 'learning_rate': 5.472981756727477e-05, 'epoch': 8.29}
{'loss': 1.0993, 'grad_norm': 40834.96484375, 'learning_rate': 5.459981800060666e-05, 'epoch': 8.29}
{'loss': 1.0983, 'grad_norm': 1276.7802734375, 'learning_rate': 5.446981843393855e-05, 'epoch': 8.29}
{'loss': 1.0991, 'grad_norm': 1513.1015625, 'learning_rate': 5.433981886727044e-05, 'epoch': 8.3}
{'loss': 1.1004, 'grad_norm': 8586.7763671875, 'learning_rate': 5.4209819300602326e-05, 'epoch': 8.3}
{'loss': 1.0981, 'grad_norm': 564189.9375, 'learning_rate': 5.407981973393422e-05, 'epoch': 8.31}
{'loss': 1.1008, 'grad_norm': 49054.56640625, 'learning_rate': 5.39498201672661e-05, 'epoch': 8.31}
{'loss': 1.0996, 'grad_norm': 334887.65625, 'learning_rate': 5.3819820600598e-05, 'epoch': 8.31}
{'loss': 1.0967, 'grad_norm': 533161.0, 'learning_rate': 5.368982103392988e-05, 'epoch': 8.32}
{'loss': 1.0992, 'grad_norm': 279680.78125, 'learning_rate': 5.3559821467261774e-05, 'epoch': 8.32}
{'loss': 1.0996, 'grad_norm': 7833092.5, 'learning_rate': 5.342982190059366e-05, 'epoch': 8.33}
{'loss': 1.0992, 'grad_norm': 12591.9267578125, 'learning_rate': 5.329982233392555e-05, 'epoch': 8.33}
{'loss': 1.0987, 'grad_norm': 11292.169921875, 'learning_rate': 5.316982276725743e-05, 'epoch': 8.33}
{'loss': 1.0985, 'grad_norm': 15374.1904296875, 'learning_rate': 5.303982320058933e-05, 'epoch': 8.34}
{'loss': 1.0991, 'grad_norm': 15037.1083984375, 'learning_rate': 5.2909823633921215e-05, 'epoch': 8.34}
{'loss': 1.0992, 'grad_norm': 150155.6875, 'learning_rate': 5.2779824067253106e-05, 'epoch': 8.35}
{'loss': 1.0979, 'grad_norm': 4980.74267578125, 'learning_rate': 5.264982450058499e-05, 'epoch': 8.35}
{'loss': 1.0988, 'grad_norm': 64559.71875, 'learning_rate': 5.251982493391688e-05, 'epoch': 8.35}
{'loss': 1.0988, 'grad_norm': 646695.3125, 'learning_rate': 5.238982536724877e-05, 'epoch': 8.36}
{'loss': 1.0998, 'grad_norm': 137240.640625, 'learning_rate': 5.225982580058066e-05, 'epoch': 8.36}
{'loss': 1.0993, 'grad_norm': 932741.4375, 'learning_rate': 5.212982623391255e-05, 'epoch': 8.37}
{'loss': 1.0996, 'grad_norm': 4219379.5, 'learning_rate': 5.199982666724444e-05, 'epoch': 8.37}
{'loss': 1.1, 'grad_norm': 428822.75, 'learning_rate': 5.186982710057632e-05, 'epoch': 8.37}
{'loss': 1.0987, 'grad_norm': 20412.064453125, 'learning_rate': 5.173982753390822e-05, 'epoch': 8.38}
{'loss': 1.0975, 'grad_norm': 26838.599609375, 'learning_rate': 5.1609827967240105e-05, 'epoch': 8.38}
{'loss': 1.1, 'grad_norm': 205520.65625, 'learning_rate': 5.1479828400571996e-05, 'epoch': 8.39}
{'loss': 1.0988, 'grad_norm': 36067.203125, 'learning_rate': 5.134982883390388e-05, 'epoch': 8.39}
{'loss': 1.1002, 'grad_norm': 123874.1953125, 'learning_rate': 5.121982926723577e-05, 'epoch': 8.4}
{'loss': 1.0985, 'grad_norm': 1139.4061279296875, 'learning_rate': 5.1089829700567655e-05, 'epoch': 8.4}
{'loss': 1.099, 'grad_norm': 141049.640625, 'learning_rate': 5.095983013389955e-05, 'epoch': 8.4}
{'loss': 1.0988, 'grad_norm': 11203.2744140625, 'learning_rate': 5.082983056723144e-05, 'epoch': 8.41}
{'loss': 1.1004, 'grad_norm': 37599.69921875, 'learning_rate': 5.069983100056333e-05, 'epoch': 8.41}
{'loss': 1.0999, 'grad_norm': 58322.671875, 'learning_rate': 5.056983143389521e-05, 'epoch': 8.42}
{'loss': 1.0985, 'grad_norm': 8433.2939453125, 'learning_rate': 5.043983186722711e-05, 'epoch': 8.42}
{'loss': 1.0977, 'grad_norm': 37065.9296875, 'learning_rate': 5.0309832300558994e-05, 'epoch': 8.42}
{'loss': 1.0981, 'grad_norm': 2829.50732421875, 'learning_rate': 5.0179832733890885e-05, 'epoch': 8.43}
{'loss': 1.0997, 'grad_norm': 14553.408203125, 'learning_rate': 5.004983316722277e-05, 'epoch': 8.43}
{'loss': 1.0998, 'grad_norm': 9969025.0, 'learning_rate': 4.991983360055466e-05, 'epoch': 8.44}
{'loss': 1.0987, 'grad_norm': 13952.8876953125, 'learning_rate': 4.9789834033886544e-05, 'epoch': 8.44}
{'loss': 1.1001, 'grad_norm': 1267215.75, 'learning_rate': 4.965983446721844e-05, 'epoch': 8.44}
{'loss': 1.1, 'grad_norm': 1058715.125, 'learning_rate': 4.9529834900550326e-05, 'epoch': 8.45}
{'loss': 1.0984, 'grad_norm': 55719.44921875, 'learning_rate': 4.939983533388222e-05, 'epoch': 8.45}
{'loss': 1.0995, 'grad_norm': 172775.203125, 'learning_rate': 4.92698357672141e-05, 'epoch': 8.46}
{'loss': 1.0993, 'grad_norm': 560993.0625, 'learning_rate': 4.913983620054599e-05, 'epoch': 8.46}
{'loss': 1.0976, 'grad_norm': 3218745.75, 'learning_rate': 4.900983663387788e-05, 'epoch': 8.46}
{'loss': 1.0979, 'grad_norm': 6728.0419921875, 'learning_rate': 4.8879837067209774e-05, 'epoch': 8.47}
{'loss': 1.0986, 'grad_norm': 5705.8662109375, 'learning_rate': 4.874983750054166e-05, 'epoch': 8.47}
{'loss': 1.0988, 'grad_norm': 64933.76953125, 'learning_rate': 4.861983793387355e-05, 'epoch': 8.48}
{'loss': 1.0988, 'grad_norm': 15126.9052734375, 'learning_rate': 4.8489838367205434e-05, 'epoch': 8.48}
{'loss': 1.0988, 'grad_norm': 22031.97265625, 'learning_rate': 4.835983880053733e-05, 'epoch': 8.48}
{'loss': 1.0977, 'grad_norm': 4369.9013671875, 'learning_rate': 4.8229839233869216e-05, 'epoch': 8.49}
{'loss': 1.0989, 'grad_norm': 37201.08203125, 'learning_rate': 4.809983966720111e-05, 'epoch': 8.49}
{'loss': 1.1, 'grad_norm': 1249452.125, 'learning_rate': 4.796984010053299e-05, 'epoch': 8.5}
{'loss': 1.0994, 'grad_norm': 79701.0234375, 'learning_rate': 4.783984053386488e-05, 'epoch': 8.5}
{'loss': 1.0989, 'grad_norm': 17425.740234375, 'learning_rate': 4.7709840967196766e-05, 'epoch': 8.51}
{'loss': 1.0997, 'grad_norm': 38706.05859375, 'learning_rate': 4.7579841400528664e-05, 'epoch': 8.51}
{'loss': 1.0993, 'grad_norm': 1371412.125, 'learning_rate': 4.744984183386055e-05, 'epoch': 8.51}
{'loss': 1.0997, 'grad_norm': 142400.90625, 'learning_rate': 4.731984226719244e-05, 'epoch': 8.52}
{'loss': 1.1002, 'grad_norm': 12339.4111328125, 'learning_rate': 4.718984270052432e-05, 'epoch': 8.52}
{'loss': 1.0994, 'grad_norm': 155206.859375, 'learning_rate': 4.705984313385622e-05, 'epoch': 8.53}
{'loss': 1.0988, 'grad_norm': 535478.875, 'learning_rate': 4.6929843567188105e-05, 'epoch': 8.53}
{'loss': 1.0987, 'grad_norm': 13770.087890625, 'learning_rate': 4.6799844000519996e-05, 'epoch': 8.53}
{'loss': 1.0984, 'grad_norm': 20760.90625, 'learning_rate': 4.666984443385188e-05, 'epoch': 8.54}
{'loss': 1.0985, 'grad_norm': 43058.7890625, 'learning_rate': 4.653984486718377e-05, 'epoch': 8.54}
{'loss': 1.0993, 'grad_norm': 7886364.5, 'learning_rate': 4.6409845300515655e-05, 'epoch': 8.55}
{'loss': 1.0992, 'grad_norm': 53811.1953125, 'learning_rate': 4.627984573384755e-05, 'epoch': 8.55}
{'loss': 1.1008, 'grad_norm': 63924.00390625, 'learning_rate': 4.614984616717944e-05, 'epoch': 8.55}
{'loss': 1.0996, 'grad_norm': 207630.734375, 'learning_rate': 4.601984660051133e-05, 'epoch': 8.56}
{'loss': 1.0979, 'grad_norm': 6829424.5, 'learning_rate': 4.588984703384321e-05, 'epoch': 8.56}
{'loss': 1.0986, 'grad_norm': 33221.875, 'learning_rate': 4.5759847467175103e-05, 'epoch': 8.57}
{'loss': 1.0988, 'grad_norm': 106080.40625, 'learning_rate': 4.5629847900506994e-05, 'epoch': 8.57}
{'loss': 1.1008, 'grad_norm': 66865.3984375, 'learning_rate': 4.5499848333838885e-05, 'epoch': 8.57}
{'loss': 1.1, 'grad_norm': 4522.40869140625, 'learning_rate': 4.536984876717077e-05, 'epoch': 8.58}
{'loss': 1.0985, 'grad_norm': 21015.33203125, 'learning_rate': 4.523984920050266e-05, 'epoch': 8.58}
{'loss': 1.099, 'grad_norm': 13747.9462890625, 'learning_rate': 4.5109849633834545e-05, 'epoch': 8.59}
{'loss': 1.0994, 'grad_norm': 18262.470703125, 'learning_rate': 4.497985006716644e-05, 'epoch': 8.59}
{'loss': 1.1002, 'grad_norm': 3882.982666015625, 'learning_rate': 4.484985050049833e-05, 'epoch': 8.59}
{'loss': 1.0996, 'grad_norm': 46235.01171875, 'learning_rate': 4.471985093383022e-05, 'epoch': 8.6}
{'loss': 1.0982, 'grad_norm': 15260.5498046875, 'learning_rate': 4.45898513671621e-05, 'epoch': 8.6}
{'loss': 1.0982, 'grad_norm': 55951.546875, 'learning_rate': 4.445985180049399e-05, 'epoch': 8.61}
{'loss': 1.1013, 'grad_norm': 569164.375, 'learning_rate': 4.432985223382588e-05, 'epoch': 8.61}
{'loss': 1.1009, 'grad_norm': 89859.453125, 'learning_rate': 4.4199852667157775e-05, 'epoch': 8.62}
{'loss': 1.1012, 'grad_norm': 94661.8671875, 'learning_rate': 4.406985310048966e-05, 'epoch': 8.62}
{'loss': 1.0985, 'grad_norm': 555923.3125, 'learning_rate': 4.393985353382155e-05, 'epoch': 8.62}
{'loss': 1.1009, 'grad_norm': 8554.4052734375, 'learning_rate': 4.3809853967153434e-05, 'epoch': 8.63}
{'loss': 1.0998, 'grad_norm': 708466.875, 'learning_rate': 4.367985440048533e-05, 'epoch': 8.63}
{'loss': 1.0982, 'grad_norm': 58616.05078125, 'learning_rate': 4.3549854833817216e-05, 'epoch': 8.64}
{'loss': 1.1006, 'grad_norm': 5824.9794921875, 'learning_rate': 4.341985526714911e-05, 'epoch': 8.64}
{'loss': 1.0991, 'grad_norm': 274335.46875, 'learning_rate': 4.328985570048099e-05, 'epoch': 8.64}
{'loss': 1.1, 'grad_norm': 960454.9375, 'learning_rate': 4.315985613381288e-05, 'epoch': 8.65}
{'loss': 1.0992, 'grad_norm': 902425.875, 'learning_rate': 4.3029856567144766e-05, 'epoch': 8.65}
{'loss': 1.0985, 'grad_norm': 3902896.75, 'learning_rate': 4.2899857000476664e-05, 'epoch': 8.66}
{'loss': 1.1005, 'grad_norm': 70000.734375, 'learning_rate': 4.276985743380855e-05, 'epoch': 8.66}
{'loss': 1.0992, 'grad_norm': 2700083.75, 'learning_rate': 4.263985786714044e-05, 'epoch': 8.66}
{'loss': 1.097, 'grad_norm': 30357.7578125, 'learning_rate': 4.2509858300472323e-05, 'epoch': 8.67}
{'loss': 1.0988, 'grad_norm': 37076.80859375, 'learning_rate': 4.2379858733804214e-05, 'epoch': 8.67}
{'loss': 1.1001, 'grad_norm': 604768.9375, 'learning_rate': 4.22498591671361e-05, 'epoch': 8.68}
{'loss': 1.1001, 'grad_norm': 46026.5625, 'learning_rate': 4.2119859600467996e-05, 'epoch': 8.68}
{'loss': 1.0999, 'grad_norm': 19269.75390625, 'learning_rate': 4.198986003379988e-05, 'epoch': 8.68}
{'loss': 1.0988, 'grad_norm': 1940.130859375, 'learning_rate': 4.185986046713177e-05, 'epoch': 8.69}
{'loss': 1.0996, 'grad_norm': 7173.81396484375, 'learning_rate': 4.1729860900463656e-05, 'epoch': 8.69}
{'loss': 1.1008, 'grad_norm': 115358.8984375, 'learning_rate': 4.1599861333795554e-05, 'epoch': 8.7}
{'loss': 1.0977, 'grad_norm': 45602.421875, 'learning_rate': 4.146986176712744e-05, 'epoch': 8.7}
{'loss': 1.0995, 'grad_norm': 64931.33203125, 'learning_rate': 4.133986220045933e-05, 'epoch': 8.7}
{'loss': 1.0992, 'grad_norm': 17096.07421875, 'learning_rate': 4.120986263379121e-05, 'epoch': 8.71}
{'loss': 1.1023, 'grad_norm': 2903.41650390625, 'learning_rate': 4.1079863067123104e-05, 'epoch': 8.71}
{'loss': 1.1022, 'grad_norm': 45963.86328125, 'learning_rate': 4.094986350045499e-05, 'epoch': 8.72}
{'loss': 1.0985, 'grad_norm': 33327.921875, 'learning_rate': 4.0819863933786886e-05, 'epoch': 8.72}
{'loss': 1.0974, 'grad_norm': 16468.103515625, 'learning_rate': 4.068986436711878e-05, 'epoch': 8.73}
{'loss': 1.0992, 'grad_norm': 11855.5556640625, 'learning_rate': 4.055986480045066e-05, 'epoch': 8.73}
{'loss': 1.0982, 'grad_norm': 60222.51171875, 'learning_rate': 4.042986523378255e-05, 'epoch': 8.73}
{'loss': 1.0982, 'grad_norm': 70763.2734375, 'learning_rate': 4.0299865667114436e-05, 'epoch': 8.74}
{'loss': 1.1004, 'grad_norm': 228525.265625, 'learning_rate': 4.0169866100446334e-05, 'epoch': 8.74}
{'loss': 1.0987, 'grad_norm': 179792.1875, 'learning_rate': 4.003986653377822e-05, 'epoch': 8.75}
{'loss': 1.0999, 'grad_norm': 89613.2578125, 'learning_rate': 3.990986696711011e-05, 'epoch': 8.75}
{'loss': 1.1002, 'grad_norm': 58114.80078125, 'learning_rate': 3.977986740044199e-05, 'epoch': 8.75}
{'loss': 1.0994, 'grad_norm': 64282.4609375, 'learning_rate': 3.964986783377389e-05, 'epoch': 8.76}
{'loss': 1.0992, 'grad_norm': 532479.6875, 'learning_rate': 3.9519868267105775e-05, 'epoch': 8.76}
{'loss': 1.0986, 'grad_norm': 60492.21484375, 'learning_rate': 3.9389868700437666e-05, 'epoch': 8.77}
{'loss': 1.1, 'grad_norm': 23182.763671875, 'learning_rate': 3.925986913376955e-05, 'epoch': 8.77}
{'loss': 1.1016, 'grad_norm': 9863.97265625, 'learning_rate': 3.912986956710144e-05, 'epoch': 8.77}
{'loss': 1.0993, 'grad_norm': 113172.1640625, 'learning_rate': 3.8999870000433325e-05, 'epoch': 8.78}
{'loss': 1.0984, 'grad_norm': 23220.046875, 'learning_rate': 3.886987043376522e-05, 'epoch': 8.78}
{'loss': 1.0995, 'grad_norm': 585939.125, 'learning_rate': 3.873987086709711e-05, 'epoch': 8.79}
{'loss': 1.0984, 'grad_norm': 39536.21875, 'learning_rate': 3.8609871300429e-05, 'epoch': 8.79}
{'loss': 1.0986, 'grad_norm': 1667834.625, 'learning_rate': 3.847987173376088e-05, 'epoch': 8.79}
{'loss': 1.0995, 'grad_norm': 224822.328125, 'learning_rate': 3.8349872167092774e-05, 'epoch': 8.8}
{'loss': 1.0993, 'grad_norm': 28631.169921875, 'learning_rate': 3.8219872600424665e-05, 'epoch': 8.8}
{'loss': 1.1005, 'grad_norm': 117022.8046875, 'learning_rate': 3.8089873033756555e-05, 'epoch': 8.81}
{'loss': 1.098, 'grad_norm': 5738.53466796875, 'learning_rate': 3.795987346708844e-05, 'epoch': 8.81}
{'loss': 1.0974, 'grad_norm': 20803.77734375, 'learning_rate': 3.782987390042033e-05, 'epoch': 8.81}
{'loss': 1.0987, 'grad_norm': 20721.021484375, 'learning_rate': 3.7699874333752215e-05, 'epoch': 8.82}
{'loss': 1.0999, 'grad_norm': 25995.69140625, 'learning_rate': 3.756987476708411e-05, 'epoch': 8.82}
{'loss': 1.0993, 'grad_norm': 2633197.75, 'learning_rate': 3.7439875200416e-05, 'epoch': 8.83}
{'loss': 1.1009, 'grad_norm': 8548.84375, 'learning_rate': 3.730987563374788e-05, 'epoch': 8.83}
{'loss': 1.0983, 'grad_norm': 359010.5625, 'learning_rate': 3.717987606707977e-05, 'epoch': 8.84}
{'loss': 1.1013, 'grad_norm': 251896.296875, 'learning_rate': 3.704987650041166e-05, 'epoch': 8.84}
{'loss': 1.0982, 'grad_norm': 17036.90625, 'learning_rate': 3.691987693374355e-05, 'epoch': 8.84}
{'loss': 1.1007, 'grad_norm': 143979.1875, 'learning_rate': 3.678987736707544e-05, 'epoch': 8.85}
{'loss': 1.099, 'grad_norm': 9548521.0, 'learning_rate': 3.665987780040733e-05, 'epoch': 8.85}
{'loss': 1.1009, 'grad_norm': 82027.1640625, 'learning_rate': 3.652987823373922e-05, 'epoch': 8.86}
{'loss': 1.0989, 'grad_norm': 22489.72265625, 'learning_rate': 3.6399878667071104e-05, 'epoch': 8.86}
{'loss': 1.0989, 'grad_norm': 363259.78125, 'learning_rate': 3.6269879100402995e-05, 'epoch': 8.86}
{'loss': 1.0987, 'grad_norm': 17203.4375, 'learning_rate': 3.6139879533734886e-05, 'epoch': 8.87}
{'loss': 1.0992, 'grad_norm': 13636.5654296875, 'learning_rate': 3.600987996706677e-05, 'epoch': 8.87}
{'loss': 1.1009, 'grad_norm': 95111.6953125, 'learning_rate': 3.587988040039866e-05, 'epoch': 8.88}
{'loss': 1.0986, 'grad_norm': 31937.8125, 'learning_rate': 3.574988083373055e-05, 'epoch': 8.88}
{'loss': 1.0983, 'grad_norm': 76173584.0, 'learning_rate': 3.5619881267062437e-05, 'epoch': 8.88}
{'loss': 1.0999, 'grad_norm': 33436.8828125, 'learning_rate': 3.548988170039433e-05, 'epoch': 8.89}
{'loss': 1.0999, 'grad_norm': 1438.50732421875, 'learning_rate': 3.535988213372622e-05, 'epoch': 8.89}
{'loss': 1.0986, 'grad_norm': 461899.5, 'learning_rate': 3.52298825670581e-05, 'epoch': 8.9}
{'loss': 1.0986, 'grad_norm': 89478.8828125, 'learning_rate': 3.5099883000389994e-05, 'epoch': 8.9}
{'loss': 1.0988, 'grad_norm': 14108.923828125, 'learning_rate': 3.4969883433721885e-05, 'epoch': 8.9}
{'loss': 1.0994, 'grad_norm': 106930.0, 'learning_rate': 3.4839883867053776e-05, 'epoch': 8.91}
{'loss': 1.0984, 'grad_norm': 27650.46484375, 'learning_rate': 3.470988430038566e-05, 'epoch': 8.91}
{'loss': 1.0989, 'grad_norm': 81779.4765625, 'learning_rate': 3.457988473371755e-05, 'epoch': 8.92}
{'loss': 1.0979, 'grad_norm': 2628.671630859375, 'learning_rate': 3.444988516704944e-05, 'epoch': 8.92}
{'loss': 1.0993, 'grad_norm': 262709.5, 'learning_rate': 3.4319885600381326e-05, 'epoch': 8.92}
{'loss': 1.0989, 'grad_norm': 5723.8076171875, 'learning_rate': 3.418988603371322e-05, 'epoch': 8.93}
{'loss': 1.0986, 'grad_norm': 156687.75, 'learning_rate': 3.405988646704511e-05, 'epoch': 8.93}
{'loss': 1.099, 'grad_norm': 10568.9580078125, 'learning_rate': 3.392988690037699e-05, 'epoch': 8.94}
{'loss': 1.0994, 'grad_norm': 5745664.5, 'learning_rate': 3.379988733370888e-05, 'epoch': 8.94}
{'loss': 1.0993, 'grad_norm': 27165.26953125, 'learning_rate': 3.3669887767040774e-05, 'epoch': 8.95}
{'loss': 1.0995, 'grad_norm': 902283.4375, 'learning_rate': 3.353988820037266e-05, 'epoch': 8.95}
{'loss': 1.1, 'grad_norm': 93736.7265625, 'learning_rate': 3.340988863370455e-05, 'epoch': 8.95}
{'loss': 1.0984, 'grad_norm': 16819.73046875, 'learning_rate': 3.327988906703644e-05, 'epoch': 8.96}
{'loss': 1.0995, 'grad_norm': 56657.71875, 'learning_rate': 3.3149889500368324e-05, 'epoch': 8.96}
{'loss': 1.0996, 'grad_norm': 232758.09375, 'learning_rate': 3.3019889933700215e-05, 'epoch': 8.97}
{'loss': 1.099, 'grad_norm': 3370174.75, 'learning_rate': 3.2889890367032106e-05, 'epoch': 8.97}
{'loss': 1.0982, 'grad_norm': 53453.9765625, 'learning_rate': 3.2759890800364e-05, 'epoch': 8.97}
{'loss': 1.0999, 'grad_norm': 37031.68359375, 'learning_rate': 3.262989123369588e-05, 'epoch': 8.98}
{'loss': 1.1001, 'grad_norm': 75827.71875, 'learning_rate': 3.249989166702777e-05, 'epoch': 8.98}
{'loss': 1.0992, 'grad_norm': 56771.30078125, 'learning_rate': 3.236989210035966e-05, 'epoch': 8.99}
{'loss': 1.0992, 'grad_norm': 91138.046875, 'learning_rate': 3.223989253369155e-05, 'epoch': 8.99}
{'loss': 1.0999, 'grad_norm': 320174.03125, 'learning_rate': 3.210989296702344e-05, 'epoch': 8.99}
{'loss': 1.0995, 'grad_norm': 8480873.0, 'learning_rate': 3.197989340035533e-05, 'epoch': 9.0}
{'eval_loss': 1.1027331352233887, 'eval_accuracy': 0.321650534895568, 'eval_runtime': 5.6127, 'eval_samples_per_second': 1748.719, 'eval_steps_per_second': 11.046, 'epoch': 9.0}
{'loss': 1.0986, 'grad_norm': 7692.22412109375, 'learning_rate': 3.1849893833687214e-05, 'epoch': 9.0}
{'loss': 1.0999, 'grad_norm': 104773960.0, 'learning_rate': 3.1719894267019105e-05, 'epoch': 9.01}
{'loss': 1.0996, 'grad_norm': 53080.25390625, 'learning_rate': 3.1589894700350996e-05, 'epoch': 9.01}
{'loss': 1.1003, 'grad_norm': 70197.1640625, 'learning_rate': 3.145989513368288e-05, 'epoch': 9.01}
{'loss': 1.0979, 'grad_norm': 1099234.875, 'learning_rate': 3.132989556701477e-05, 'epoch': 9.02}
{'loss': 1.0995, 'grad_norm': 19915.00390625, 'learning_rate': 3.119989600034666e-05, 'epoch': 9.02}
{'loss': 1.0994, 'grad_norm': 68304.9609375, 'learning_rate': 3.106989643367855e-05, 'epoch': 9.03}
{'loss': 1.1003, 'grad_norm': 250607.15625, 'learning_rate': 3.093989686701044e-05, 'epoch': 9.03}
{'loss': 1.0997, 'grad_norm': 10765658.0, 'learning_rate': 3.080989730034233e-05, 'epoch': 9.03}
{'loss': 1.0982, 'grad_norm': 186763.09375, 'learning_rate': 3.067989773367422e-05, 'epoch': 9.04}
{'loss': 1.0991, 'grad_norm': 7359.23046875, 'learning_rate': 3.054989816700611e-05, 'epoch': 9.04}
{'loss': 1.1004, 'grad_norm': 511829.75, 'learning_rate': 3.0419898600337997e-05, 'epoch': 9.05}
{'loss': 1.1012, 'grad_norm': 16079.36328125, 'learning_rate': 3.028989903366989e-05, 'epoch': 9.05}
{'loss': 1.101, 'grad_norm': 42057.41015625, 'learning_rate': 3.0159899467001776e-05, 'epoch': 9.05}
{'loss': 1.098, 'grad_norm': 228829.578125, 'learning_rate': 3.0029899900333664e-05, 'epoch': 9.06}
{'loss': 1.0972, 'grad_norm': 140886.578125, 'learning_rate': 2.9899900333665555e-05, 'epoch': 9.06}
{'loss': 1.0995, 'grad_norm': 41821.91796875, 'learning_rate': 2.9769900766997442e-05, 'epoch': 9.07}
{'loss': 1.0988, 'grad_norm': 22662.509765625, 'learning_rate': 2.9639901200329333e-05, 'epoch': 9.07}
{'loss': 1.0989, 'grad_norm': 1020943.6875, 'learning_rate': 2.950990163366122e-05, 'epoch': 9.08}
{'loss': 1.0986, 'grad_norm': 848910.25, 'learning_rate': 2.9379902066993108e-05, 'epoch': 9.08}
{'loss': 1.0995, 'grad_norm': 3798.611083984375, 'learning_rate': 2.9249902500325e-05, 'epoch': 9.08}
{'loss': 1.1007, 'grad_norm': 9753.751953125, 'learning_rate': 2.9119902933656887e-05, 'epoch': 9.09}
{'loss': 1.0994, 'grad_norm': 10382.3818359375, 'learning_rate': 2.8989903366988774e-05, 'epoch': 9.09}
{'loss': 1.0989, 'grad_norm': 23136.0390625, 'learning_rate': 2.8859903800320665e-05, 'epoch': 9.1}
{'loss': 1.0985, 'grad_norm': 7889.07421875, 'learning_rate': 2.8729904233652553e-05, 'epoch': 9.1}
{'loss': 1.0993, 'grad_norm': 6915489.0, 'learning_rate': 2.8599904666984444e-05, 'epoch': 9.1}
{'loss': 1.0989, 'grad_norm': 6851.52587890625, 'learning_rate': 2.846990510031633e-05, 'epoch': 9.11}
{'loss': 1.0998, 'grad_norm': 103748.7734375, 'learning_rate': 2.833990553364822e-05, 'epoch': 9.11}
{'loss': 1.0994, 'grad_norm': 22565.4140625, 'learning_rate': 2.820990596698011e-05, 'epoch': 9.12}
{'loss': 1.0997, 'grad_norm': 371823.625, 'learning_rate': 2.8079906400311998e-05, 'epoch': 9.12}
{'loss': 1.098, 'grad_norm': 3476.859619140625, 'learning_rate': 2.794990683364389e-05, 'epoch': 9.12}
{'loss': 1.0996, 'grad_norm': 33434.828125, 'learning_rate': 2.7819907266975776e-05, 'epoch': 9.13}
{'loss': 1.0999, 'grad_norm': 228747.671875, 'learning_rate': 2.7689907700307664e-05, 'epoch': 9.13}
{'loss': 1.0981, 'grad_norm': 178128.59375, 'learning_rate': 2.7559908133639555e-05, 'epoch': 9.14}
{'loss': 1.0993, 'grad_norm': 175705.75, 'learning_rate': 2.7429908566971442e-05, 'epoch': 9.14}
{'loss': 1.0995, 'grad_norm': 5985.615234375, 'learning_rate': 2.729990900030333e-05, 'epoch': 9.14}
{'loss': 1.0994, 'grad_norm': 6622.5703125, 'learning_rate': 2.716990943363522e-05, 'epoch': 9.15}
{'loss': 1.0992, 'grad_norm': 3664.32666015625, 'learning_rate': 2.703990986696711e-05, 'epoch': 9.15}
{'loss': 1.0993, 'grad_norm': 3690.9931640625, 'learning_rate': 2.6909910300299e-05, 'epoch': 9.16}
{'loss': 1.098, 'grad_norm': 239984.5, 'learning_rate': 2.6779910733630887e-05, 'epoch': 9.16}
{'loss': 1.0993, 'grad_norm': 63420.984375, 'learning_rate': 2.6649911166962775e-05, 'epoch': 9.16}
{'loss': 1.0994, 'grad_norm': 291213.3125, 'learning_rate': 2.6519911600294666e-05, 'epoch': 9.17}
{'loss': 1.1003, 'grad_norm': 242524.953125, 'learning_rate': 2.6389912033626553e-05, 'epoch': 9.17}
{'loss': 1.0982, 'grad_norm': 19221.4609375, 'learning_rate': 2.625991246695844e-05, 'epoch': 9.18}
{'loss': 1.1002, 'grad_norm': 318466.25, 'learning_rate': 2.612991290029033e-05, 'epoch': 9.18}
{'loss': 1.0995, 'grad_norm': 132878.28125, 'learning_rate': 2.599991333362222e-05, 'epoch': 9.19}
{'loss': 1.0991, 'grad_norm': 260462.78125, 'learning_rate': 2.586991376695411e-05, 'epoch': 9.19}
{'loss': 1.0989, 'grad_norm': 5398.96533203125, 'learning_rate': 2.5739914200285998e-05, 'epoch': 9.19}
{'loss': 1.0997, 'grad_norm': 27433.625, 'learning_rate': 2.5609914633617885e-05, 'epoch': 9.2}
{'loss': 1.0992, 'grad_norm': 3918358.0, 'learning_rate': 2.5479915066949776e-05, 'epoch': 9.2}
{'loss': 1.1007, 'grad_norm': 313432.65625, 'learning_rate': 2.5349915500281664e-05, 'epoch': 9.21}
{'loss': 1.1, 'grad_norm': 48332.58203125, 'learning_rate': 2.5219915933613555e-05, 'epoch': 9.21}
{'loss': 1.0989, 'grad_norm': 573982.875, 'learning_rate': 2.5089916366945442e-05, 'epoch': 9.21}
{'loss': 1.0998, 'grad_norm': 2293.029541015625, 'learning_rate': 2.495991680027733e-05, 'epoch': 9.22}
{'loss': 1.0999, 'grad_norm': 6115.66796875, 'learning_rate': 2.482991723360922e-05, 'epoch': 9.22}
{'loss': 1.0996, 'grad_norm': 140633.3125, 'learning_rate': 2.469991766694111e-05, 'epoch': 9.23}
{'loss': 1.0998, 'grad_norm': 4366792.5, 'learning_rate': 2.4569918100272996e-05, 'epoch': 9.23}
{'loss': 1.0983, 'grad_norm': 100767.0703125, 'learning_rate': 2.4439918533604887e-05, 'epoch': 9.23}
{'loss': 1.0985, 'grad_norm': 10168.0498046875, 'learning_rate': 2.4309918966936775e-05, 'epoch': 9.24}
{'loss': 1.0998, 'grad_norm': 1520.0562744140625, 'learning_rate': 2.4179919400268666e-05, 'epoch': 9.24}
{'loss': 1.1004, 'grad_norm': 91780.34375, 'learning_rate': 2.4049919833600553e-05, 'epoch': 9.25}
{'loss': 1.0999, 'grad_norm': 44814424.0, 'learning_rate': 2.391992026693244e-05, 'epoch': 9.25}
{'loss': 1.0985, 'grad_norm': 83131.2734375, 'learning_rate': 2.3789920700264332e-05, 'epoch': 9.25}
{'loss': 1.1001, 'grad_norm': 66708.8359375, 'learning_rate': 2.365992113359622e-05, 'epoch': 9.26}
{'loss': 1.0998, 'grad_norm': 303944.875, 'learning_rate': 2.352992156692811e-05, 'epoch': 9.26}
{'loss': 1.0998, 'grad_norm': 75808.796875, 'learning_rate': 2.3399922000259998e-05, 'epoch': 9.27}
{'loss': 1.0991, 'grad_norm': 92569.734375, 'learning_rate': 2.3269922433591886e-05, 'epoch': 9.27}
{'loss': 1.0997, 'grad_norm': 16088.1416015625, 'learning_rate': 2.3139922866923777e-05, 'epoch': 9.27}
{'loss': 1.098, 'grad_norm': 9141.8251953125, 'learning_rate': 2.3009923300255664e-05, 'epoch': 9.28}
{'loss': 1.0972, 'grad_norm': 55305.80859375, 'learning_rate': 2.2879923733587552e-05, 'epoch': 9.28}
{'loss': 1.1004, 'grad_norm': 31436.830078125, 'learning_rate': 2.2749924166919443e-05, 'epoch': 9.29}
{'loss': 1.1009, 'grad_norm': 22990.5859375, 'learning_rate': 2.261992460025133e-05, 'epoch': 9.29}
{'loss': 1.0988, 'grad_norm': 10366.490234375, 'learning_rate': 2.248992503358322e-05, 'epoch': 9.3}
{'loss': 1.099, 'grad_norm': 76818.171875, 'learning_rate': 2.235992546691511e-05, 'epoch': 9.3}
{'loss': 1.0999, 'grad_norm': 194809.671875, 'learning_rate': 2.2229925900246996e-05, 'epoch': 9.3}
{'loss': 1.1007, 'grad_norm': 123780.515625, 'learning_rate': 2.2099926333578887e-05, 'epoch': 9.31}
{'loss': 1.0991, 'grad_norm': 5995.61328125, 'learning_rate': 2.1969926766910775e-05, 'epoch': 9.31}
{'loss': 1.1002, 'grad_norm': 3994.228271484375, 'learning_rate': 2.1839927200242666e-05, 'epoch': 9.32}
{'loss': 1.0988, 'grad_norm': 3515735.25, 'learning_rate': 2.1709927633574554e-05, 'epoch': 9.32}
{'loss': 1.0984, 'grad_norm': 334693.3125, 'learning_rate': 2.157992806690644e-05, 'epoch': 9.32}
{'loss': 1.098, 'grad_norm': 34128.01171875, 'learning_rate': 2.1449928500238332e-05, 'epoch': 9.33}
{'loss': 1.0989, 'grad_norm': 261097.09375, 'learning_rate': 2.131992893357022e-05, 'epoch': 9.33}
{'loss': 1.0997, 'grad_norm': 15042.521484375, 'learning_rate': 2.1189929366902107e-05, 'epoch': 9.34}
{'loss': 1.0989, 'grad_norm': 5155.2744140625, 'learning_rate': 2.1059929800233998e-05, 'epoch': 9.34}
{'loss': 1.1, 'grad_norm': 192047.515625, 'learning_rate': 2.0929930233565886e-05, 'epoch': 9.34}
{'loss': 1.0979, 'grad_norm': 19635.236328125, 'learning_rate': 2.0799930666897777e-05, 'epoch': 9.35}
{'loss': 1.0988, 'grad_norm': 115558.203125, 'learning_rate': 2.0669931100229664e-05, 'epoch': 9.35}
{'loss': 1.0985, 'grad_norm': 1319.8653564453125, 'learning_rate': 2.0539931533561552e-05, 'epoch': 9.36}
{'loss': 1.0989, 'grad_norm': 3554.13916015625, 'learning_rate': 2.0409931966893443e-05, 'epoch': 9.36}
{'loss': 1.0989, 'grad_norm': 4510.59423828125, 'learning_rate': 2.027993240022533e-05, 'epoch': 9.36}
{'loss': 1.101, 'grad_norm': 92464.4296875, 'learning_rate': 2.0149932833557218e-05, 'epoch': 9.37}
{'loss': 1.0969, 'grad_norm': 272502.4375, 'learning_rate': 2.001993326688911e-05, 'epoch': 9.37}
{'loss': 1.0994, 'grad_norm': 686827.875, 'learning_rate': 1.9889933700220997e-05, 'epoch': 9.38}
{'loss': 1.0998, 'grad_norm': 8734.138671875, 'learning_rate': 1.9759934133552888e-05, 'epoch': 9.38}
{'loss': 1.0993, 'grad_norm': 4394537.0, 'learning_rate': 1.9629934566884775e-05, 'epoch': 9.38}
{'loss': 1.1002, 'grad_norm': 34529.36328125, 'learning_rate': 1.9499935000216663e-05, 'epoch': 9.39}
{'loss': 1.1001, 'grad_norm': 12675.9765625, 'learning_rate': 1.9369935433548554e-05, 'epoch': 9.39}
{'loss': 1.0992, 'grad_norm': 48896.3125, 'learning_rate': 1.923993586688044e-05, 'epoch': 9.4}
{'loss': 1.0998, 'grad_norm': 27226.06640625, 'learning_rate': 1.9109936300212332e-05, 'epoch': 9.4}
{'loss': 1.0989, 'grad_norm': 1779.7437744140625, 'learning_rate': 1.897993673354422e-05, 'epoch': 9.41}
{'loss': 1.0994, 'grad_norm': 141212.453125, 'learning_rate': 1.8849937166876107e-05, 'epoch': 9.41}
{'loss': 1.0986, 'grad_norm': 13686.4345703125, 'learning_rate': 1.8719937600208e-05, 'epoch': 9.41}
{'loss': 1.0985, 'grad_norm': 6838.29443359375, 'learning_rate': 1.8589938033539886e-05, 'epoch': 9.42}
{'loss': 1.0994, 'grad_norm': 158117.375, 'learning_rate': 1.8459938466871774e-05, 'epoch': 9.42}
{'loss': 1.0988, 'grad_norm': 6213.18359375, 'learning_rate': 1.8329938900203665e-05, 'epoch': 9.43}
{'loss': 1.0992, 'grad_norm': 6779.46337890625, 'learning_rate': 1.8199939333535552e-05, 'epoch': 9.43}
{'loss': 1.1, 'grad_norm': 8498.6494140625, 'learning_rate': 1.8069939766867443e-05, 'epoch': 9.43}
{'loss': 1.0972, 'grad_norm': 13987.681640625, 'learning_rate': 1.793994020019933e-05, 'epoch': 9.44}
{'loss': 1.0992, 'grad_norm': 8778.869140625, 'learning_rate': 1.7809940633531218e-05, 'epoch': 9.44}
{'loss': 1.0986, 'grad_norm': 8664.5458984375, 'learning_rate': 1.767994106686311e-05, 'epoch': 9.45}
{'loss': 1.0985, 'grad_norm': 100181.3203125, 'learning_rate': 1.7549941500194997e-05, 'epoch': 9.45}
{'loss': 1.099, 'grad_norm': 2913509.0, 'learning_rate': 1.7419941933526888e-05, 'epoch': 9.45}
{'loss': 1.0987, 'grad_norm': 19098.68359375, 'learning_rate': 1.7289942366858775e-05, 'epoch': 9.46}
{'loss': 1.0976, 'grad_norm': 25505.29296875, 'learning_rate': 1.7159942800190663e-05, 'epoch': 9.46}
{'loss': 1.0992, 'grad_norm': 158999.6875, 'learning_rate': 1.7029943233522554e-05, 'epoch': 9.47}
{'loss': 1.0991, 'grad_norm': 1402974.375, 'learning_rate': 1.689994366685444e-05, 'epoch': 9.47}
{'loss': 1.0994, 'grad_norm': 147237.125, 'learning_rate': 1.676994410018633e-05, 'epoch': 9.47}
{'loss': 1.098, 'grad_norm': 23859.16796875, 'learning_rate': 1.663994453351822e-05, 'epoch': 9.48}
{'loss': 1.0992, 'grad_norm': 33035.35546875, 'learning_rate': 1.6509944966850108e-05, 'epoch': 9.48}
{'loss': 1.0998, 'grad_norm': 3650430.5, 'learning_rate': 1.6379945400182e-05, 'epoch': 9.49}
{'loss': 1.0984, 'grad_norm': 1445.4501953125, 'learning_rate': 1.6249945833513886e-05, 'epoch': 9.49}
{'loss': 1.0983, 'grad_norm': 887568.5, 'learning_rate': 1.6119946266845774e-05, 'epoch': 9.49}
{'loss': 1.0994, 'grad_norm': 9878.884765625, 'learning_rate': 1.5989946700177665e-05, 'epoch': 9.5}
{'loss': 1.0994, 'grad_norm': 2001.5882568359375, 'learning_rate': 1.5859947133509552e-05, 'epoch': 9.5}
{'loss': 1.0998, 'grad_norm': 157784.171875, 'learning_rate': 1.572994756684144e-05, 'epoch': 9.51}
{'loss': 1.1006, 'grad_norm': 184516.578125, 'learning_rate': 1.559994800017333e-05, 'epoch': 9.51}
{'loss': 1.101, 'grad_norm': 743453.375, 'learning_rate': 1.546994843350522e-05, 'epoch': 9.52}
{'loss': 1.1029, 'grad_norm': 23752.0, 'learning_rate': 1.533994886683711e-05, 'epoch': 9.52}
{'loss': 1.0991, 'grad_norm': 3913.041015625, 'learning_rate': 1.5209949300168999e-05, 'epoch': 9.52}
{'loss': 1.1001, 'grad_norm': 537052.25, 'learning_rate': 1.5079949733500888e-05, 'epoch': 9.53}
{'loss': 1.1015, 'grad_norm': 555514.75, 'learning_rate': 1.4949950166832777e-05, 'epoch': 9.53}
{'loss': 1.0996, 'grad_norm': 6180.998046875, 'learning_rate': 1.4819950600164667e-05, 'epoch': 9.54}
{'loss': 1.097, 'grad_norm': 1428454.75, 'learning_rate': 1.4689951033496554e-05, 'epoch': 9.54}
{'loss': 1.1003, 'grad_norm': 2667.256591796875, 'learning_rate': 1.4559951466828443e-05, 'epoch': 9.54}
{'loss': 1.1011, 'grad_norm': 133668.1875, 'learning_rate': 1.4429951900160333e-05, 'epoch': 9.55}
{'loss': 1.1006, 'grad_norm': 14089.9794921875, 'learning_rate': 1.4299952333492222e-05, 'epoch': 9.55}
{'loss': 1.0983, 'grad_norm': 9301.5537109375, 'learning_rate': 1.416995276682411e-05, 'epoch': 9.56}
{'loss': 1.1, 'grad_norm': 96599.3203125, 'learning_rate': 1.4039953200155999e-05, 'epoch': 9.56}
{'loss': 1.0981, 'grad_norm': 43929.7109375, 'learning_rate': 1.3909953633487888e-05, 'epoch': 9.56}
{'loss': 1.0997, 'grad_norm': 6817.98779296875, 'learning_rate': 1.3779954066819777e-05, 'epoch': 9.57}
{'loss': 1.0991, 'grad_norm': 8369.2490234375, 'learning_rate': 1.3649954500151665e-05, 'epoch': 9.57}
{'loss': 1.0991, 'grad_norm': 1217.2520751953125, 'learning_rate': 1.3519954933483554e-05, 'epoch': 9.58}
{'loss': 1.0982, 'grad_norm': 13164.806640625, 'learning_rate': 1.3389955366815443e-05, 'epoch': 9.58}
{'loss': 1.1004, 'grad_norm': 22943.203125, 'learning_rate': 1.3259955800147333e-05, 'epoch': 9.58}
{'loss': 1.098, 'grad_norm': 24112.341796875, 'learning_rate': 1.312995623347922e-05, 'epoch': 9.59}
{'loss': 1.0988, 'grad_norm': 255442.140625, 'learning_rate': 1.299995666681111e-05, 'epoch': 9.59}
{'loss': 1.099, 'grad_norm': 2742269.0, 'learning_rate': 1.2869957100142999e-05, 'epoch': 9.6}
{'loss': 1.1011, 'grad_norm': 19693.42578125, 'learning_rate': 1.2739957533474888e-05, 'epoch': 9.6}
{'loss': 1.099, 'grad_norm': 5939.44970703125, 'learning_rate': 1.2609957966806777e-05, 'epoch': 9.6}
{'loss': 1.1004, 'grad_norm': 70607.671875, 'learning_rate': 1.2479958400138665e-05, 'epoch': 9.61}
{'loss': 1.0989, 'grad_norm': 51310.9921875, 'learning_rate': 1.2349958833470554e-05, 'epoch': 9.61}
{'loss': 1.099, 'grad_norm': 495324.78125, 'learning_rate': 1.2219959266802444e-05, 'epoch': 9.62}
{'loss': 1.0988, 'grad_norm': 68630.953125, 'learning_rate': 1.2089959700134333e-05, 'epoch': 9.62}
{'loss': 1.0999, 'grad_norm': 23657.26953125, 'learning_rate': 1.195996013346622e-05, 'epoch': 9.63}
{'loss': 1.0996, 'grad_norm': 17218.951171875, 'learning_rate': 1.182996056679811e-05, 'epoch': 9.63}
{'loss': 1.0985, 'grad_norm': 2992.251953125, 'learning_rate': 1.1699961000129999e-05, 'epoch': 9.63}
{'loss': 1.1003, 'grad_norm': 6113.8408203125, 'learning_rate': 1.1569961433461888e-05, 'epoch': 9.64}
{'loss': 1.0986, 'grad_norm': 7517.9619140625, 'learning_rate': 1.1439961866793776e-05, 'epoch': 9.64}
{'loss': 1.1, 'grad_norm': 2857745.75, 'learning_rate': 1.1309962300125665e-05, 'epoch': 9.65}
{'loss': 1.0989, 'grad_norm': 1744.0914306640625, 'learning_rate': 1.1179962733457554e-05, 'epoch': 9.65}
{'loss': 1.0996, 'grad_norm': 36506.37109375, 'learning_rate': 1.1049963166789444e-05, 'epoch': 9.65}
{'loss': 1.0994, 'grad_norm': 8511.6015625, 'learning_rate': 1.0919963600121333e-05, 'epoch': 9.66}
{'loss': 1.0992, 'grad_norm': 57048.30078125, 'learning_rate': 1.078996403345322e-05, 'epoch': 9.66}
{'loss': 1.0997, 'grad_norm': 53310.64453125, 'learning_rate': 1.065996446678511e-05, 'epoch': 9.67}
{'loss': 1.0986, 'grad_norm': 7880.5751953125, 'learning_rate': 1.0529964900116999e-05, 'epoch': 9.67}
{'loss': 1.0986, 'grad_norm': 114485.6640625, 'learning_rate': 1.0399965333448888e-05, 'epoch': 9.67}
{'loss': 1.0984, 'grad_norm': 32211.6171875, 'learning_rate': 1.0269965766780776e-05, 'epoch': 9.68}
{'loss': 1.0991, 'grad_norm': 10323400.0, 'learning_rate': 1.0139966200112665e-05, 'epoch': 9.68}
{'loss': 1.0982, 'grad_norm': 69866.09375, 'learning_rate': 1.0009966633444555e-05, 'epoch': 9.69}
{'loss': 1.0996, 'grad_norm': 24120.908203125, 'learning_rate': 9.879967066776444e-06, 'epoch': 9.69}
{'loss': 1.0985, 'grad_norm': 3969.017333984375, 'learning_rate': 9.749967500108331e-06, 'epoch': 9.69}
{'loss': 1.0982, 'grad_norm': 4008.508056640625, 'learning_rate': 9.61996793344022e-06, 'epoch': 9.7}
{'loss': 1.0989, 'grad_norm': 1100284.375, 'learning_rate': 9.48996836677211e-06, 'epoch': 9.7}
{'loss': 1.099, 'grad_norm': 55632.49609375, 'learning_rate': 9.359968800104e-06, 'epoch': 9.71}
{'loss': 1.0983, 'grad_norm': 72578.578125, 'learning_rate': 9.229969233435887e-06, 'epoch': 9.71}
{'loss': 1.0985, 'grad_norm': 916.383544921875, 'learning_rate': 9.099969666767776e-06, 'epoch': 9.71}
{'loss': 1.0988, 'grad_norm': 100080.0859375, 'learning_rate': 8.969970100099665e-06, 'epoch': 9.72}
{'loss': 1.0995, 'grad_norm': 21211.216796875, 'learning_rate': 8.839970533431555e-06, 'epoch': 9.72}
{'loss': 1.0988, 'grad_norm': 649125.75, 'learning_rate': 8.709970966763444e-06, 'epoch': 9.73}
{'loss': 1.099, 'grad_norm': 4846.42041015625, 'learning_rate': 8.579971400095331e-06, 'epoch': 9.73}
{'loss': 1.0987, 'grad_norm': 567634.875, 'learning_rate': 8.44997183342722e-06, 'epoch': 9.74}
{'loss': 1.0999, 'grad_norm': 4098.921875, 'learning_rate': 8.31997226675911e-06, 'epoch': 9.74}
{'loss': 1.0989, 'grad_norm': 3807.630859375, 'learning_rate': 8.189972700091e-06, 'epoch': 9.74}
{'loss': 1.0993, 'grad_norm': 5662.32421875, 'learning_rate': 8.059973133422887e-06, 'epoch': 9.75}
{'loss': 1.0986, 'grad_norm': 15629.6728515625, 'learning_rate': 7.929973566754776e-06, 'epoch': 9.75}
{'loss': 1.099, 'grad_norm': 6687.33837890625, 'learning_rate': 7.799974000086665e-06, 'epoch': 9.76}
{'loss': 1.0995, 'grad_norm': 3601.596923828125, 'learning_rate': 7.669974433418555e-06, 'epoch': 9.76}
{'loss': 1.1001, 'grad_norm': 3104.50732421875, 'learning_rate': 7.539974866750444e-06, 'epoch': 9.76}
{'loss': 1.1, 'grad_norm': 23925.240234375, 'learning_rate': 7.409975300082333e-06, 'epoch': 9.77}
{'loss': 1.0993, 'grad_norm': 92548.875, 'learning_rate': 7.279975733414222e-06, 'epoch': 9.77}
{'loss': 1.0989, 'grad_norm': 22370.60546875, 'learning_rate': 7.149976166746111e-06, 'epoch': 9.78}
{'loss': 1.0984, 'grad_norm': 5494.44775390625, 'learning_rate': 7.019976600077999e-06, 'epoch': 9.78}
{'loss': 1.1003, 'grad_norm': 3734.687255859375, 'learning_rate': 6.889977033409889e-06, 'epoch': 9.78}
{'loss': 1.0979, 'grad_norm': 625500.625, 'learning_rate': 6.759977466741777e-06, 'epoch': 9.79}
{'loss': 1.1, 'grad_norm': 3433.26708984375, 'learning_rate': 6.629977900073666e-06, 'epoch': 9.79}
{'loss': 1.0996, 'grad_norm': 2922.179443359375, 'learning_rate': 6.499978333405555e-06, 'epoch': 9.8}
{'loss': 1.0996, 'grad_norm': 85058.3828125, 'learning_rate': 6.369978766737444e-06, 'epoch': 9.8}
{'loss': 1.0979, 'grad_norm': 4626.33935546875, 'learning_rate': 6.2399792000693325e-06, 'epoch': 9.8}
{'loss': 1.0996, 'grad_norm': 5345.1279296875, 'learning_rate': 6.109979633401222e-06, 'epoch': 9.81}
{'loss': 1.0989, 'grad_norm': 178541.234375, 'learning_rate': 5.97998006673311e-06, 'epoch': 9.81}
{'loss': 1.1001, 'grad_norm': 17801.18359375, 'learning_rate': 5.8499805000649995e-06, 'epoch': 9.82}
{'loss': 1.0985, 'grad_norm': 13084.44921875, 'learning_rate': 5.719980933396888e-06, 'epoch': 9.82}
{'loss': 1.0977, 'grad_norm': 5335.58203125, 'learning_rate': 5.589981366728777e-06, 'epoch': 9.82}
{'loss': 1.0987, 'grad_norm': 22387.923828125, 'learning_rate': 5.4599818000606665e-06, 'epoch': 9.83}
{'loss': 1.0989, 'grad_norm': 9751.68359375, 'learning_rate': 5.329982233392555e-06, 'epoch': 9.83}
{'loss': 1.0986, 'grad_norm': 2538.3955078125, 'learning_rate': 5.199982666724444e-06, 'epoch': 9.84}
{'loss': 1.0989, 'grad_norm': 23006.4140625, 'learning_rate': 5.069983100056333e-06, 'epoch': 9.84}
{'loss': 1.0987, 'grad_norm': 49034.0, 'learning_rate': 4.939983533388222e-06, 'epoch': 9.85}
{'loss': 1.0997, 'grad_norm': 229150.640625, 'learning_rate': 4.80998396672011e-06, 'epoch': 9.85}
{'loss': 1.0994, 'grad_norm': 87832.3984375, 'learning_rate': 4.679984400052e-06, 'epoch': 9.85}
{'loss': 1.0998, 'grad_norm': 54147.0625, 'learning_rate': 4.549984833383888e-06, 'epoch': 9.86}
{'loss': 1.0992, 'grad_norm': 149371.78125, 'learning_rate': 4.419985266715777e-06, 'epoch': 9.86}
{'loss': 1.0995, 'grad_norm': 509.0706481933594, 'learning_rate': 4.289985700047666e-06, 'epoch': 9.87}
{'loss': 1.0991, 'grad_norm': 24542.720703125, 'learning_rate': 4.159986133379555e-06, 'epoch': 9.87}
{'loss': 1.0985, 'grad_norm': 37980.06640625, 'learning_rate': 4.0299865667114434e-06, 'epoch': 9.87}
{'loss': 1.0992, 'grad_norm': 6155.32373046875, 'learning_rate': 3.899987000043333e-06, 'epoch': 9.88}
{'loss': 1.0991, 'grad_norm': 1830.385498046875, 'learning_rate': 3.769987433375222e-06, 'epoch': 9.88}
{'loss': 1.0991, 'grad_norm': 1875.45654296875, 'learning_rate': 3.639987866707111e-06, 'epoch': 9.89}
{'loss': 1.0994, 'grad_norm': 887.4625244140625, 'learning_rate': 3.5099883000389997e-06, 'epoch': 9.89}
{'loss': 1.0989, 'grad_norm': 4960.2265625, 'learning_rate': 3.3799887333708886e-06, 'epoch': 9.89}
{'loss': 1.0988, 'grad_norm': 19100.0078125, 'learning_rate': 3.2499891667027774e-06, 'epoch': 9.9}
{'loss': 1.0988, 'grad_norm': 70190.5390625, 'learning_rate': 3.1199896000346663e-06, 'epoch': 9.9}
{'loss': 1.0983, 'grad_norm': 19919.55078125, 'learning_rate': 2.989990033366555e-06, 'epoch': 9.91}
{'loss': 1.0993, 'grad_norm': 12412.0400390625, 'learning_rate': 2.859990466698444e-06, 'epoch': 9.91}
{'loss': 1.0989, 'grad_norm': 5591643.5, 'learning_rate': 2.7299909000303332e-06, 'epoch': 9.91}
{'loss': 1.0985, 'grad_norm': 21152.171875, 'learning_rate': 2.599991333362222e-06, 'epoch': 9.92}
{'loss': 1.1005, 'grad_norm': 123892.046875, 'learning_rate': 2.469991766694111e-06, 'epoch': 9.92}
{'loss': 1.0988, 'grad_norm': 1080.570556640625, 'learning_rate': 2.339992200026e-06, 'epoch': 9.93}
{'loss': 1.0994, 'grad_norm': 565300.6875, 'learning_rate': 2.2099926333578887e-06, 'epoch': 9.93}
{'loss': 1.098, 'grad_norm': 8006.77392578125, 'learning_rate': 2.0799930666897775e-06, 'epoch': 9.93}
{'loss': 1.0979, 'grad_norm': 2694.132568359375, 'learning_rate': 1.9499935000216664e-06, 'epoch': 9.94}
{'loss': 1.1, 'grad_norm': 53720.546875, 'learning_rate': 1.8199939333535554e-06, 'epoch': 9.94}
{'loss': 1.0993, 'grad_norm': 16984.390625, 'learning_rate': 1.6899943666854443e-06, 'epoch': 9.95}
{'loss': 1.0982, 'grad_norm': 128966.40625, 'learning_rate': 1.5599948000173331e-06, 'epoch': 9.95}
{'loss': 1.0988, 'grad_norm': 6968.48486328125, 'learning_rate': 1.429995233349222e-06, 'epoch': 9.96}
{'loss': 1.0982, 'grad_norm': 2986.412109375, 'learning_rate': 1.299995666681111e-06, 'epoch': 9.96}
{'loss': 1.1006, 'grad_norm': 11944.6708984375, 'learning_rate': 1.169996100013e-06, 'epoch': 9.96}
{'loss': 1.0996, 'grad_norm': 57538.4296875, 'learning_rate': 1.0399965333448888e-06, 'epoch': 9.97}
{'loss': 1.1004, 'grad_norm': 16345.1572265625, 'learning_rate': 9.099969666767777e-07, 'epoch': 9.97}
{'loss': 1.0987, 'grad_norm': 161425.171875, 'learning_rate': 7.799974000086666e-07, 'epoch': 9.98}
{'loss': 1.0986, 'grad_norm': 29027.27734375, 'learning_rate': 6.499978333405555e-07, 'epoch': 9.98}
{'loss': 1.0976, 'grad_norm': 4908.45166015625, 'learning_rate': 5.199982666724444e-07, 'epoch': 9.98}
{'loss': 1.0981, 'grad_norm': 20244.837890625, 'learning_rate': 3.899987000043333e-07, 'epoch': 9.99}
{'loss': 1.0989, 'grad_norm': 4756.73095703125, 'learning_rate': 2.599991333362222e-07, 'epoch': 9.99}
{'loss': 1.0985, 'grad_norm': 610535.5625, 'learning_rate': 1.299995666681111e-07, 'epoch': 10.0}
{'loss': 1.0987, 'grad_norm': 2556.26708984375, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 1.1009973287582397, 'eval_accuracy': 0.3544574630667346, 'eval_runtime': 5.8332, 'eval_samples_per_second': 1682.62, 'eval_steps_per_second': 10.629, 'epoch': 10.0}
{'train_runtime': 5124.3999, 'train_samples_per_second': 766.338, 'train_steps_per_second': 4.791, 'train_loss': 1.0234806184613292, 'epoch': 10.0}
***** train metrics *****
  epoch                    =        10.0
  total_flos               = 857475878GF
  train_loss               =      1.0235
  train_runtime            =  1:25:24.39
  train_samples            =      392702
  train_samples_per_second =     766.338
  train_steps_per_second   =       4.791
05/05/2024 17:41:35 - INFO - __main__ - *** Evaluate ***
05/05/2024 17:41:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:41:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0003,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/log,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.06,
warmup_steps=0,
weight_decay=0.0,
)
05/05/2024 17:41:48 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='FacebookAI/roberta-large', lora_path=None, l_num=None, mode='base', config_name=None, rank=[8], lora_alpha=[768], target_modules=['query', 'value'], lora_dropout=0.0, lora_bias='none', lora_task_type='SEQ_CLS', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False)
05/05/2024 17:41:48 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:41:48 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:41:48 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:41:48 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:42:24 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
05/05/2024 17:42:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
05/05/2024 17:42:24 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
05/05/2024 17:42:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	560.11ms[0m
*** Parameter number after share ***
*** label and id ***
{'not_duplicate': 0, 'duplicate': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	583.89ms[0m
*** Parameter number after share ***
pissa params: 786,432 || trainable params: 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
pissa params: 786,432 || trainable params(wo classfier): 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight 是可训练的: True
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	594.97ms[0m
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight 是可训练的: True*** Parameter number after share ***

base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.bias 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.bias 是可训练的: True
*** label and id ***
{'not_duplicate': 0, 'duplicate': 1}
*** label and id ***
{'not_duplicate': 0, 'duplicate': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	606.28ms[0m
*** Parameter number after share ***
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	604.54ms[0m
*** Parameter number after share ***
*** label and id ***
{'not_duplicate': 0, 'duplicate': 1}
*** label and id ***
{'not_duplicate': 0, 'duplicate': 1}
05/05/2024 17:42:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-3fbfe0f9a501af73.arrow
05/05/2024 17:44:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-fa470ba23c72cca1.arrow
05/05/2024 17:44:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-372391bace7f7ada.arrow
05/05/2024 17:46:08 - INFO - __main__ - Sample 335243 of the training set: {'question1': 'How do I crack JEE in a month?', 'question2': 'How do I crack JEE in 4-5 months?', 'label': 0, 'idx': 335243, 'input_ids': [0, 6179, 109, 38, 7009, 344, 9993, 11, 10, 353, 116, 2, 2, 6179, 109, 38, 7009, 344, 9993, 11, 204, 12, 245, 377, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 17:46:08 - INFO - __main__ - Sample 58369 of the training set: {'question1': 'Who are the greatest people in the world?', 'question2': 'Can you name some people who have really saved the world?', 'label': 0, 'idx': 58369, 'input_ids': [0, 12375, 32, 5, 3968, 82, 11, 5, 232, 116, 2, 2, 10836, 47, 766, 103, 82, 54, 33, 269, 5305, 5, 232, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 17:46:08 - INFO - __main__ - Sample 13112 of the training set: {'question1': 'What is inside a Camel Crush cigarette?', 'question2': 'Are Camel Crush cigarettes designed to attract teen smokers?', 'label': 0, 'idx': 13112, 'input_ids': [0, 2264, 16, 1025, 10, 36204, 40016, 16596, 116, 2, 2, 13755, 36204, 40016, 14407, 1887, 7, 5696, 6066, 25494, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
05/05/2024 17:46:41 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:46:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:46:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/log,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.06,
warmup_steps=0,
weight_decay=0.0,
)
05/05/2024 17:46:42 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='FacebookAI/roberta-large', lora_path=None, l_num=None, mode='base', config_name=None, rank=[8], lora_alpha=[768], target_modules=['query', 'value'], lora_dropout=0.0, lora_bias='none', lora_task_type='SEQ_CLS', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False)
05/05/2024 17:46:42 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:46:42 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:46:42 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:47:19 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
05/05/2024 17:47:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
05/05/2024 17:47:19 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
05/05/2024 17:47:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
*** PiSSA !!! ***
*** PiSSA !!! ***
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	591.63ms[0m
*** Parameter number after share ***
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	594.29ms[0m
*** Parameter number after share ***
*** label and id ***
{'entailment': 0, 'not_entailment': 1}
pissa params: 786,432 || trainable params: 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
pissa params: 786,432 || trainable params(wo classfier): 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.bias 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.bias 是可训练的: True
*** label and id ***
{'entailment': 0, 'not_entailment': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	616.69ms[0m
*** Parameter number after share ***
*** label and id ***
{'entailment': 0, 'not_entailment': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	626.89ms[0m
*** Parameter number after share ***
*** label and id ***
{'entailment': 0, 'not_entailment': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	659.48ms[0m
*** Parameter number after share ***
*** label and id ***
{'entailment': 0, 'not_entailment': 1}
05/05/2024 17:47:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-365fd151c58a2f35.arrow
05/05/2024 17:48:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b47319f9463b6476.arrow
05/05/2024 17:48:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e1e5b3fa72632930.arrow
05/05/2024 17:48:08 - INFO - __main__ - Sample 83810 of the training set: {'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': "On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.", 'label': 0, 'idx': 83810, 'input_ids': [0, 2264, 8893, 9, 5, 221, 9788, 58, 382, 8, 248, 9335, 3517, 45, 2460, 7, 3679, 116, 2, 2, 4148, 974, 759, 23, 5, 2238, 4580, 760, 6, 10, 121, 4, 104, 4, 262, 212, 35614, 2925, 6304, 40955, 30456, 2711, 36, 246, 6, 151, 3878, 43, 8, 5, 121, 4, 104, 4, 112, 620, 6144, 2925, 36, 1092, 6, 151, 2383, 996, 6, 151, 42669, 43, 58, 35578, 13, 5, 221, 9788, 361, 212, 2938, 826, 18, 130, 12, 4862, 1657, 196, 9689, 21163, 13767, 8893, 23, 5, 9846, 9, 732, 366, 179, 23895, 13878, 6, 53, 51, 2312, 7, 5111, 223, 1754, 3177, 8, 1577, 8848, 323, 668, 578, 41324, 19, 103, 379, 6, 151, 6981, 12675, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 17:48:08 - INFO - __main__ - Sample 14592 of the training set: {'question': "How did the Egyptian people feel about Nasser's response to the attack?", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.', 'label': 1, 'idx': 14592, 'input_ids': [0, 6179, 222, 5, 10377, 82, 619, 59, 234, 16151, 18, 1263, 7, 5, 908, 116, 2, 2, 487, 16151, 222, 45, 619, 14, 5, 10377, 2938, 21, 1227, 13, 10, 12065, 8, 222, 45, 28528, 16959, 19048, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 17:48:08 - INFO - __main__ - Sample 3278 of the training set: {'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.', 'label': 0, 'idx': 3278, 'input_ids': [0, 2264, 761, 9, 3054, 32, 4453, 9, 602, 160, 32039, 116, 2, 2, 13863, 4062, 13565, 574, 3054, 32, 4453, 9, 602, 160, 32039, 31, 10, 1514, 15, 5, 9124, 6, 634, 5, 6854, 8, 10, 878, 386, 16, 444, 55, 2423, 5693, 8, 8621, 10, 19351, 1709, 2408, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
05/05/2024 17:48:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:48:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0004,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/log,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.06,
warmup_steps=0,
weight_decay=0.0,
)
05/05/2024 17:48:30 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='FacebookAI/roberta-large', lora_path=None, l_num=None, mode='base', config_name=None, rank=[8], lora_alpha=[768], target_modules=['query', 'value'], lora_dropout=0.0, lora_bias='none', lora_task_type='SEQ_CLS', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False)
05/05/2024 17:48:30 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:48:30 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:48:31 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:48:31 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:49:07 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
05/05/2024 17:49:07 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
05/05/2024 17:49:07 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
05/05/2024 17:49:07 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	559.80ms[0m
*** Parameter number after share ***
*** label and id ***
{'entailment': 0, 'not_entailment': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	598.14ms[0m
*** Parameter number after share ***
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	594.82ms[0m
*** Parameter number after share ***
*** label and id ***
{'entailment': 0, 'not_entailment': 1}
pissa params: 786,432 || trainable params: 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
pissa params: 786,432 || trainable params(wo classfier): 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.bias 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.bias 是可训练的: True
*** label and id ***
{'entailment': 0, 'not_entailment': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	626.01ms[0m
*** Parameter number after share ***
*** label and id ***
{'entailment': 0, 'not_entailment': 1}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	641.39ms[0m
*** Parameter number after share ***
*** label and id ***
{'entailment': 0, 'not_entailment': 1}
05/05/2024 17:49:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b18eb1a3d041e8b0.arrow
05/05/2024 17:49:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-27657e2cc00281ee.arrow
05/05/2024 17:49:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-dc070f0b8e9d8232.arrow
05/05/2024 17:49:25 - INFO - __main__ - Sample 456 of the training set: {'sentence1': "A computer system failure closed down share trading at the Tokyo Stock Exchange for most of yesterday, the worst disruption to date for Asia's largest bourse.", 'sentence2': 'The Tokyo Stock Exchange was closed down by computer system failure.', 'label': 0, 'idx': 456, 'input_ids': [0, 250, 3034, 467, 2988, 1367, 159, 458, 1446, 23, 5, 5308, 3412, 3080, 13, 144, 9, 2350, 6, 5, 2373, 10044, 7, 1248, 13, 1817, 18, 1154, 741, 21117, 4, 2, 2, 133, 5308, 3412, 3080, 21, 1367, 159, 30, 3034, 467, 2988, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 17:49:25 - INFO - __main__ - Sample 102 of the training set: {'sentence1': 'Nagin defended his plan to return up to 180,000 people to the city, within a week and a half, despite concerns about the short supply of drinking water and heavily polluted floodwaters.', 'sentence2': 'Thousands of people are expected to return to New Orleans this week, as areas of the city are opened up to residents.', 'label': 1, 'idx': 102, 'input_ids': [0, 487, 23065, 7194, 39, 563, 7, 671, 62, 7, 8963, 6, 151, 82, 7, 5, 343, 6, 624, 10, 186, 8, 10, 457, 6, 1135, 1379, 59, 5, 765, 1787, 9, 4835, 514, 8, 4008, 30002, 5005, 21648, 4, 2, 2, 33383, 9, 82, 32, 421, 7, 671, 7, 188, 4942, 42, 186, 6, 25, 911, 9, 5, 343, 32, 1357, 62, 7, 1196, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 17:49:25 - INFO - __main__ - Sample 1126 of the training set: {'sentence1': "In 1969, he drew up the report proposing the expulsion from the party of the Manifesto group. In 1984, after Berlinguer's death, Natta was elected as party secretary.", 'sentence2': 'Natta supported the Manifesto group.', 'label': 1, 'idx': 1126, 'input_ids': [0, 1121, 15077, 6, 37, 4855, 62, 5, 266, 15670, 5, 27020, 31, 5, 537, 9, 5, 41373, 139, 333, 4, 96, 11724, 6, 71, 4323, 1527, 7414, 18, 744, 6, 234, 10460, 21, 2736, 25, 537, 2971, 4, 2, 2, 487, 10460, 2800, 5, 41373, 139, 333, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
05/05/2024 17:49:44 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:49:44 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0004,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/log,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.06,
warmup_steps=0,
weight_decay=0.0,
)
05/05/2024 17:49:44 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='FacebookAI/roberta-large', lora_path=None, l_num=None, mode='base', config_name=None, rank=[8], lora_alpha=[768], target_modules=['query', 'value'], lora_dropout=0.0, lora_bias='none', lora_task_type='SEQ_CLS', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False)
05/05/2024 17:49:44 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:49:45 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:49:45 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:49:45 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 17:50:21 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
05/05/2024 17:50:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
05/05/2024 17:50:21 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
05/05/2024 17:50:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	542.35ms[0m
*** Parameter number after share ***
pissa params: 786,432 || trainable params: 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
pissa params: 786,432 || trainable params(wo classfier): 1,838,082 || all params: 357,199,876 || trainable%: 0.5145808057335384
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight 是可训练的: True                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	549.89ms[0m

*** Parameter number after share ***
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight 是可训练的: True*** label and id ***

{'negative': 0, 'positive': 1}
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight 是可训练的: True
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	544.71ms[0m
*** Parameter number after share ***
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.bias 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.bias 是可训练的: True
*** label and id ***
{'negative': 0, 'positive': 1}
*** label and id ***
{'negative': 0, 'positive': 1}
05/05/2024 17:50:38 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9c47aec14af24eb8.arrow
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	583.53ms[0m
*** Parameter number after share ***
*** label and id ***
{'negative': 0, 'positive': 1}
05/05/2024 17:50:38 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-996c3cc0b9b9a77b.arrow
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	604.53ms[0m
*** Parameter number after share ***
*** label and id ***
{'negative': 0, 'positive': 1}
05/05/2024 17:50:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f253b23e3c646185.arrow
05/05/2024 17:50:38 - INFO - __main__ - Sample 14592 of the training set: {'sentence': 'a great movie ', 'label': 1, 'idx': 14592, 'input_ids': [0, 102, 372, 1569, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 17:50:38 - INFO - __main__ - Sample 3278 of the training set: {'sentence': 'entertaining , if somewhat standardized , action ', 'label': 1, 'idx': 3278, 'input_ids': [0, 1342, 2399, 8173, 2156, 114, 5568, 28631, 2156, 814, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 17:50:38 - INFO - __main__ - Sample 36048 of the training set: {'sentence': 'even when there are lulls , the emotions seem authentic , ', 'label': 1, 'idx': 36048, 'input_ids': [0, 12963, 77, 89, 32, 29620, 29, 2156, 5, 8597, 2045, 12757, 2156, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
{'loss': 0.751, 'grad_norm': 6.786690711975098, 'learning_rate': 1.58102766798419e-05, 'epoch': 0.02}
{'loss': 0.7038, 'grad_norm': 14.019113540649414, 'learning_rate': 3.16205533596838e-05, 'epoch': 0.05}
{'loss': 0.6992, 'grad_norm': 6.288963317871094, 'learning_rate': 4.743083003952569e-05, 'epoch': 0.07}
{'loss': 0.6923, 'grad_norm': 3.474560260772705, 'learning_rate': 6.32411067193676e-05, 'epoch': 0.1}
{'loss': 0.6648, 'grad_norm': 5.35317850112915, 'learning_rate': 7.905138339920949e-05, 'epoch': 0.12}
{'loss': 0.4616, 'grad_norm': 44.885040283203125, 'learning_rate': 9.486166007905138e-05, 'epoch': 0.14}
{'loss': 0.3441, 'grad_norm': 31.547439575195312, 'learning_rate': 0.00011067193675889329, 'epoch': 0.17}
{'loss': 0.3008, 'grad_norm': 20.140125274658203, 'learning_rate': 0.0001264822134387352, 'epoch': 0.19}
{'loss': 0.2823, 'grad_norm': 17.343318939208984, 'learning_rate': 0.00014229249011857707, 'epoch': 0.21}
{'loss': 0.2621, 'grad_norm': 34.9547004699707, 'learning_rate': 0.00015810276679841898, 'epoch': 0.24}
{'loss': 0.2396, 'grad_norm': 34.138160705566406, 'learning_rate': 0.00017391304347826088, 'epoch': 0.26}
{'loss': 0.2261, 'grad_norm': 23.997684478759766, 'learning_rate': 0.00018972332015810276, 'epoch': 0.29}
{'loss': 0.231, 'grad_norm': 22.376136779785156, 'learning_rate': 0.0002055335968379447, 'epoch': 0.31}
{'loss': 0.3048, 'grad_norm': 51.19255065917969, 'learning_rate': 0.00022134387351778657, 'epoch': 0.33}
{'loss': 0.4159, 'grad_norm': 146.67337036132812, 'learning_rate': 0.00023715415019762848, 'epoch': 0.36}
{'loss': 0.3447, 'grad_norm': 21.56697654724121, 'learning_rate': 0.0002529644268774704, 'epoch': 0.38}
{'loss': 0.2784, 'grad_norm': 19.705669403076172, 'learning_rate': 0.0002687747035573123, 'epoch': 0.4}
{'loss': 0.2555, 'grad_norm': 19.17057991027832, 'learning_rate': 0.00028458498023715414, 'epoch': 0.43}
{'loss': 0.2764, 'grad_norm': 28.196048736572266, 'learning_rate': 0.00030039525691699605, 'epoch': 0.45}
{'loss': 0.2472, 'grad_norm': 48.820377349853516, 'learning_rate': 0.00031620553359683795, 'epoch': 0.48}
{'loss': 0.2617, 'grad_norm': 55.440162658691406, 'learning_rate': 0.00033201581027667986, 'epoch': 0.5}
{'loss': 0.317, 'grad_norm': 29.399080276489258, 'learning_rate': 0.00034782608695652176, 'epoch': 0.52}
{'loss': 0.2843, 'grad_norm': 36.33662414550781, 'learning_rate': 0.00036363636363636367, 'epoch': 0.55}
{'loss': 0.2736, 'grad_norm': 27.0938777923584, 'learning_rate': 0.0003794466403162055, 'epoch': 0.57}
{'loss': 0.2966, 'grad_norm': 33.18619155883789, 'learning_rate': 0.0003952569169960474, 'epoch': 0.59}
{'loss': 0.3063, 'grad_norm': 18.815567016601562, 'learning_rate': 0.00039929239322719233, 'epoch': 0.62}
{'loss': 0.3049, 'grad_norm': 19.894481658935547, 'learning_rate': 0.00039828152640889564, 'epoch': 0.64}
{'loss': 0.3199, 'grad_norm': 60.55906295776367, 'learning_rate': 0.00039727065959059896, 'epoch': 0.67}
{'loss': 0.3246, 'grad_norm': 134.85128784179688, 'learning_rate': 0.0003962597927723023, 'epoch': 0.69}
{'loss': 0.4246, 'grad_norm': 69.69883728027344, 'learning_rate': 0.0003952489259540056, 'epoch': 0.71}
{'loss': 0.324, 'grad_norm': 56.79163360595703, 'learning_rate': 0.0003942380591357089, 'epoch': 0.74}
{'loss': 0.4768, 'grad_norm': 64.92915344238281, 'learning_rate': 0.00039322719231741217, 'epoch': 0.76}
{'loss': 0.42, 'grad_norm': 35.04891586303711, 'learning_rate': 0.0003922163254991155, 'epoch': 0.78}
{'loss': 0.4667, 'grad_norm': 85.25907897949219, 'learning_rate': 0.00039120545868081885, 'epoch': 0.81}
{'loss': 0.4303, 'grad_norm': 70.35641479492188, 'learning_rate': 0.00039019459186252217, 'epoch': 0.83}
{'loss': 0.4156, 'grad_norm': 40.76691818237305, 'learning_rate': 0.0003891837250442254, 'epoch': 0.86}
{'loss': 0.4209, 'grad_norm': 62.594478607177734, 'learning_rate': 0.00038817285822592874, 'epoch': 0.88}
{'loss': 0.4201, 'grad_norm': 65.27642059326172, 'learning_rate': 0.00038716199140763206, 'epoch': 0.9}
{'loss': 0.4851, 'grad_norm': 78.9166030883789, 'learning_rate': 0.00038615112458933537, 'epoch': 0.93}
{'loss': 0.6467, 'grad_norm': 34.724220275878906, 'learning_rate': 0.0003851402577710387, 'epoch': 0.95}
{'loss': 0.7143, 'grad_norm': 4.902586936950684, 'learning_rate': 0.000384129390952742, 'epoch': 0.97}
{'loss': 0.6899, 'grad_norm': 0.8626901507377625, 'learning_rate': 0.0003831185241344453, 'epoch': 1.0}
{'eval_loss': 0.7234525084495544, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.5222, 'eval_samples_per_second': 1669.854, 'eval_steps_per_second': 11.49, 'epoch': 1.0}
{'loss': 0.7024, 'grad_norm': 0.5322889089584351, 'learning_rate': 0.00038210765731614863, 'epoch': 1.02}
{'loss': 0.7108, 'grad_norm': 2.207636594772339, 'learning_rate': 0.0003810967904978519, 'epoch': 1.05}
{'loss': 0.8956, 'grad_norm': 101.09386444091797, 'learning_rate': 0.0003800859236795552, 'epoch': 1.07}
{'loss': 0.7339, 'grad_norm': 5.4339375495910645, 'learning_rate': 0.0003790750568612585, 'epoch': 1.09}
{'loss': 0.6918, 'grad_norm': 0.37749895453453064, 'learning_rate': 0.0003780641900429619, 'epoch': 1.12}
{'loss': 0.6958, 'grad_norm': 4.229619979858398, 'learning_rate': 0.00037705332322466516, 'epoch': 1.14}
{'loss': 0.7095, 'grad_norm': 4.054166793823242, 'learning_rate': 0.00037604245640636847, 'epoch': 1.16}
{'loss': 0.7273, 'grad_norm': 0.724985659122467, 'learning_rate': 0.0003750315895880718, 'epoch': 1.19}
{'loss': 0.7345, 'grad_norm': 0.7844206690788269, 'learning_rate': 0.0003740207227697751, 'epoch': 1.21}
{'loss': 0.6984, 'grad_norm': 1.2133238315582275, 'learning_rate': 0.0003730098559514784, 'epoch': 1.24}
{'loss': 0.6946, 'grad_norm': 2.646298408508301, 'learning_rate': 0.00037199898913318173, 'epoch': 1.26}
{'loss': 0.695, 'grad_norm': 0.9523214101791382, 'learning_rate': 0.00037098812231488505, 'epoch': 1.28}
{'loss': 0.6973, 'grad_norm': 1.2727344036102295, 'learning_rate': 0.00036997725549658836, 'epoch': 1.31}
{'loss': 0.6945, 'grad_norm': 1.7107747793197632, 'learning_rate': 0.0003689663886782917, 'epoch': 1.33}
{'loss': 0.6942, 'grad_norm': 0.6798468232154846, 'learning_rate': 0.00036795552185999494, 'epoch': 1.35}
{'loss': 0.6893, 'grad_norm': 3.041984796524048, 'learning_rate': 0.00036694465504169825, 'epoch': 1.38}
{'loss': 0.6932, 'grad_norm': 0.6283738017082214, 'learning_rate': 0.00036593378822340157, 'epoch': 1.4}
{'loss': 0.6906, 'grad_norm': 2.254845380783081, 'learning_rate': 0.00036492292140510494, 'epoch': 1.43}
{'loss': 0.6958, 'grad_norm': 1.188005805015564, 'learning_rate': 0.0003639120545868082, 'epoch': 1.45}
{'loss': 0.6973, 'grad_norm': 44.08903121948242, 'learning_rate': 0.0003629011877685115, 'epoch': 1.47}
{'loss': 0.7048, 'grad_norm': 1.6653566360473633, 'learning_rate': 0.00036189032095021483, 'epoch': 1.5}
{'loss': 0.6917, 'grad_norm': 0.40216970443725586, 'learning_rate': 0.00036087945413191814, 'epoch': 1.52}
{'loss': 0.6941, 'grad_norm': 1.4982713460922241, 'learning_rate': 0.0003598685873136214, 'epoch': 1.54}
{'loss': 0.6964, 'grad_norm': 0.9293339848518372, 'learning_rate': 0.0003588577204953248, 'epoch': 1.57}
{'loss': 0.6961, 'grad_norm': 1.792846918106079, 'learning_rate': 0.0003578468536770281, 'epoch': 1.59}
{'loss': 0.6893, 'grad_norm': 0.8285326957702637, 'learning_rate': 0.0003568359868587314, 'epoch': 1.62}
{'loss': 0.6907, 'grad_norm': 1.33376944065094, 'learning_rate': 0.00035582512004043467, 'epoch': 1.64}
{'loss': 0.6943, 'grad_norm': 0.8067375421524048, 'learning_rate': 0.000354814253222138, 'epoch': 1.66}
{'loss': 0.6899, 'grad_norm': 0.9937979578971863, 'learning_rate': 0.0003538033864038413, 'epoch': 1.69}
{'loss': 0.6889, 'grad_norm': 0.5170089602470398, 'learning_rate': 0.0003527925195855446, 'epoch': 1.71}
{'loss': 0.68, 'grad_norm': 0.5703122615814209, 'learning_rate': 0.00035178165276724793, 'epoch': 1.73}
{'loss': 0.6887, 'grad_norm': 0.9665079116821289, 'learning_rate': 0.00035077078594895124, 'epoch': 1.76}
{'loss': 0.6916, 'grad_norm': 1.743820071220398, 'learning_rate': 0.00034975991913065456, 'epoch': 1.78}
{'loss': 0.6934, 'grad_norm': 1.3941214084625244, 'learning_rate': 0.0003487490523123579, 'epoch': 1.81}
{'loss': 0.6928, 'grad_norm': 0.5266116261482239, 'learning_rate': 0.0003477381854940612, 'epoch': 1.83}
{'loss': 0.6902, 'grad_norm': 0.4889275133609772, 'learning_rate': 0.00034672731867576445, 'epoch': 1.85}
{'loss': 0.6908, 'grad_norm': 0.8848739266395569, 'learning_rate': 0.0003457164518574678, 'epoch': 1.88}
{'loss': 0.6863, 'grad_norm': 0.4622041881084442, 'learning_rate': 0.00034470558503917113, 'epoch': 1.9}
{'loss': 0.6921, 'grad_norm': 0.9194414019584656, 'learning_rate': 0.00034369471822087445, 'epoch': 1.92}
{'loss': 0.6884, 'grad_norm': 0.9421283006668091, 'learning_rate': 0.0003426838514025777, 'epoch': 1.95}
{'loss': 0.69, 'grad_norm': 2.0503532886505127, 'learning_rate': 0.000341672984584281, 'epoch': 1.97}
{'loss': 0.6933, 'grad_norm': 1.1726926565170288, 'learning_rate': 0.00034066211776598434, 'epoch': 2.0}
{'eval_loss': 0.6989163756370544, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.5285, 'eval_samples_per_second': 1649.827, 'eval_steps_per_second': 11.352, 'epoch': 2.0}
{'loss': 0.6857, 'grad_norm': 1.1621174812316895, 'learning_rate': 0.00033965125094768766, 'epoch': 2.02}
{'loss': 0.6964, 'grad_norm': 1.0510584115982056, 'learning_rate': 0.00033864038412939097, 'epoch': 2.04}
{'loss': 0.6884, 'grad_norm': 0.7768766283988953, 'learning_rate': 0.0003376295173110943, 'epoch': 2.07}
{'loss': 0.6983, 'grad_norm': 33.397647857666016, 'learning_rate': 0.0003366186504927976, 'epoch': 2.09}
{'loss': 0.69, 'grad_norm': 1.7777281999588013, 'learning_rate': 0.0003356077836745009, 'epoch': 2.11}
{'loss': 0.6928, 'grad_norm': 1.8024543523788452, 'learning_rate': 0.0003345969168562042, 'epoch': 2.14}
{'loss': 0.6912, 'grad_norm': 1.2947497367858887, 'learning_rate': 0.0003335860500379075, 'epoch': 2.16}
{'loss': 0.6931, 'grad_norm': 1.2058043479919434, 'learning_rate': 0.00033257518321961086, 'epoch': 2.19}
{'loss': 0.6884, 'grad_norm': 1.8175307512283325, 'learning_rate': 0.0003315643164013142, 'epoch': 2.21}
{'loss': 0.705, 'grad_norm': 3.181021213531494, 'learning_rate': 0.00033055344958301744, 'epoch': 2.23}
{'loss': 0.6945, 'grad_norm': 0.7562975287437439, 'learning_rate': 0.00032954258276472075, 'epoch': 2.26}
{'loss': 0.6923, 'grad_norm': 0.37219008803367615, 'learning_rate': 0.00032853171594642407, 'epoch': 2.28}
{'loss': 0.6864, 'grad_norm': 75.62222290039062, 'learning_rate': 0.0003275208491281274, 'epoch': 2.3}
{'loss': 0.6956, 'grad_norm': 1.3493702411651611, 'learning_rate': 0.0003265099823098307, 'epoch': 2.33}
{'loss': 0.705, 'grad_norm': 8.493245124816895, 'learning_rate': 0.000325499115491534, 'epoch': 2.35}
{'loss': 0.6876, 'grad_norm': 0.5458782911300659, 'learning_rate': 0.00032448824867323733, 'epoch': 2.38}
{'loss': 0.6853, 'grad_norm': 0.5198119878768921, 'learning_rate': 0.00032347738185494065, 'epoch': 2.4}
{'loss': 0.6878, 'grad_norm': 0.39545324444770813, 'learning_rate': 0.00032246651503664396, 'epoch': 2.42}
{'loss': 0.6863, 'grad_norm': 1.8222216367721558, 'learning_rate': 0.0003214556482183472, 'epoch': 2.45}
{'loss': 0.6856, 'grad_norm': 1.5273725986480713, 'learning_rate': 0.00032044478140005054, 'epoch': 2.47}
{'loss': 0.6904, 'grad_norm': 1.094565987586975, 'learning_rate': 0.0003194339145817539, 'epoch': 2.49}
{'loss': 0.6876, 'grad_norm': 0.27920621633529663, 'learning_rate': 0.00031842304776345717, 'epoch': 2.52}
{'loss': 0.6878, 'grad_norm': 0.6098011136054993, 'learning_rate': 0.0003174121809451605, 'epoch': 2.54}
{'loss': 0.701, 'grad_norm': 2.862022638320923, 'learning_rate': 0.0003164013141268638, 'epoch': 2.57}
{'loss': 0.6956, 'grad_norm': 1.7404383420944214, 'learning_rate': 0.0003153904473085671, 'epoch': 2.59}
{'loss': 0.686, 'grad_norm': 0.269475519657135, 'learning_rate': 0.00031437958049027043, 'epoch': 2.61}
{'loss': 0.6876, 'grad_norm': 0.4552339017391205, 'learning_rate': 0.0003133687136719737, 'epoch': 2.64}
{'loss': 0.6926, 'grad_norm': 0.6380156874656677, 'learning_rate': 0.00031235784685367706, 'epoch': 2.66}
{'loss': 0.6903, 'grad_norm': 0.6706538796424866, 'learning_rate': 0.0003113469800353804, 'epoch': 2.68}
{'loss': 0.688, 'grad_norm': 1.9342906475067139, 'learning_rate': 0.0003103361132170837, 'epoch': 2.71}
{'loss': 0.6926, 'grad_norm': 0.5964370369911194, 'learning_rate': 0.00030932524639878695, 'epoch': 2.73}
{'loss': 0.6916, 'grad_norm': 0.9630399346351624, 'learning_rate': 0.00030831437958049027, 'epoch': 2.76}
{'loss': 0.6893, 'grad_norm': 0.36961230635643005, 'learning_rate': 0.0003073035127621936, 'epoch': 2.78}
{'loss': 0.6896, 'grad_norm': 0.843712568283081, 'learning_rate': 0.00030629264594389695, 'epoch': 2.8}
{'loss': 0.6832, 'grad_norm': 0.3292039632797241, 'learning_rate': 0.0003052817791256002, 'epoch': 2.83}
{'loss': 0.696, 'grad_norm': 0.7239813804626465, 'learning_rate': 0.0003042709123073035, 'epoch': 2.85}
{'loss': 0.6932, 'grad_norm': 0.4664829671382904, 'learning_rate': 0.00030326004548900684, 'epoch': 2.87}
{'loss': 0.6959, 'grad_norm': 1.5693961381912231, 'learning_rate': 0.00030224917867071016, 'epoch': 2.9}
{'loss': 0.6865, 'grad_norm': 0.47992202639579773, 'learning_rate': 0.00030123831185241347, 'epoch': 2.92}
{'loss': 0.6978, 'grad_norm': 0.9739516973495483, 'learning_rate': 0.00030022744503411673, 'epoch': 2.95}
{'loss': 0.6906, 'grad_norm': 0.4825195372104645, 'learning_rate': 0.0002992165782158201, 'epoch': 2.97}
{'loss': 0.686, 'grad_norm': 0.5113252997398376, 'learning_rate': 0.0002982057113975234, 'epoch': 2.99}
{'eval_loss': 0.6941320896148682, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.5342, 'eval_samples_per_second': 1632.496, 'eval_steps_per_second': 11.233, 'epoch': 3.0}
{'loss': 0.6866, 'grad_norm': 0.7602331042289734, 'learning_rate': 0.0002971948445792267, 'epoch': 3.02}
{'loss': 0.6937, 'grad_norm': 0.30968448519706726, 'learning_rate': 0.00029618397776093, 'epoch': 3.04}
{'loss': 0.6927, 'grad_norm': 1.6414510011672974, 'learning_rate': 0.0002951731109426333, 'epoch': 3.06}
{'loss': 0.688, 'grad_norm': 1.4192314147949219, 'learning_rate': 0.0002941622441243366, 'epoch': 3.09}
{'loss': 0.6938, 'grad_norm': 1.1791536808013916, 'learning_rate': 0.00029315137730603994, 'epoch': 3.11}
{'loss': 0.6882, 'grad_norm': 0.5167109370231628, 'learning_rate': 0.00029214051048774326, 'epoch': 3.14}
{'loss': 0.6876, 'grad_norm': 0.35966381430625916, 'learning_rate': 0.00029112964366944657, 'epoch': 3.16}
{'loss': 0.6901, 'grad_norm': 0.6081494092941284, 'learning_rate': 0.0002901187768511499, 'epoch': 3.18}
{'loss': 0.6877, 'grad_norm': 0.26950740814208984, 'learning_rate': 0.0002891079100328532, 'epoch': 3.21}
{'loss': 0.6914, 'grad_norm': 0.41174378991127014, 'learning_rate': 0.00028809704321455646, 'epoch': 3.23}
{'loss': 0.6889, 'grad_norm': 1.1031572818756104, 'learning_rate': 0.0002870861763962598, 'epoch': 3.25}
{'loss': 0.6919, 'grad_norm': 0.49062371253967285, 'learning_rate': 0.00028607530957796315, 'epoch': 3.28}
{'loss': 0.6918, 'grad_norm': 1.9313384294509888, 'learning_rate': 0.00028506444275966646, 'epoch': 3.3}
{'loss': 0.6861, 'grad_norm': 1.1721028089523315, 'learning_rate': 0.0002840535759413697, 'epoch': 3.33}
{'loss': 0.6848, 'grad_norm': 0.5482298135757446, 'learning_rate': 0.00028304270912307304, 'epoch': 3.35}
{'loss': 0.6943, 'grad_norm': 1.6276589632034302, 'learning_rate': 0.00028203184230477635, 'epoch': 3.37}
{'loss': 0.6871, 'grad_norm': 0.7957805395126343, 'learning_rate': 0.00028102097548647967, 'epoch': 3.4}
{'loss': 0.6844, 'grad_norm': 0.3606953024864197, 'learning_rate': 0.000280010108668183, 'epoch': 3.42}
{'loss': 0.6899, 'grad_norm': 0.910214900970459, 'learning_rate': 0.0002789992418498863, 'epoch': 3.44}
{'loss': 0.6946, 'grad_norm': 0.5314242243766785, 'learning_rate': 0.0002779883750315896, 'epoch': 3.47}
{'loss': 0.6872, 'grad_norm': 0.3461568355560303, 'learning_rate': 0.00027697750821329293, 'epoch': 3.49}
{'loss': 0.6902, 'grad_norm': 1.2339969873428345, 'learning_rate': 0.0002759666413949962, 'epoch': 3.52}
{'loss': 0.6863, 'grad_norm': 1.5971804857254028, 'learning_rate': 0.0002749557745766995, 'epoch': 3.54}
{'loss': 0.6849, 'grad_norm': 0.8212927579879761, 'learning_rate': 0.0002739449077584028, 'epoch': 3.56}
{'loss': 0.6897, 'grad_norm': 0.48324865102767944, 'learning_rate': 0.0002729340409401062, 'epoch': 3.59}
{'loss': 0.6901, 'grad_norm': 0.9447436332702637, 'learning_rate': 0.00027192317412180945, 'epoch': 3.61}
{'loss': 0.6862, 'grad_norm': 0.6937811374664307, 'learning_rate': 0.00027091230730351277, 'epoch': 3.63}
{'loss': 0.6942, 'grad_norm': 0.30124109983444214, 'learning_rate': 0.0002699014404852161, 'epoch': 3.66}
{'loss': 0.6903, 'grad_norm': 0.5632033944129944, 'learning_rate': 0.0002688905736669194, 'epoch': 3.68}
{'loss': 0.6959, 'grad_norm': 0.6639860272407532, 'learning_rate': 0.0002678797068486227, 'epoch': 3.71}
{'loss': 0.6871, 'grad_norm': 1.0839787721633911, 'learning_rate': 0.00026686884003032603, 'epoch': 3.73}
{'loss': 0.6794, 'grad_norm': 1.1274514198303223, 'learning_rate': 0.00026585797321202934, 'epoch': 3.75}
{'loss': 0.6871, 'grad_norm': 1.0319932699203491, 'learning_rate': 0.00026484710639373266, 'epoch': 3.78}
{'loss': 0.689, 'grad_norm': 0.4465554356575012, 'learning_rate': 0.000263836239575436, 'epoch': 3.8}
{'loss': 0.6834, 'grad_norm': 0.49985185265541077, 'learning_rate': 0.00026282537275713923, 'epoch': 3.82}
{'loss': 0.6879, 'grad_norm': 0.8486624956130981, 'learning_rate': 0.00026181450593884255, 'epoch': 3.85}
{'loss': 0.6896, 'grad_norm': 2.2004432678222656, 'learning_rate': 0.00026080363912054586, 'epoch': 3.87}
{'loss': 0.6859, 'grad_norm': 0.5205738544464111, 'learning_rate': 0.00025979277230224923, 'epoch': 3.9}
{'loss': 0.6892, 'grad_norm': 1.5314040184020996, 'learning_rate': 0.0002587819054839525, 'epoch': 3.92}
{'loss': 0.6839, 'grad_norm': 0.21242912113666534, 'learning_rate': 0.0002577710386656558, 'epoch': 3.94}
{'loss': 0.6921, 'grad_norm': 0.49300822615623474, 'learning_rate': 0.0002567601718473591, 'epoch': 3.97}
{'loss': 0.6873, 'grad_norm': 0.296485960483551, 'learning_rate': 0.00025574930502906244, 'epoch': 3.99}
{'eval_loss': 0.6944009065628052, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.6064, 'eval_samples_per_second': 1437.986, 'eval_steps_per_second': 9.894, 'epoch': 4.0}
{'loss': 0.6896, 'grad_norm': 0.7460135221481323, 'learning_rate': 0.0002547384382107657, 'epoch': 4.01}
{'loss': 0.6896, 'grad_norm': 0.2624852657318115, 'learning_rate': 0.000253727571392469, 'epoch': 4.04}
{'loss': 0.6853, 'grad_norm': 0.698079526424408, 'learning_rate': 0.0002527167045741724, 'epoch': 4.06}
{'loss': 0.6823, 'grad_norm': 0.2686043679714203, 'learning_rate': 0.0002517058377558757, 'epoch': 4.09}
{'loss': 0.6893, 'grad_norm': 1.0438449382781982, 'learning_rate': 0.00025069497093757896, 'epoch': 4.11}
{'loss': 0.6831, 'grad_norm': 1.1445584297180176, 'learning_rate': 0.0002496841041192823, 'epoch': 4.13}
{'loss': 0.6901, 'grad_norm': 0.7362515330314636, 'learning_rate': 0.0002486732373009856, 'epoch': 4.16}
{'loss': 0.6911, 'grad_norm': 0.7417823672294617, 'learning_rate': 0.0002476623704826889, 'epoch': 4.18}
{'loss': 0.6907, 'grad_norm': 0.755130410194397, 'learning_rate': 0.0002466515036643922, 'epoch': 4.2}
{'loss': 0.6887, 'grad_norm': 0.6007121801376343, 'learning_rate': 0.00024564063684609554, 'epoch': 4.23}
{'loss': 0.6854, 'grad_norm': 0.3604896664619446, 'learning_rate': 0.00024462977002779885, 'epoch': 4.25}
{'loss': 0.6867, 'grad_norm': 2.0607118606567383, 'learning_rate': 0.00024361890320950217, 'epoch': 4.28}
{'loss': 0.6902, 'grad_norm': 1.167594313621521, 'learning_rate': 0.00024260803639120546, 'epoch': 4.3}
{'loss': 0.6936, 'grad_norm': 0.20853237807750702, 'learning_rate': 0.00024159716957290877, 'epoch': 4.32}
{'loss': 0.693, 'grad_norm': 0.29142314195632935, 'learning_rate': 0.00024058630275461206, 'epoch': 4.35}
{'loss': 0.6793, 'grad_norm': 0.284757137298584, 'learning_rate': 0.00023957543593631543, 'epoch': 4.37}
{'loss': 0.692, 'grad_norm': 0.5115408897399902, 'learning_rate': 0.00023856456911801872, 'epoch': 4.39}
{'loss': 0.6867, 'grad_norm': 0.521077573299408, 'learning_rate': 0.00023755370229972203, 'epoch': 4.42}
{'loss': 0.6858, 'grad_norm': 0.6855685114860535, 'learning_rate': 0.00023654283548142532, 'epoch': 4.44}
{'loss': 0.6825, 'grad_norm': 0.52265864610672, 'learning_rate': 0.00023553196866312864, 'epoch': 4.47}
{'loss': 0.6877, 'grad_norm': 0.5286606550216675, 'learning_rate': 0.00023452110184483192, 'epoch': 4.49}
{'loss': 0.6871, 'grad_norm': 0.35135647654533386, 'learning_rate': 0.0002335102350265353, 'epoch': 4.51}
{'loss': 0.6822, 'grad_norm': 0.3969751000404358, 'learning_rate': 0.00023249936820823858, 'epoch': 4.54}
{'loss': 0.6866, 'grad_norm': 0.21015116572380066, 'learning_rate': 0.0002314885013899419, 'epoch': 4.56}
{'loss': 0.6874, 'grad_norm': 0.44735875725746155, 'learning_rate': 0.00023047763457164519, 'epoch': 4.58}
{'loss': 0.6871, 'grad_norm': 0.6160016059875488, 'learning_rate': 0.0002294667677533485, 'epoch': 4.61}
{'loss': 0.6857, 'grad_norm': 1.5183528661727905, 'learning_rate': 0.00022845590093505182, 'epoch': 4.63}
{'loss': 0.6942, 'grad_norm': 0.6160646080970764, 'learning_rate': 0.0002274450341167551, 'epoch': 4.66}
{'loss': 0.6945, 'grad_norm': 0.8082401156425476, 'learning_rate': 0.00022643416729845845, 'epoch': 4.68}
{'loss': 0.6824, 'grad_norm': 0.2333565354347229, 'learning_rate': 0.00022542330048016176, 'epoch': 4.7}
{'loss': 0.6845, 'grad_norm': 0.7067831158638, 'learning_rate': 0.00022441243366186505, 'epoch': 4.73}
{'loss': 0.6895, 'grad_norm': 0.9203864932060242, 'learning_rate': 0.00022340156684356837, 'epoch': 4.75}
{'loss': 0.6908, 'grad_norm': 0.33342957496643066, 'learning_rate': 0.00022239070002527168, 'epoch': 4.77}
{'loss': 0.682, 'grad_norm': 1.0310455560684204, 'learning_rate': 0.00022137983320697497, 'epoch': 4.8}
{'loss': 0.6884, 'grad_norm': 3.868746042251587, 'learning_rate': 0.0002203689663886783, 'epoch': 4.82}
{'loss': 0.6922, 'grad_norm': 2.0237603187561035, 'learning_rate': 0.00021935809957038163, 'epoch': 4.85}
{'loss': 0.686, 'grad_norm': 0.25948670506477356, 'learning_rate': 0.00021834723275208494, 'epoch': 4.87}
{'loss': 0.6897, 'grad_norm': 1.2097339630126953, 'learning_rate': 0.00021733636593378823, 'epoch': 4.89}
{'loss': 0.6859, 'grad_norm': 1.1297225952148438, 'learning_rate': 0.00021632549911549154, 'epoch': 4.92}
{'loss': 0.687, 'grad_norm': 0.9223136305809021, 'learning_rate': 0.00021531463229719483, 'epoch': 4.94}
{'loss': 0.6897, 'grad_norm': 0.31919047236442566, 'learning_rate': 0.00021430376547889815, 'epoch': 4.96}
{'loss': 0.6911, 'grad_norm': 1.1371724605560303, 'learning_rate': 0.0002132928986606015, 'epoch': 4.99}
{'eval_loss': 0.7022660374641418, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.5278, 'eval_samples_per_second': 1652.016, 'eval_steps_per_second': 11.367, 'epoch': 5.0}
{'loss': 0.6877, 'grad_norm': 0.47461891174316406, 'learning_rate': 0.0002122820318423048, 'epoch': 5.01}
{'loss': 0.6834, 'grad_norm': 1.1014765501022339, 'learning_rate': 0.0002112711650240081, 'epoch': 5.04}
{'loss': 0.689, 'grad_norm': 4.386129856109619, 'learning_rate': 0.0002102602982057114, 'epoch': 5.06}
{'loss': 0.6901, 'grad_norm': 0.9132633805274963, 'learning_rate': 0.0002092494313874147, 'epoch': 5.08}
{'loss': 0.6896, 'grad_norm': 0.5118312239646912, 'learning_rate': 0.000208238564569118, 'epoch': 5.11}
{'loss': 0.6873, 'grad_norm': 2.806267738342285, 'learning_rate': 0.00020722769775082135, 'epoch': 5.13}
{'loss': 0.691, 'grad_norm': 0.5768542289733887, 'learning_rate': 0.00020621683093252467, 'epoch': 5.15}
{'loss': 0.6865, 'grad_norm': 1.3445168733596802, 'learning_rate': 0.00020520596411422796, 'epoch': 5.18}
{'loss': 0.6907, 'grad_norm': 0.436103492975235, 'learning_rate': 0.00020419509729593127, 'epoch': 5.2}
{'loss': 0.687, 'grad_norm': 0.6423561573028564, 'learning_rate': 0.00020318423047763456, 'epoch': 5.23}
{'loss': 0.6863, 'grad_norm': 0.6854720711708069, 'learning_rate': 0.00020217336365933788, 'epoch': 5.25}
{'loss': 0.69, 'grad_norm': 0.46571141481399536, 'learning_rate': 0.0002011624968410412, 'epoch': 5.27}
{'loss': 0.6893, 'grad_norm': 0.8573927879333496, 'learning_rate': 0.00020015163002274453, 'epoch': 5.3}
{'loss': 0.6858, 'grad_norm': 0.848392903804779, 'learning_rate': 0.00019914076320444782, 'epoch': 5.32}
{'loss': 0.6877, 'grad_norm': 0.49657320976257324, 'learning_rate': 0.00019812989638615114, 'epoch': 5.34}
{'loss': 0.6824, 'grad_norm': 0.768932580947876, 'learning_rate': 0.00019711902956785445, 'epoch': 5.37}
{'loss': 0.6864, 'grad_norm': 1.725633978843689, 'learning_rate': 0.00019610816274955774, 'epoch': 5.39}
{'loss': 0.6837, 'grad_norm': 1.140824556350708, 'learning_rate': 0.00019509729593126108, 'epoch': 5.42}
{'loss': 0.6865, 'grad_norm': 0.9748864769935608, 'learning_rate': 0.00019408642911296437, 'epoch': 5.44}
{'loss': 0.6873, 'grad_norm': 2.1604626178741455, 'learning_rate': 0.00019307556229466769, 'epoch': 5.46}
{'loss': 0.6835, 'grad_norm': 2.293520450592041, 'learning_rate': 0.000192064695476371, 'epoch': 5.49}
{'loss': 0.6899, 'grad_norm': 0.47566089034080505, 'learning_rate': 0.00019105382865807432, 'epoch': 5.51}
{'loss': 0.6811, 'grad_norm': 1.831066608428955, 'learning_rate': 0.0001900429618397776, 'epoch': 5.53}
{'loss': 0.6853, 'grad_norm': 0.9474623203277588, 'learning_rate': 0.00018903209502148095, 'epoch': 5.56}
{'loss': 0.6849, 'grad_norm': 1.0124233961105347, 'learning_rate': 0.00018802122820318424, 'epoch': 5.58}
{'loss': 0.6861, 'grad_norm': 0.981083333492279, 'learning_rate': 0.00018701036138488755, 'epoch': 5.61}
{'loss': 0.6853, 'grad_norm': 1.2814176082611084, 'learning_rate': 0.00018599949456659087, 'epoch': 5.63}
{'loss': 0.6855, 'grad_norm': 1.761127233505249, 'learning_rate': 0.00018498862774829418, 'epoch': 5.65}
{'loss': 0.6829, 'grad_norm': 1.0623716115951538, 'learning_rate': 0.00018397776092999747, 'epoch': 5.68}
{'loss': 0.6836, 'grad_norm': 0.5971371531486511, 'learning_rate': 0.00018296689411170078, 'epoch': 5.7}
{'loss': 0.6937, 'grad_norm': 1.3624892234802246, 'learning_rate': 0.0001819560272934041, 'epoch': 5.72}
{'loss': 0.691, 'grad_norm': 1.8357110023498535, 'learning_rate': 0.00018094516047510741, 'epoch': 5.75}
{'loss': 0.6926, 'grad_norm': 0.7778657078742981, 'learning_rate': 0.0001799342936568107, 'epoch': 5.77}
{'loss': 0.6862, 'grad_norm': 1.700947880744934, 'learning_rate': 0.00017892342683851405, 'epoch': 5.8}
{'loss': 0.6792, 'grad_norm': 5.607035160064697, 'learning_rate': 0.00017791256002021733, 'epoch': 5.82}
{'loss': 0.6782, 'grad_norm': 1.5818672180175781, 'learning_rate': 0.00017690169320192065, 'epoch': 5.84}
{'loss': 0.6886, 'grad_norm': 0.9470676779747009, 'learning_rate': 0.00017589082638362396, 'epoch': 5.87}
{'loss': 0.6881, 'grad_norm': 0.8889241218566895, 'learning_rate': 0.00017487995956532728, 'epoch': 5.89}
{'loss': 0.6847, 'grad_norm': 0.4269487261772156, 'learning_rate': 0.0001738690927470306, 'epoch': 5.91}
{'loss': 0.6863, 'grad_norm': 1.0751394033432007, 'learning_rate': 0.0001728582259287339, 'epoch': 5.94}
{'loss': 0.6837, 'grad_norm': 0.5435797572135925, 'learning_rate': 0.00017184735911043722, 'epoch': 5.96}
{'loss': 0.6936, 'grad_norm': 0.9552979469299316, 'learning_rate': 0.0001708364922921405, 'epoch': 5.99}
{'eval_loss': 0.6942015290260315, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.5904, 'eval_samples_per_second': 1476.904, 'eval_steps_per_second': 10.162, 'epoch': 6.0}
{'loss': 0.6895, 'grad_norm': 0.463319331407547, 'learning_rate': 0.00016982562547384383, 'epoch': 6.01}
{'loss': 0.6867, 'grad_norm': 1.027294397354126, 'learning_rate': 0.00016881475865554714, 'epoch': 6.03}
{'loss': 0.6891, 'grad_norm': 1.132257103919983, 'learning_rate': 0.00016780389183725046, 'epoch': 6.06}
{'loss': 0.6874, 'grad_norm': 1.6653244495391846, 'learning_rate': 0.00016679302501895375, 'epoch': 6.08}
{'loss': 0.6908, 'grad_norm': 1.0662078857421875, 'learning_rate': 0.0001657821582006571, 'epoch': 6.1}
{'loss': 0.6914, 'grad_norm': 1.2674295902252197, 'learning_rate': 0.00016477129138236038, 'epoch': 6.13}
{'loss': 0.6895, 'grad_norm': 1.543961524963379, 'learning_rate': 0.0001637604245640637, 'epoch': 6.15}
{'loss': 0.6872, 'grad_norm': 0.9681376218795776, 'learning_rate': 0.000162749557745767, 'epoch': 6.18}
{'loss': 0.686, 'grad_norm': 0.6338911652565002, 'learning_rate': 0.00016173869092747032, 'epoch': 6.2}
{'loss': 0.688, 'grad_norm': 0.9826351404190063, 'learning_rate': 0.0001607278241091736, 'epoch': 6.22}
{'loss': 0.6857, 'grad_norm': 0.802261471748352, 'learning_rate': 0.00015971695729087695, 'epoch': 6.25}
{'loss': 0.6884, 'grad_norm': 0.4993560016155243, 'learning_rate': 0.00015870609047258024, 'epoch': 6.27}
{'loss': 0.6833, 'grad_norm': 1.067254900932312, 'learning_rate': 0.00015769522365428356, 'epoch': 6.29}
{'loss': 0.6905, 'grad_norm': 2.174431085586548, 'learning_rate': 0.00015668435683598684, 'epoch': 6.32}
{'loss': 0.6834, 'grad_norm': 1.2330812215805054, 'learning_rate': 0.0001556734900176902, 'epoch': 6.34}
{'loss': 0.6875, 'grad_norm': 2.03243088722229, 'learning_rate': 0.00015466262319939348, 'epoch': 6.37}
{'loss': 0.6883, 'grad_norm': 1.424761176109314, 'learning_rate': 0.0001536517563810968, 'epoch': 6.39}
{'loss': 0.6848, 'grad_norm': 0.7015816569328308, 'learning_rate': 0.0001526408895628001, 'epoch': 6.41}
{'loss': 0.688, 'grad_norm': 4.214911460876465, 'learning_rate': 0.00015163002274450342, 'epoch': 6.44}
{'loss': 0.6863, 'grad_norm': 1.0592665672302246, 'learning_rate': 0.00015061915592620674, 'epoch': 6.46}
{'loss': 0.6821, 'grad_norm': 3.4911463260650635, 'learning_rate': 0.00014960828910791005, 'epoch': 6.48}
{'loss': 0.6862, 'grad_norm': 1.6535162925720215, 'learning_rate': 0.00014859742228961334, 'epoch': 6.51}
{'loss': 0.6834, 'grad_norm': 2.454270124435425, 'learning_rate': 0.00014758655547131665, 'epoch': 6.53}
{'loss': 0.6831, 'grad_norm': 7.501477241516113, 'learning_rate': 0.00014657568865301997, 'epoch': 6.56}
{'loss': 0.681, 'grad_norm': 21.156442642211914, 'learning_rate': 0.00014556482183472329, 'epoch': 6.58}
{'loss': 0.6946, 'grad_norm': 1.0671484470367432, 'learning_rate': 0.0001445539550164266, 'epoch': 6.6}
{'loss': 0.6876, 'grad_norm': 12.373312950134277, 'learning_rate': 0.0001435430881981299, 'epoch': 6.63}
{'loss': 0.6879, 'grad_norm': 1.7346553802490234, 'learning_rate': 0.00014253222137983323, 'epoch': 6.65}
{'loss': 0.6867, 'grad_norm': 0.7325024604797363, 'learning_rate': 0.00014152135456153652, 'epoch': 6.67}
{'loss': 0.685, 'grad_norm': 1.4024722576141357, 'learning_rate': 0.00014051048774323983, 'epoch': 6.7}
{'loss': 0.6882, 'grad_norm': 6.931972026824951, 'learning_rate': 0.00013949962092494315, 'epoch': 6.72}
{'loss': 0.6838, 'grad_norm': 9.208909034729004, 'learning_rate': 0.00013848875410664646, 'epoch': 6.75}
{'loss': 0.6819, 'grad_norm': 1.4816389083862305, 'learning_rate': 0.00013747788728834975, 'epoch': 6.77}
{'loss': 0.6877, 'grad_norm': 1.0466562509536743, 'learning_rate': 0.0001364670204700531, 'epoch': 6.79}
{'loss': 0.6878, 'grad_norm': 3.8824195861816406, 'learning_rate': 0.00013545615365175638, 'epoch': 6.82}
{'loss': 0.688, 'grad_norm': 1.9836394786834717, 'learning_rate': 0.0001344452868334597, 'epoch': 6.84}
{'loss': 0.6842, 'grad_norm': 4.867530822753906, 'learning_rate': 0.00013343442001516301, 'epoch': 6.86}
{'loss': 0.6805, 'grad_norm': 8.462228775024414, 'learning_rate': 0.00013242355319686633, 'epoch': 6.89}
{'loss': 0.6813, 'grad_norm': 1.1161503791809082, 'learning_rate': 0.00013141268637856962, 'epoch': 6.91}
{'loss': 0.6862, 'grad_norm': 4.100646495819092, 'learning_rate': 0.00013040181956027293, 'epoch': 6.94}
{'loss': 0.6892, 'grad_norm': 9.865009307861328, 'learning_rate': 0.00012939095274197625, 'epoch': 6.96}
{'loss': 0.6842, 'grad_norm': 1.9954859018325806, 'learning_rate': 0.00012838008592367956, 'epoch': 6.98}
{'eval_loss': 0.7130126953125, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.6892, 'eval_samples_per_second': 1265.285, 'eval_steps_per_second': 8.706, 'epoch': 7.0}
{'loss': 0.6872, 'grad_norm': 2.0173797607421875, 'learning_rate': 0.00012736921910538285, 'epoch': 7.01}
{'loss': 0.6842, 'grad_norm': 3.0453460216522217, 'learning_rate': 0.0001263583522870862, 'epoch': 7.03}
{'loss': 0.6834, 'grad_norm': 0.8283935189247131, 'learning_rate': 0.00012534748546878948, 'epoch': 7.05}
{'loss': 0.6818, 'grad_norm': 2.0425896644592285, 'learning_rate': 0.0001243366186504928, 'epoch': 7.08}
{'loss': 0.6819, 'grad_norm': 1.538401484489441, 'learning_rate': 0.0001233257518321961, 'epoch': 7.1}
{'loss': 0.6821, 'grad_norm': 7.1336283683776855, 'learning_rate': 0.00012231488501389943, 'epoch': 7.13}
{'loss': 0.6915, 'grad_norm': 4.78331184387207, 'learning_rate': 0.00012130401819560273, 'epoch': 7.15}
{'loss': 0.6896, 'grad_norm': 9.38216495513916, 'learning_rate': 0.00012029315137730603, 'epoch': 7.17}
{'loss': 0.6844, 'grad_norm': 0.8501335978507996, 'learning_rate': 0.00011928228455900936, 'epoch': 7.2}
{'loss': 0.6838, 'grad_norm': 2.251248359680176, 'learning_rate': 0.00011827141774071266, 'epoch': 7.22}
{'loss': 0.688, 'grad_norm': 2.653312921524048, 'learning_rate': 0.00011726055092241596, 'epoch': 7.24}
{'loss': 0.6874, 'grad_norm': 3.946528434753418, 'learning_rate': 0.00011624968410411929, 'epoch': 7.27}
{'loss': 0.6842, 'grad_norm': 2.2804322242736816, 'learning_rate': 0.00011523881728582259, 'epoch': 7.29}
{'loss': 0.6829, 'grad_norm': 2.124013900756836, 'learning_rate': 0.00011422795046752591, 'epoch': 7.32}
{'loss': 0.6879, 'grad_norm': 1.2858988046646118, 'learning_rate': 0.00011321708364922922, 'epoch': 7.34}
{'loss': 0.6818, 'grad_norm': 17.656450271606445, 'learning_rate': 0.00011220621683093252, 'epoch': 7.36}
{'loss': 0.6809, 'grad_norm': 2.542548894882202, 'learning_rate': 0.00011119535001263584, 'epoch': 7.39}
{'loss': 0.688, 'grad_norm': 0.7827087044715881, 'learning_rate': 0.00011018448319433916, 'epoch': 7.41}
{'loss': 0.6876, 'grad_norm': 0.646426260471344, 'learning_rate': 0.00010917361637604247, 'epoch': 7.43}
{'loss': 0.6889, 'grad_norm': 1.1741173267364502, 'learning_rate': 0.00010816274955774577, 'epoch': 7.46}
{'loss': 0.6853, 'grad_norm': 1.6399961709976196, 'learning_rate': 0.00010715188273944907, 'epoch': 7.48}
{'loss': 0.6833, 'grad_norm': 1.1147314310073853, 'learning_rate': 0.0001061410159211524, 'epoch': 7.51}
{'loss': 0.6857, 'grad_norm': 2.589219331741333, 'learning_rate': 0.0001051301491028557, 'epoch': 7.53}
{'loss': 0.6905, 'grad_norm': 1.6146928071975708, 'learning_rate': 0.000104119282284559, 'epoch': 7.55}
{'loss': 0.688, 'grad_norm': 1.517995834350586, 'learning_rate': 0.00010310841546626233, 'epoch': 7.58}
{'loss': 0.685, 'grad_norm': 8.484061241149902, 'learning_rate': 0.00010209754864796564, 'epoch': 7.6}
{'loss': 0.6826, 'grad_norm': 2.843867063522339, 'learning_rate': 0.00010108668182966894, 'epoch': 7.62}
{'loss': 0.69, 'grad_norm': 0.8834275603294373, 'learning_rate': 0.00010007581501137227, 'epoch': 7.65}
{'loss': 0.6886, 'grad_norm': 2.537055492401123, 'learning_rate': 9.906494819307557e-05, 'epoch': 7.67}
{'loss': 0.6906, 'grad_norm': 1.2092149257659912, 'learning_rate': 9.805408137477887e-05, 'epoch': 7.7}
{'loss': 0.6881, 'grad_norm': 1.3064974546432495, 'learning_rate': 9.704321455648219e-05, 'epoch': 7.72}
{'loss': 0.6884, 'grad_norm': 4.82521915435791, 'learning_rate': 9.60323477381855e-05, 'epoch': 7.74}
{'loss': 0.6901, 'grad_norm': 0.5621276497840881, 'learning_rate': 9.50214809198888e-05, 'epoch': 7.77}
{'loss': 0.6867, 'grad_norm': 1.9082890748977661, 'learning_rate': 9.401061410159212e-05, 'epoch': 7.79}
{'loss': 0.6822, 'grad_norm': 1.2940186262130737, 'learning_rate': 9.299974728329543e-05, 'epoch': 7.81}
{'loss': 0.6895, 'grad_norm': 18.08988380432129, 'learning_rate': 9.198888046499873e-05, 'epoch': 7.84}
{'loss': 0.6827, 'grad_norm': 2.021700859069824, 'learning_rate': 9.097801364670205e-05, 'epoch': 7.86}
{'loss': 0.6892, 'grad_norm': 7.852705478668213, 'learning_rate': 8.996714682840535e-05, 'epoch': 7.89}
{'loss': 0.6807, 'grad_norm': 6.701119899749756, 'learning_rate': 8.895628001010867e-05, 'epoch': 7.91}
{'loss': 0.6889, 'grad_norm': 1.1302520036697388, 'learning_rate': 8.794541319181198e-05, 'epoch': 7.93}
{'loss': 0.684, 'grad_norm': 8.84456729888916, 'learning_rate': 8.69345463735153e-05, 'epoch': 7.96}
{'loss': 0.6889, 'grad_norm': 1.3240411281585693, 'learning_rate': 8.592367955521861e-05, 'epoch': 7.98}
{'eval_loss': 0.7042538523674011, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.5336, 'eval_samples_per_second': 1634.03, 'eval_steps_per_second': 11.243, 'epoch': 8.0}
{'loss': 0.6882, 'grad_norm': 3.7014219760894775, 'learning_rate': 8.491281273692191e-05, 'epoch': 8.0}
{'loss': 0.6858, 'grad_norm': 2.423424482345581, 'learning_rate': 8.390194591862523e-05, 'epoch': 8.03}
{'loss': 0.6943, 'grad_norm': 10.859580039978027, 'learning_rate': 8.289107910032854e-05, 'epoch': 8.05}
{'loss': 0.6947, 'grad_norm': 7.936357498168945, 'learning_rate': 8.188021228203185e-05, 'epoch': 8.08}
{'loss': 0.6896, 'grad_norm': 3.902386426925659, 'learning_rate': 8.086934546373516e-05, 'epoch': 8.1}
{'loss': 0.6873, 'grad_norm': 10.12100601196289, 'learning_rate': 7.985847864543848e-05, 'epoch': 8.12}
{'loss': 0.6852, 'grad_norm': 40.16244125366211, 'learning_rate': 7.884761182714178e-05, 'epoch': 8.15}
{'loss': 0.6902, 'grad_norm': 1.9003064632415771, 'learning_rate': 7.78367450088451e-05, 'epoch': 8.17}
{'loss': 0.6874, 'grad_norm': 4.382854461669922, 'learning_rate': 7.68258781905484e-05, 'epoch': 8.19}
{'loss': 0.6837, 'grad_norm': 12.93093204498291, 'learning_rate': 7.581501137225171e-05, 'epoch': 8.22}
{'loss': 0.6872, 'grad_norm': 1.4642767906188965, 'learning_rate': 7.480414455395503e-05, 'epoch': 8.24}
{'loss': 0.6864, 'grad_norm': 2.615997314453125, 'learning_rate': 7.379327773565833e-05, 'epoch': 8.27}
{'loss': 0.6874, 'grad_norm': 1.282927393913269, 'learning_rate': 7.278241091736164e-05, 'epoch': 8.29}
{'loss': 0.6834, 'grad_norm': 2.9746344089508057, 'learning_rate': 7.177154409906494e-05, 'epoch': 8.31}
{'loss': 0.6874, 'grad_norm': 1.3382399082183838, 'learning_rate': 7.076067728076826e-05, 'epoch': 8.34}
{'loss': 0.6875, 'grad_norm': 25.595863342285156, 'learning_rate': 6.974981046247157e-05, 'epoch': 8.36}
{'loss': 0.6911, 'grad_norm': 4.44508171081543, 'learning_rate': 6.873894364417488e-05, 'epoch': 8.38}
{'loss': 0.6857, 'grad_norm': 0.9448908567428589, 'learning_rate': 6.772807682587819e-05, 'epoch': 8.41}
{'loss': 0.6867, 'grad_norm': 44.04539108276367, 'learning_rate': 6.671721000758151e-05, 'epoch': 8.43}
{'loss': 0.6828, 'grad_norm': 22.416414260864258, 'learning_rate': 6.570634318928481e-05, 'epoch': 8.46}
{'loss': 0.6811, 'grad_norm': 2.117994785308838, 'learning_rate': 6.469547637098812e-05, 'epoch': 8.48}
{'loss': 0.6894, 'grad_norm': 6.704606533050537, 'learning_rate': 6.368460955269143e-05, 'epoch': 8.5}
{'loss': 0.6863, 'grad_norm': 18.336349487304688, 'learning_rate': 6.267374273439474e-05, 'epoch': 8.53}
{'loss': 0.6908, 'grad_norm': 0.9440175890922546, 'learning_rate': 6.166287591609806e-05, 'epoch': 8.55}
{'loss': 0.6868, 'grad_norm': 1.1593977212905884, 'learning_rate': 6.0652009097801364e-05, 'epoch': 8.57}
{'loss': 0.6812, 'grad_norm': 1.604027509689331, 'learning_rate': 5.964114227950468e-05, 'epoch': 8.6}
{'loss': 0.6854, 'grad_norm': 3.1357076168060303, 'learning_rate': 5.863027546120798e-05, 'epoch': 8.62}
{'loss': 0.6802, 'grad_norm': 1.9184688329696655, 'learning_rate': 5.7619408642911296e-05, 'epoch': 8.65}
{'loss': 0.6806, 'grad_norm': 31.346269607543945, 'learning_rate': 5.660854182461461e-05, 'epoch': 8.67}
{'loss': 0.6828, 'grad_norm': 1.6556048393249512, 'learning_rate': 5.559767500631792e-05, 'epoch': 8.69}
{'loss': 0.6864, 'grad_norm': 95.5251693725586, 'learning_rate': 5.4586808188021235e-05, 'epoch': 8.72}
{'loss': 0.6874, 'grad_norm': 15.829652786254883, 'learning_rate': 5.357594136972454e-05, 'epoch': 8.74}
{'loss': 0.6863, 'grad_norm': 1.1849849224090576, 'learning_rate': 5.256507455142785e-05, 'epoch': 8.76}
{'loss': 0.6886, 'grad_norm': 1.3020247220993042, 'learning_rate': 5.155420773313117e-05, 'epoch': 8.79}
{'loss': 0.6834, 'grad_norm': 3.6714589595794678, 'learning_rate': 5.054334091483447e-05, 'epoch': 8.81}
{'loss': 0.6909, 'grad_norm': 2.1522700786590576, 'learning_rate': 4.9532474096537784e-05, 'epoch': 8.84}
{'loss': 0.6849, 'grad_norm': 0.8436204791069031, 'learning_rate': 4.852160727824109e-05, 'epoch': 8.86}
{'loss': 0.6828, 'grad_norm': 4.010292053222656, 'learning_rate': 4.75107404599444e-05, 'epoch': 8.88}
{'loss': 0.6837, 'grad_norm': 1.5579590797424316, 'learning_rate': 4.6499873641647716e-05, 'epoch': 8.91}
{'loss': 0.6839, 'grad_norm': 16.654409408569336, 'learning_rate': 4.5489006823351025e-05, 'epoch': 8.93}
{'loss': 0.6829, 'grad_norm': 2.021960496902466, 'learning_rate': 4.447814000505433e-05, 'epoch': 8.95}
{'loss': 0.6889, 'grad_norm': 10.189488410949707, 'learning_rate': 4.346727318675765e-05, 'epoch': 8.98}
{'eval_loss': 0.7073963284492493, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.54, 'eval_samples_per_second': 1614.754, 'eval_steps_per_second': 11.111, 'epoch': 9.0}
{'loss': 0.6846, 'grad_norm': 6.204289436340332, 'learning_rate': 4.245640636846096e-05, 'epoch': 9.0}
{'loss': 0.6825, 'grad_norm': 14.266681671142578, 'learning_rate': 4.144553955016427e-05, 'epoch': 9.03}
{'loss': 0.6863, 'grad_norm': 11.030625343322754, 'learning_rate': 4.043467273186758e-05, 'epoch': 9.05}
{'loss': 0.6889, 'grad_norm': 2.1816294193267822, 'learning_rate': 3.942380591357089e-05, 'epoch': 9.07}
{'loss': 0.6873, 'grad_norm': 2.6307077407836914, 'learning_rate': 3.84129390952742e-05, 'epoch': 9.1}
{'loss': 0.6886, 'grad_norm': 3.207083225250244, 'learning_rate': 3.740207227697751e-05, 'epoch': 9.12}
{'loss': 0.6796, 'grad_norm': 1.476269245147705, 'learning_rate': 3.639120545868082e-05, 'epoch': 9.14}
{'loss': 0.6846, 'grad_norm': 4.267165184020996, 'learning_rate': 3.538033864038413e-05, 'epoch': 9.17}
{'loss': 0.6798, 'grad_norm': 6.469908714294434, 'learning_rate': 3.436947182208744e-05, 'epoch': 9.19}
{'loss': 0.6909, 'grad_norm': 1.3911192417144775, 'learning_rate': 3.3358605003790753e-05, 'epoch': 9.22}
{'loss': 0.686, 'grad_norm': 0.8256000280380249, 'learning_rate': 3.234773818549406e-05, 'epoch': 9.24}
{'loss': 0.6848, 'grad_norm': 8.40899658203125, 'learning_rate': 3.133687136719737e-05, 'epoch': 9.26}
{'loss': 0.68, 'grad_norm': 137.4888153076172, 'learning_rate': 3.0326004548900682e-05, 'epoch': 9.29}
{'loss': 0.6851, 'grad_norm': 7.115705490112305, 'learning_rate': 2.931513773060399e-05, 'epoch': 9.31}
{'loss': 0.6869, 'grad_norm': 1.9809191226959229, 'learning_rate': 2.8304270912307306e-05, 'epoch': 9.33}
{'loss': 0.681, 'grad_norm': 0.9385533928871155, 'learning_rate': 2.7293404094010618e-05, 'epoch': 9.36}
{'loss': 0.6896, 'grad_norm': 1.8257482051849365, 'learning_rate': 2.6282537275713926e-05, 'epoch': 9.38}
{'loss': 0.6826, 'grad_norm': 1.354392409324646, 'learning_rate': 2.5271670457417235e-05, 'epoch': 9.41}
{'loss': 0.6858, 'grad_norm': 165.7080841064453, 'learning_rate': 2.4260803639120546e-05, 'epoch': 9.43}
{'loss': 0.688, 'grad_norm': 1.0742026567459106, 'learning_rate': 2.3249936820823858e-05, 'epoch': 9.45}
{'loss': 0.685, 'grad_norm': 6.025027751922607, 'learning_rate': 2.2239070002527167e-05, 'epoch': 9.48}
{'loss': 0.6843, 'grad_norm': 3.0701792240142822, 'learning_rate': 2.122820318423048e-05, 'epoch': 9.5}
{'loss': 0.6878, 'grad_norm': 3.3104171752929688, 'learning_rate': 2.021733636593379e-05, 'epoch': 9.52}
{'loss': 0.6862, 'grad_norm': 0.9896804094314575, 'learning_rate': 1.92064695476371e-05, 'epoch': 9.55}
{'loss': 0.6832, 'grad_norm': 2.8261330127716064, 'learning_rate': 1.819560272934041e-05, 'epoch': 9.57}
{'loss': 0.6872, 'grad_norm': 2.0053389072418213, 'learning_rate': 1.718473591104372e-05, 'epoch': 9.6}
{'loss': 0.6846, 'grad_norm': 2.313352108001709, 'learning_rate': 1.617386909274703e-05, 'epoch': 9.62}
{'loss': 0.6889, 'grad_norm': 1.5373125076293945, 'learning_rate': 1.5163002274450341e-05, 'epoch': 9.64}
{'loss': 0.6843, 'grad_norm': 17.724075317382812, 'learning_rate': 1.4152135456153653e-05, 'epoch': 9.67}
{'loss': 0.6845, 'grad_norm': 2.0757343769073486, 'learning_rate': 1.3141268637856963e-05, 'epoch': 9.69}
{'loss': 0.6864, 'grad_norm': 2.8807196617126465, 'learning_rate': 1.2130401819560273e-05, 'epoch': 9.71}
{'loss': 0.6843, 'grad_norm': 3.590881824493408, 'learning_rate': 1.1119535001263583e-05, 'epoch': 9.74}
{'loss': 0.6888, 'grad_norm': 6.133586406707764, 'learning_rate': 1.0108668182966895e-05, 'epoch': 9.76}
{'loss': 0.688, 'grad_norm': 2.779799461364746, 'learning_rate': 9.097801364670205e-06, 'epoch': 9.79}
{'loss': 0.6864, 'grad_norm': 2.4146652221679688, 'learning_rate': 8.086934546373515e-06, 'epoch': 9.81}
{'loss': 0.6901, 'grad_norm': 2.4083566665649414, 'learning_rate': 7.0760677280768265e-06, 'epoch': 9.83}
{'loss': 0.6853, 'grad_norm': 1.6693661212921143, 'learning_rate': 6.065200909780137e-06, 'epoch': 9.86}
{'loss': 0.6851, 'grad_norm': 0.7671698927879333, 'learning_rate': 5.054334091483448e-06, 'epoch': 9.88}
{'loss': 0.6845, 'grad_norm': 30.830738067626953, 'learning_rate': 4.043467273186758e-06, 'epoch': 9.9}
{'loss': 0.6849, 'grad_norm': 3.7297286987304688, 'learning_rate': 3.0326004548900683e-06, 'epoch': 9.93}
{'loss': 0.6864, 'grad_norm': 1.7354507446289062, 'learning_rate': 2.021733636593379e-06, 'epoch': 9.95}
{'loss': 0.6892, 'grad_norm': 13.263422012329102, 'learning_rate': 1.0108668182966894e-06, 'epoch': 9.98}
{'loss': 0.6833, 'grad_norm': 4.496210098266602, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 0.70952308177948, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.6005, 'eval_samples_per_second': 1452.174, 'eval_steps_per_second': 9.992, 'epoch': 10.0}
{'train_runtime': 908.2572, 'train_samples_per_second': 741.519, 'train_steps_per_second': 4.635, 'train_loss': 0.6605989922253933, 'epoch': 10.0}
***** train metrics *****
  epoch                    =        10.0
  total_flos               = 147044775GF
  train_loss               =      0.6606
  train_runtime            =  0:15:08.25
  train_samples            =       67349
  train_samples_per_second =     741.519
  train_steps_per_second   =       4.635
05/05/2024 18:05:53 - INFO - __main__ - *** Evaluate ***
***** eval metrics *****
  epoch                   =       10.0
  eval_accuracy           =     0.5092
  eval_loss               =     0.7235
  eval_runtime            = 0:00:00.63
  eval_samples            =        872
  eval_samples_per_second =    1372.92
  eval_steps_per_second   =      9.447
05/05/2024 18:06:09 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 18:06:09 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 18:06:09 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/log,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=pearson,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=30.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.06,
warmup_steps=0,
weight_decay=0.0,
)
05/05/2024 18:06:09 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='FacebookAI/roberta-large', lora_path=None, l_num=None, mode='base', config_name=None, rank=[8], lora_alpha=[768], target_modules=['query', 'value'], lora_dropout=0.0, lora_bias='none', lora_task_type='SEQ_CLS', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False)
05/05/2024 18:06:09 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 18:06:09 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 18:06:09 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
05/05/2024 18:06:47 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
05/05/2024 18:06:47 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
05/05/2024 18:06:47 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
05/05/2024 18:06:47 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...*** PiSSA !!! ***
                                                                                                    get_PiSSA_PEFT_model...                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	567.76ms[0m
*** Parameter number after share ***
*** label and id ***
{'LABEL_0': 0}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	586.33ms[0m
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	583.40ms[0m
*** Parameter number after share ***
*** Parameter number after share ***
*** label and id ***
{'LABEL_0': 0}
*** label and id ***
{'LABEL_0': 0}
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	717.31ms[0m
                                                                                                    get_PiSSA_PEFT_model[32m...[DONE][0m[34m	713.94ms[0m
*** Parameter number after share ***
*** Parameter number after share ***
*** label and id ***
{'LABEL_0': 0}
pissa params: 786,432 || trainable params: 1,837,057 || all params: 357,197,826 || trainable%: 0.5142968031389978
pissa params: 786,432 || trainable params(wo classfier): 1,837,057 || all params: 357,197,826 || trainable%: 0.5142968031389978
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight 是可训练的: True
base_model.model.module.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.dense.bias 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.weight 是可训练的: True
base_model.model.module.classifier.modules_to_save.default.out_proj.bias 是可训练的: True
*** label and id ***
{'LABEL_0': 0}
05/05/2024 18:07:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7cf707b63ffb2592.arrow
05/05/2024 18:07:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f7d95cf3a45c20f5.arrow
05/05/2024 18:07:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-867338de1be5fbd1.arrow
05/05/2024 18:07:06 - INFO - __main__ - Sample 5238 of the training set: {'sentence1': 'Didier Reynders on Syria', 'sentence2': "Obama's day: Prime time on Syria", 'label': 1.600000023841858, 'idx': 5238, 'input_ids': [0, 20328, 906, 15278, 1187, 268, 15, 1854, 2, 2, 33382, 18, 183, 35, 1489, 86, 15, 1854, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 18:07:06 - INFO - __main__ - Sample 912 of the training set: {'sentence1': 'A man is cleaning the windows.', 'sentence2': 'A man is driving a car.', 'label': 0.4000000059604645, 'idx': 912, 'input_ids': [0, 250, 313, 16, 8143, 5, 6410, 4, 2, 2, 250, 313, 16, 1428, 10, 512, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
05/05/2024 18:07:06 - INFO - __main__ - Sample 204 of the training set: {'sentence1': 'A woman holds a kangaroo.', 'sentence2': 'A woman is picking up a kangaroo.', 'label': 3.25, 'idx': 204, 'input_ids': [0, 250, 693, 3106, 10, 449, 1097, 32750, 4, 2, 2, 250, 693, 16, 6201, 62, 10, 449, 1097, 32750, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
========================================
hello! PiSSA
